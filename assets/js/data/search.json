[ { "title": "[Python] Virtual Env(venv)로 가상환경 만들기", "url": "/posts/venv-Python-32bit/", "categories": "Python, venv", "tags": "Python, Virtual Environment, VSCode, Python 32bit", "date": "2023-02-20 18:55:00 +0800", "snippet": "가상환경 설정을 통해 다양한 개발환경 세팅하는 방법을 정리해본다.가상환경이라고 하니까 뭐 대단히 별거인거 같지만, 막상 해보면 쉽게 된다. 하나의 PC에서 python v3.2도 쓰고 v3.6도 쓰고 그러고 싶으면 하는 일들이라고 생각하면 편하다.자세하게는 Python 64bit가 설치된 PC에서 python 32bit 개발환경을 가상환경으로 만드는 방법에 대해 정리해본다. vscode를 사용할 것이다. 구글에 ‘python venv 가상환경’ 이렇게 검색해보면 많이들 나온다.1. Intro여태까지 단순한 공학용 계산이나 아주 기초적인 자동화만 했었다. 이런 상황에서는 기존에 잘 사용하던 Python version에서 벗어나 새로운 개발환경이 필요한 경우는 아직까지는 한 번도 없었다.최근에 어떤 이슈가 있어서 python 32bit를 통해 특정 회사에서 제공하는 API를 이용해야 하는 상황을 겪게 되면서 개발환경 세팅을 새로 해야 할 필요가 생겼다.처음에는 포맷해야 하나 엄청 난감했었는데 가상환경 설정으로 하나의 PC에서 여러개의 새로운 개발 환경 설정 및 세팅이 가능했고 그렇게 복잡하거나 어렵지는 않았다.이런 경우 외에도 다른 version의 tensorflow를 써야 하는 상황처럼 특정 버젼의 python 라이브러리를 사용해야 하는 경우에도 가상환경을 통해 쉽게 개발환경 세팅을 할 수 있다.이번달 포스팅 내용은 딱 가상환경 설정까지여서 대놓고 날먹인 수준이다. 구현중인 프로그램 내용이 정리가 되는대로 다음달 포스팅 작성해 보려 한다. 다음달 포스팅이 힘들어질 것도 같다..2. Virtual Enviroment SettingPhthon 64bit가 설치되어 있는 상황 하에서 32bit python을 쓸수 있는 가상환경을 만드는 게 목적이다. 먼저 Python 32bit를 설치하자. 64bit때와 동일하게 공식 사이트에서 32bit 설치파일을 받고 설치해준다.설치 할 때 환경변수 체크 ‘안’하는 점만 주의한다. 32bit가 설치된 Directory도 기억해두자. 일부러 Custom으로 설정하지 않는 이상 보통 C:\\Users\\~~\\AppData\\Local\\Programs\\Python 이런 경로에 python**-32 이런 식으로 폴더가 생길것이다.2.1. VenvPython을 처음에 설치하게 될 때 자동으로 같이 설치되는 표준 라이브러리가 있다. 가상환경을 만들어주는 venv라는 모듈도 이런 표준 라이브러리 중 하나이다. 가상환경을 만들려고 할 때 python이 설치가 되어 있다면 새로운 모듈 설치가 필요 없다는 뜻이다.먼저 vscode로 커맨드 창을 열거다. 그 전에 default로 Terminal Profile을 Command Prompt로 설정해준다. 처음에는 powershell로 되어 있을 텐데 CMD로 하는 게 추천된다고 한다. 설정 방법은 여기를 참고하자.이제 vscode에서 터미널을 열고 가상환경 ‘폴더’를 만들 위치로 가보자. 커맨드창에서 특정 경로로 가는 방법은 cd '원하는 directory' 이다. 잘 모르겠으면 구글링 해보자.아니면 바탕화면에 test라는 폴더 하나 만들고 우클릭해서 vscode로 열어준다. 그리고 terminal열고 아래 명령어를 입력한다python -m venv venv-test이렇게 하면 커맨드 directory내에 venv-test라는 이름의 폴더가 생길 것이다. venv-test라는 폴더 이름은 아무렇게나 지어도 된다. 여기까지 하면 아래 영상처럼 작동하고 가상환경 만들기 거의 다 끝났다.이제 vent-test라는 가상환경 폴더내에 pyvenv.cfg라는 파일을 메모장으로 연다. (우클릭 &gt; 연결프로그램 &gt; 메모장) 아니면 커맨드에서 venv-test 경로로 간 이후에 아래처럼 명령어를 입력한다.notepad pyvenv.cfg아래 그림처럼 보일 것이다.여기서는 PC에 python 64bit version 3.9.13이 설치되어 있었고, 32bit도 3.9.13으로 설치했다. 32bit python을 설치한 경우 위치는 위 그림 home경로에서 마지막에 폴더이름만 Python39-32였고 이 부분만 수정하고 저장한다.각자 상황에 맞게 32bit가 설치되어 있는 경로를 home부분에 적절하게 넣어주면 된다.이후에 vscode에서 Crtl+Shift+p로 명령어창을 열고 python:Select interpreter를 클릭해서 venv-tset:venv라고 표시된 항목을 클릭하면 가상환경이 Activate된다. 이 상태에서 terminal을 열면 Directory에 (venv-test)라고 뜨는 것을 볼 수 있다. 이러면 성공이다. 이 상태에서 python이라고 커맨드창에 입력하면 32bit로 뜨는 것을 볼 수 있다.가상환경을 끄고 싶으면 다시 Ctrl+Shift+p로 명령어창을 열고 python:Select interpretre에서 gloabal로 표시되어 있는 기존 python 64bit를 클릭해준다. 확인하려면 다시 terminal열고 python이라고 입력하면 64bit로 뜬다. 가상환경이 필요 없어졌다면 venv-test 폴더를 그냥 지워도 된다.여기까지 설명한 부분은 아래 영상처럼 작동된다.Vscode가 아닌 까만색 Command Prompt로도 가상환경 만들고 설정하고 다 할 수 있다. 가상환경 만들 때 생긴 venv-test 폴더 안에 Scripts 디렉토리에 접근해서 activate 커맨드를 입력하면 (activate파일을 실행하면) 된다. 가상환경을 끌 때는 동일한 디렉토리에서 deactivate 커맨드를 입력한다. 더 한 것들도 할 수 있겠지만, Python 코드 실행도 Command Prompt로 할 생각이 아니라면 이 정도에서 멈추자.까만색 커맨드 프롬프트에서 명령어 좀 쓰다보면 처음 몇 분은 약간 개발자된 느낌에 뽕이 차오르다가 슬슬 불편해지고 후회된다. 가능한 vscode 쓰자.Summary다양한 version의 모듈을 쓰거나 다른 bit의 python을 써야하는 경우 등 다양한 개발환경이 필요한 경우 가상환경 venv 통해 개발환경 세팅을 할 수 있었다.처음 Python 설치할 때 Anaconda통해 설치했었는데 version관리나 가상환경 관리가 편하다는 애기를 들었던 거 같다. 그런거 한 번도 안써보고 vscode로 갈아탔는데, 돌고 돌아 가상환경이라니. 평생 쓸일 없을 줄 알았는데, 기왕이면 Anaconda 안쓰고 vscode로도 이런거 편하게(?) 해볼 수 있다는거 구글 검색해고 직접 해본 내용들 정리한다." }, { "title": "[머신러닝] 공부 시작한다면 이 순서로", "url": "/posts/How-to-study-ML-Coursera/", "categories": "Machine Learning, Syllabus", "tags": "Coursera, Andrew NG, AI, Machine Learning", "date": "2023-01-04 19:55:00 +0800", "snippet": "매우 건방진 얘기 같겠지만 체계적으로 머신러닝 기초 공부를 시작 해보고 싶다면 아래 순서대로 해보시기를 추천드린다. What is Machine Learning? Linear Regression Gradient Descent Logistic Regression Artifical Neural Network Decision Tree Supervised Machine Learning Applications Some Unsupervised Machine Learning (Clustering, Anomaly detection, PCA)처음 머신러닝이란것에 관심이 생겨서 공부해보려 했을 때 인터넷에 수많은 정보 중에 기초부터 다질만한 체계적인 과정을 접하기 까지 많은 시간을 헤맸었다. 다른 포스팅에서도 종종 말했었던 Coursera 강의내 목차들을 살펴보면서 공부 순서를 한 번 정리해본다.1. IntroCoursera라는 강의 플랫폼이 있다. 제한적이지만 일정기간 무료로 강의를 들을 수 있다.여기서 들었던 Andrew NG 교수의 강의 목차를 소개하려고 한다. 둘 다 들으면 좋고 아니면 이 흐름대로 공부해보자. 개인적으로 머신러닝 기초를 다지고 어느정도의 기능구현을 시작해보는데 이보다 좋은 방법은 없을 것 같다. Supervised Machine Learning: Regression and Classification이 강의는 GNU Octave라는 무료 수치해석 프로그램으로 실습해나가면서 머신러닝 공부하는 강의다. (Matlab의 무료버전으로 생각하면 된다. 실제로 m파일 수정하고 실행한다.) 내 머신러닝 첫 강의였고 개인적으로 굉장히 만족스러웠었다. 머신러닝이 뭔지, Regression이 뭔지 이런거 수학으로 얘기하고 실습한다.매주 코드짜는 과제 수행해야한다. 가능하면 직접 해보되 답은 구글링하면 있으니 걱정하지 말고 시간이 없다면 배껴서 제출하자. Machine Learning Specialization이 강의를 좀 더 추천드린다. 꽤 최근에 마음먹고 들었는데, 이 강의는 Python 기반으로 Tensorflow와 Scikit-learn같은 머신러닝 모듈도 경험해 볼 수 있다. 7일간 무료인데 주말끼고 해도 다 듣느라 일주일내 다 듣는게 정말 힘들었다. 위 강의를 먼저 들었던 덕에 그나마 가능했던 일정이였다고 생각한다. 7일 후에는 $45/월 정도 지불해야 들을 수 있고 여유가 된다면 돈내고 차근차근 듣는것도 나쁘지 않다고 본다.마찬가지로 매주 코드짜서 제출해야 하는 과제가 있다. Github에 누군가 답을 올려놨으니 시간없다면 여기 한번 보자.이번 포스팅에서는 이 강의에서 가르쳐주는 내용과 목차 간단히 정리해 보려고 한다.2. Coursera - Machine Learning Specialization이 강의는 아래 3개 챕터로 이루어져 있다. Supervised Machine Learning: Regression and Classification Advanced Learning Algorithms Unsupervised Learning, Recommenders, Reinforcement Learning뭘 배우는지 하나씩 살펴보자.2.1. Supervised Machine Learning: Regression and Classification이 챕터 하나가 3주짜리 강의로 구성되어 있다. 7일안에 3개 챕터 다 배워야 하니 실상은 한 주 강의를 하루안에 무조건 돌파해야 한다. 어찌됬든 각 주마다 배우는 내용은 아래와 같다. Week 1: Introduction to Machine Learning Overview of Machine Learning Supervised vs. Unsupervised Machine Learning Regression Model Train the model with gradient descent Keyword: Jupyter Notebook, Python, Linear Regression, Cost function, Gradient descent첫 주에는 머신러닝 Intro다. 머신러닝이 뭔지, 지도-비지도학습이 뭔지, Regression이 뭔지, 이때 쓰는 Gradient descent 알고리즘에 대해 설명한다. 관련 내용들은 이 블로그에도 나름 정리해둔 포스팅들이 있지만 강의를 듣는게 더 좋다.Python으로 실습이 수행되고 Jupyter Notebook으로 한다. 따로 설치할 필요 없이 웹에서 실행가능하다. Week 2: Regression with multiple input variables Multiple linear regression Gradient descent in practice Keyword: Vectorization, Numpy, Multiple Linear Regression, Feature Scailing, Convergence, Learning rate, Feature Engineering두번째 주에는 Regression을 자세하게 실습해본다. 위 키워드들에 대해 공부하는데, 이런 걸 알아둬야 한다는 느낌으로 봐도 될 것 같다. Week 3: Classification Classification with logistic regression Cost function for logistic regression Gradient descent for logistic regression The problem of overfitting Keyword: Logistic regression, Decision Boundary, Cost function, Scikit-learn, Overfitting, Regularization세번째 주에는 Logistic regression으로 분류에 대해 배운다. Lienar regression 공부할 때와 비슷한 느낌이지만 Cost function이 다르다. Entropy라는 단어도 볼 수 있다. 학문들간에 연결되어 있다는 느낌이 드는 부분도 꽤 재밌다.Linear/Logistic regression 이 두 개는 자연스럽게 뒤에 배울 인공신경망으로 이어지는데 중요한 역할을 한다.2.2. Advanced Learning Algorithms이 챕터는 4주짜리다. 쉴생각은 꿈도꾸지말고 달리자. Week 1: Neural Networks Neural Networks Intuition Neural network model Tensorflow implementation Neural network implementation in Python Speculations on artificial general intelligence (AGI) Keyword: Neural network, Neuron and Layer, Tensorflow, Matrix multiplication첫번째 주는 드디어 인공신경망에 대해 배운다. Tensorflow로 간단한 실습도 해본다. Numpy로 행렬연산 연습도 하는데 알아두면 무조건 좋다. 앞에서 배운 내용들과 자연스럽게 이어지니 이전 과정들을 잘 들었다면 따라가는데 문제 없을것 같다. Week 2: Neural network training Neural Network Training Activation Functions Multiclass Classification Additional Neural Network Concepts Back Propagation Keyword: Training, Acitivation function, ReLU, Softmax, Advanced optimization, Back propagation두번째 주는 인공신경망을 학습시키는 것에 대해 배운다. Multiclass 분류에 대해서도 알려준다. Week 3: Advice for applying machine learning Advice for applying machine learning Bias and variance Machine learning development process Skewed datasets Keyword: Model evaluation, Model selection, Training/Cross validation/Test data set, Diagnosing bias/variance, Learning curve, Transfer learning, Error matrix, Precision/Recall세번째 주는 인공신경망 학습중에 주의해야 할 점, 모델을 선택하는 기준같이 실제 머신러닝을 해볼 때 주의해야 할 Practical한 내용에 대해 실습한다. Tensorflow 블로그에 있는 Regression 예제와 같이 봐도 좋을 것 같다.추가로 Data가 한쪽으로 쏠려있는 Skewed dataset을 사용하는 경우에 대해서도 알려준다. 뒤에 비정상을 진단하는 Anomaly detection에서도 쓰이는 개념이다. Week 4: Decision trees Decision trees Decision tree learning Tree ensembles Keyword: Decision tree, Purity, One-hot encoding, Random forest, XGBoost마지막주는 Decision tree에 대해 배운다. 개인적으로 Regression에서 인공신경망이 가장 좋은 모델이라고 막연히 생각해왔었는데, 이 강의 들으면서 생각이 많이 바꼈다. 좀 더 자세하게 다뤄보고 포스팅 해볼 생각이다.2.3. Unsupervised Learning, Recommenders, Reinforcement Learning마지막은 3주짜리다. 시간에 쫓기기도 했고 비지도학습은 개인적으로 너무 어렵고 관심도 덜해서 몇개 빼고는 대충 듣긴했다. 그냥 이런게 있구나 하는 정도로 가볍게 들어도 크게 상관 없지 않을까 생각한다. Week 1: Unsupervised learning Clustering Anomaly detection Keyword: K-means algorithm, Gaussian distribution첫 주는 비지도 학습 강의에서 그나마 알아듣기 쉬운 부분이었다. 특히 Anomaly detection은 한 번 써먹어볼수도 있겠다는 생각이 들기도 했다. 통계적으로 평균값에 멀어지는 것들을 비정상으로 처리하려는 느낌이었다. 실제로 잘 들어 맞을지는 모르겠지만. Week 2: Recommender systems Collaborative filtering Recommender systems implementation detail Content-based filtering Principal Component Analysis Keyword: Recommender두 번째 주는 추천알고리즘에 대해 다룬다. Netflix나 유튜브같은 플랫폼에서 사용자들에게 영상추천해주는 그런것들이다. 마지막에 PCA에 대해서도 말하는데, 언젠가 자세하게 공부해봐야 겠다. Week 3: Reinforcement learning Reinforcement learning introduction State-action value function Continuous state spaces Keyword: Reinforcement learning, Return, Bellman Equation비지도학습 끝판왕 강화학습이다. 알고리즘을 어떻게든 설명해 주려고 하지만 잘 이해가 안됬다. 달착륙선이 안전하게 착륙할 수 있도록 학습하는 예제를 하는데 한 번 보고는 전혀 모르겠다. 이해는 안가지만 어떻게든 코드돌려서 학습한 머신러닝 모델로 달착륙선이 넘어지지 않고 잘 착륙하는 움짤 보면 신기하긴 하다.Summary머신러닝을 한다고 해도 이미지나 영상처리에 더 관심이 있다거나 자연어 처리로 챗봇 같은 것에 관심이 있다거나 하는 식으로 관심분야들은 각자 다를 수 있다. 개인적으로는 수식 모델링에 관심이 있어서 그동안 인공신경망과 딥러닝으로 Regression하는 데에만 관심이 있었다. 최근에는 Decision tree나 Anomaly detection도 해보면 재밌을 것 같다.관심이 어디있든 위 내용들은 머신러닝 전반에 대한 기초지식이기 때문에 공부해두면 반드시 언젠가 써먹을 일이 생길 것이라고 생각한다.관련해서 블로그에 몇 개 주제는 이미 포스팅을 했고, 나머지 주제들도 조금씩 써서 나름 정리해 나가려고 한다. 배경 이론, 수식들과 코드로 기능구현까지 각 세부 주제별로 정리해 볼 생각이다. 목차 정리된 이 글에 하나씩 링크 걸어두면 좋을 것도 같다. 기본 뼈대는 Coursera강의 내용이겠지만 너무 베끼지는 말고 가능하면 좀 더 공부도 해서 예제도 바꿔보고 내용도 더 추가해보려 한다." }, { "title": "[머신러닝] Tensorflow", "url": "/posts/AI-Tensorflow/", "categories": "Machine Learning", "tags": "Deep Learning, Tensorflow", "date": "2022-12-25 19:55:00 +0800", "snippet": "Tensorflow로 Deep-Learning 해보자. Tensorflow Blog에 잘 정리된 튜토리얼 따라할거니까 생각보다 어렵지 않다.물론 제대로 하려면 여러가지를 알아둬야 한다. 인공신경망, 딥러닝, 여러 수학들이랑 정의들, 라이브러리 테크닉들, 그리고 제일 중요한건 딥러닝을 적용해보려는 분야에 대한 지식이다. 이건, 어떻게 할 수 없으니 알아서 잘 해보자.좀 더 원활한 이해를 위해서는 여기와 여기를 읽어보고 시작하기를 추천드린다. 아니면 이 포스팅은 그냥 가볍게 읽으면서 지나가도 괜찮을 것 같다.1. Intro딥러닝, 인공신경망 이거는 다른 포스팅에 정리되어 있고, Tensorflow에 대해 간단히 알아본다. 위키에서는 이렇게 말한다. 구글이 2011년에 개발을 시작하여 2015년에 오픈 소스로 공개한 기계학습 라이브러리.일단 구글에서 만들었고, 잘은 모르지만 Python만으로 만들어진건 아니고 계산속도 빠른 C같은 언어들도 섞여있다고 한다. 당연히 학습 속도가 빠르고 조금 제한이 있지만 GPU연산도 지원한다고 한다.보통 구글링으로 딥러닝을 검색해보면 많이 나오는게 이미지 인식, 자연어 처리 같은 예제 들이고 이런 곳에서 쓰이는 방법들(convolutional, recurrent 류들) 구현이 코드 몇 줄로 가능하다. 이 외에도 효율적인 학습을 위한 다양한 계산 옵션들, 콜백들(early stopping, dropout 등)을 제공한다.완전 커스터마이징이 가능한 건 아닌 것 같지만, 상당한 수준의 모델빌드가 생각보다 쉽게 가능하다. Teonsorflow Blog에 튜토리얼이 잘 정리되어 있어서 그대로 따라하면 기능구현을 어렵지 않게 따라해 볼 수 있다.2. TensorFlow Tutorial - Regression이미지나 자연어에 관한 딥러닝에 대해서는 실은 크게 관심이 없다. 일하면서 별로 겪을 수 있는 일들도 없고, 특히 데이터를 미리 정제하고 준비시키는게 조금 짜증나서 별로 재미 없었다. 물론, 딥러닝을 잘 이해하려면 이런 예제들 한 번 정도 겪어보는게 나쁘진 않은 것 같다. 확실히 인기는 많은지 구글링 해보면 대부분 나오는 예제들이 이 두 가지다.여기서는 단순한 Regression 예제를 Deep Learning으로 해본다. 숫자를 넣고, 숫자가 나온다. 물론, 이미지나 자연어 처리도 숫자를 넣고 숫자가 나오기는 매한가지긴 하다.3. 자동차 연비 예측하기: 회귀이 튜토리얼은 개인적으로 정말 좋아하는 예제다. 이미지나 자연어처리만 넘쳐나는 딥러닝 예제에서 얼마안되는 단순회귀 관련 문제이다. 자동차 무게, 배기량, 실린더수, 마력, 연식 등의 특성에 따른 자동차의 연비 데이터를 활용해서 자동차 연비 예측하는 딥러닝 모델을 만들거다. 그리고 이 방법을 조금만 잘 이용하면 여러가지 일들을 시도해볼 수 있다.먼저 이 예제를 실행하려면 Python 라이브러리 중 Tensorflow, Seaborn, Pandas가 필요하다. 없거나 처음 들어봤다면 설치해준다.3.1. Data 준비 및 전처리import matplotlib.pyplot as pltimport pandas as pdimport seaborn as snsimport tensorflow as tffrom tensorflow import kerasfrom tensorflow.keras import layersTensorflow는 2.11.0을 썼다. Tensorflow 내 keras에서 자동차연비 데이터를 불러온다.dataset_path = keras.utils.get_file(\"auto-mpg.data\", \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")dataset_pathcolumn_names = ['MPG','Cylinders','Displacement','Horsepower','Weight', 'Acceleration', 'Model Year', 'Origin']raw_dataset = pd.read_csv(dataset_path, names=column_names, na_values = \"?\", comment='\\t', sep=\" \", skipinitialspace=True)dataset = raw_dataset.copy()dataset.tail() MPG Cylinders Displacement Horsepower Weight Acceleration Model Year Origin 393 27.0 4 140.0 86.0 2790.0 15.6 82 1 394 44.0 4 97.0 52.0 2130.0 24.6 82 2 395 32.0 4 135.0 84.0 2295.0 11.6 82 1 396 28.0 4 120.0 79.0 2625.0 18.6 82 1 397 31.0 4 119.0 82.0 2720.0 19.4 82 1 자동차 무게, 배기량, 실린더수, 마력, 연식 등의 특성에 따른 자동차의 연비가 테이블로 보인다. Dataset의 크기는 397개다.isna()는 결측값이 있는지 확인하는 함수다. 아래에서 보면 Horsepower부분에 6개의 결측치가 있는 것으로 보인다. 유저가 확인해서 채워넣을 수도 있겠지만 보통 이런부분은 그냥 통째로 날린다. Pandas에 dropna()라는 편한 함수가 있다.dataset.isna().sum()MPG 0Cylinders 0Displacement 0Horsepower 6Weight 0Acceleration 0Model Year 0Origin 0dtype: int64dataset = dataset.dropna()Origin은 제조 장소를 의미한다. 숫자 1, 2, 3으로 되어 있고 각각 미국, 유럽, 일본에 해당한다. 머신러닝에 넣어주는 데이터는 텍스트를 넣을 수는 없으므로 보통 이런 식으로 숫자로 표현한다.Origin은 categorical이라고 말하고 범주형이라고도 표현하며 연속적인 값을 가지지 않는다. 그래서 여기서는 미국, 유럽, 일본의 특성을 만들고 각 지역에 해당하면 1(True)을 넣고 아니면 0(False)을 넣는 식으로 데이터를 처리한다.데이터를 이런 식으로도 표현하는구나 하고 넘어가면 언젠가 쓰일 것 같다.origin = dataset.pop('Origin')dataset['USA'] = (origin == 1)*1.0dataset['Europe'] = (origin == 2)*1.0dataset['Japan'] = (origin == 3)*1.0Dataset을 훈련 세트와 테스트 세트로 분할한다.이 부분에서는 예제들마다 약간씩 설정이 다른데, training, validation, test set로 3개로 6/2/2로 분할하기도 한다. 분할 비율이나 데이터를 나누는 방법도 상황마다 다르기도 하다. 여기는 특별히 정답은 없는 것 같다.training은 말 그대로 인공신경망 모델내 파라미터를 피팅하는데 쓰이고 validation은 피팅된 파라미터를 가진 모델을 1차적으로 평가하는데 쓰인다. 이때 훈련성과가 별로 좋지 않다면 모델의 구조를 바꾸거나 새로운 특성을 추가하거나 하는 일들을 한다.마지막으로 training과 validation data에 적당한 fitness를 보여주는 모델을 최종 확인해 볼 때 test dataset을 활용한다. 어떠한 학습과정에서도 사용되지 않은 정말 깨끗한 데이터를 통해 확인한다는 의미 인 것 같다.어찌됬든, 여기서는 위의 설명에 있는 용어의 의미를 따르자면 test set는 없고, training과 validation 이렇게 두개의 세트로 데이터를 분할한다. 분할비율은 8/2다. 7/3이나 6/4도 뭐, 상관은 없을 것 같다.train_dataset = dataset.sample(frac=0.8,random_state=0)test_dataset = dataset.drop(train_dataset.index)밑에서는 seaborn 라이브러리에서 pairplot이라는 것으로 각 데이터 특성간의 영향을 확인한다. 두 특성들이 주고받는 영향은 파악할 수 있겠다. 복잡한 경우는 하나마나겠지만.sns.pairplot(train_dataset[[\"MPG\", \"Cylinders\", \"Displacement\", \"Weight\"]], diag_kind=\"kde\")데이터 특성들의 통계적 수치들도 확인한다. Pandas 라이브러리에서 describe() 함수를 쓴다. 요거는 그래도 쓸만해 보인다.train_stats = train_dataset.describe()train_stats.pop(\"MPG\")train_stats = train_stats.transpose()train_stats count mean std min 25% 50% 75% max Cylinders 314.0 5.477707 1.699788 3.0 4.00 4.0 8.00 8.0 Displacement 314.0 195.318471 104.331589 68.0 105.50 151.0 265.75 455.0 Horsepower 314.0 104.869427 38.096214 46.0 76.25 94.5 128.00 225.0 Weight 314.0 2990.251592 843.898596 1649.0 2256.50 2822.5 3608.00 5140.0 Acceleration 314.0 15.559236 2.789230 8.0 13.80 15.5 17.20 24.8 Model Year 314.0 75.898089 3.675642 70.0 73.00 76.0 79.00 82.0 USA 314.0 0.624204 0.485101 0.0 0.00 1.0 1.00 1.0 Europe 314.0 0.178344 0.383413 0.0 0.00 0.0 0.00 1.0 Japan 314.0 0.197452 0.398712 0.0 0.00 0.0 0.00 1.0 데이터에서 input 특성들과 output 특성(자동차 연비, MPG)을 분리한다. 딥러닝 모델 학습의 input으로 쓰일 dataset은 train_labels에 해당한다.train_labels = train_dataset.pop('MPG')test_labels = test_dataset.pop('MPG')데이터 정규화라는 것을 해본다. Dataset의 특성들(feature, 혹은 input)의 스케일과 범위가 다르면 normalization, 정규화를 하는 것이 추천된다고 한다. 특성을 정규화하지 않아도 모델은 수렴하지만 훈련시키기 어렵거나 입력단위에 의존하는 모델이 만들어 질 수 있다고 한다.정규화는 수학적으로는 평균 0, 표준편차 1을 갖는 분포를 갖도록 데이터세트를 조정하는 작업이다. Standard Normal Distribution, 표준정규분포로 만드는 작업으로 기억안나면 구글링 해보자.def norm(x): return (x - train_stats['mean']) / train_stats['std'] normed_train_data = norm(train_dataset)normed_test_data = norm(test_dataset)여기까지 데이터 전처리에 대해 정리해보자. 데이터를 준비한다. 데이터 내 결측치가 있는지 확인하고, 처리한다. (채워넣거나 싹 다 지운다.) 범주형 데이터의 경우 적절하게 수치형으로 변환해준다. 또는 데이터에 noise가 있는 경우 적당한 filter 통한 처리들도 고민해 볼 수 있다. 특성들의 통계치들을 확인하고, 전체 dataset을 정규화 해준다. 전체 데이터를 training, validation, test set로 적절히 분할해준다.3.2. 모델 빌드와 학습드디어 딥러닝 모델을 빌드하고 학습시켜보자.아래 코드에서 build_model은 node가 64개씩있는 hidden layer가 2개짜리 완전연결(densely connected) 인공신경망 모델을 만들어준다. hidden layer의 개수나 node의 개수같은 hyper parameter를 정하는 것은 해보면서 바꿔봐야 할거같다. 정답은 없다. 이거 도와주는 tensorflow 내 기능이 있는것 같은데 여기 한번 보자.활성함수로 relu가 쓰인다. 다른 포스팅에서 활성함수로 sigmoid를 사용한 다른 예제가 있다.keras.Sequential내에서 layer.Dense로 하나씩 layer를 추가한다. input layer는 딱히 정의하지 않으며 그 다음 부터 정의한다. output layer를 보면 활성함수가 정의되어 있지 않다. 이전 layer에서의 선형조합만으로 결과값을 가지겠다는 의미로 모든 범위의 실수가 결과값으로 나올 수 있다.optimizer는 RMSprop라는 것을 쓴다고 한다. 어떤 수학적인 기법인지 알아보면 좋겠지만, 일단 넘어가자. 인공신경망 모델 내 파라미터를 피팅하는 수학적인 알고리즘중 하나다. Gradient Descent방식을 쓸 수도 있다. SGD (Stochastic Gradient Descent)으로 옵션을 주면 된다. 예제는 여기 한 번 보자.loss로 ‘mse’를 지정해 준 것은 mean squared error를 의미하며 보통 오차함수로 많이쓰는 오차제곱평균이다. ‘mae’등 다른 옵션들도 가능하다. 커스터마이징된 오차함수를 정의하는 것도 가능한 것 같은데, 필요하면 각자 찾아보자.def build_model(): model = keras.Sequential([ layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]), layers.Dense(64, activation='relu'), layers.Dense(1) ]) optimizer = tf.keras.optimizers.RMSprop(0.001) model.compile(loss='mse', optimizer=optimizer, metrics=['mae', 'mse']) return modelmodel = build_model()파라미터의 개수를 보면서 내가 만드는 인공신경망 모델의 구조를 파악해 볼 수도 있다.준비한 dataset의 input 특성들의 종류는 9개다. 각 hidden layer별 node개수는 64개로 정했다. input layer와 첫 번째 hidden layer간의 파라미터 개수는 bias 64개와 weight 9 $\\times$ 64 = 576개로 총 640개다.첫 번재 hidden layer와 두 번째 hidden layer간의 파라미터 개수는 bias 64개와 weight 64 $\\times$ 64 = 4096개로 총 4160개다.마지막으로 두 번째 hidden layer와 마지막 ouput layer간의 파라미터 개수는 bias 1개와 weight 64 $\\times$ 1 = 64개로 총 65개다.model.summary()Model: \"sequential\"_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 64) 640 dense_1 (Dense) (None, 64) 4160 dense_2 (Dense) (None, 1) 65 =================================================================Total params: 4,865Trainable params: 4,865Non-trainable params: 0_________________________________________________________________모델 학습이 진행되고 있는지 확인하는 PrintDot이라는 콜백 함수를 정의한다. 학습회수 100번마다 점하나씩 프린트 한다고 한다. model.fit()으로 드디어 모델 학습을 수행하는 명령을 내린다. history라는 변수에 모델 학습 내역들이 기록된다.class PrintDot(keras.callbacks.Callback): def on_epoch_end(self, epoch, logs): if epoch % 100 == 0: print('') print('.', end='')EPOCHS = 1000history = model.fit( normed_train_data, train_labels, epochs=EPOCHS, validation_split = 0.2, verbose=0, callbacks=[PrintDot()])....................................................................................................import matplotlib.pyplot as pltdef plot_history(history): hist = pd.DataFrame(history.history) hist['epoch'] = history.epoch plt.figure(figsize=(8,12)) plt.subplot(2,1,1) plt.xlabel('Epoch') plt.ylabel('Mean Abs Error [MPG]') plt.plot(hist['epoch'], hist['mae'], label='Train Error') plt.plot(hist['epoch'], hist['val_mae'], label = 'Val Error') plt.ylim([0,5]) plt.legend() plt.subplot(2,1,2) plt.xlabel('Epoch') plt.ylabel('Mean Square Error [$MPG^2$]') plt.plot(hist['epoch'], hist['mse'], label='Train Error') plt.plot(hist['epoch'], hist['val_mse'], label = 'Val Error') plt.ylim([0,20]) plt.legend() plt.show()plot_history(history)파란색의 training dataset에 대한 오차는 Epoch, 학습 회수가 증가할 수록 줄어드는 것을 확인할 수 있다. 하지만 주황색의 validation datset에 대한 오차는 처음에는 좀 줄어들었다가 학습회수가 증가하면서 오히려 늘어난다.overfitting이라고도 부르는 이런 현상은 딥러닝 모델이 training dataset에만 맞도록 지나치게 fitting되면서 오히려 전체적인 dataset에 대한 general한 특성을 잃게되는 것을 의미한다.여러가지 해결법이 있겠지만 여기서는 early stopping, 학습을 조기종료하는 방식으로 이 문제를 해결해보려는 것 같다. 아래 keras.callbacks.EarlyStopping에서 validation dataset에 대한 오차를 계속 모니터하면서 10번이 지나도 이전보다 학습성능이 좋아지지 않으면 강제로 멈추겠다는 설정을 해준다.model = build_model()early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)history = model.fit(normed_train_data, train_labels, epochs=EPOCHS, validation_split = 0.2, verbose=0, callbacks=[early_stop, PrintDot()])plot_history(history)............................................................모델 빌드와 학습도 적당히 정리해보자. 모델을 정의한다. 여기서는 완전연결 인공신경망, hidden layer2개짜리 딥러닝 모델이다. Hyper parameter는 일단 해보고 바꿀 생각으로 정한다. optimizer방식과 오차함수 형태 등의 세부 옵션을 설정한다. 일단 학습 진행해 보면서 Overfitting 혹은 underfitting여부를 확인한다. overfitting이면 early stopping등의 콜백옵션들을 생각해본다. underfitting이면 특성의 종류를 늘리거나 모델의 구조를 더 복잡한 것으로 (파라미터를 늘리는 방향으로) 변경해 볼지 생각해본다.학습횟수에 따른 오차의 변화를 확인해보면서(learning curve를 본다고도 한다.) 고생해서라도 데이터를 더 추가해야할지, 모델을 어느 방향으로 변경할 지, 학습의 세부 옵션을 어떻게 변경할지 등등의 대응 방식들을 고민해야 하는 것 같다.자세한 건, 따로 정리할 기회가 있으면 좋겠지만, 나도 잘 모르기 때문에 강의나 책을 찾아보길 추천한다. Coursera 한 번 빡세게 듣거나 여러 유튜브 보면 좋을 것 같다.3.3. 모델 평가loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)print(\"테스트 세트의 평균 절대 오차: {:5.2f} MPG\".format(mae))3/3 - 0s - loss: 5.6281 - mae: 1.8268 - mse: 5.6281 - 19ms/epoch - 6ms/step테스트 세트의 평균 절대 오차: 1.83 MPGtest_predictions = model.predict(normed_test_data).flatten()plt.scatter(test_labels, test_predictions)plt.xlabel('True Values [MPG]')plt.ylabel('Predictions [MPG]')plt.axis('equal')plt.axis('square')plt.xlim([0,plt.xlim()[1]])plt.ylim([0,plt.ylim()[1]])_ = plt.plot([-100, 100], [-100, 100])3/3 [==============================] - 0s 977us/steperror = test_predictions - test_labelsplt.hist(error, bins = 25)plt.xlabel(\"Prediction Error [MPG]\")_ = plt.ylabel(\"Count\")학습이 완료된 모델을 이용해 예측하고 평가한다. 오차에 대한 히스토그램을 보면 어디 한 군데에 집중되지 않고 적당한 분포를 보인다.모델을 저장하고 불러오는 부분은 이 예제에 없어서 여기를 보면 될 것 같다.HDF5라는 표준 기본 저장 포맷이 있다고 한다. 아래 명령어로 저장하고 불러오면 될 것 같다.# 새로운 모델 객체를 만들고 훈련합니다model = create_model()model.fit(train_images, train_labels, epochs=5)# 전체 모델을 HDF5 파일로 저장합니다# '.h5' 확장자는 이 모델이 HDF5로 저장되었다는 것을 나타냅니다model.save('my_model.h5')new_model = tf.keras.models.load_model('my_model.h5')4. Application &amp; Limitations자동차 연비가 배기량이나 실린더수나 자동차 무게에 관련이 있을거 같긴 하지만 이를 연결해주는 어떤 이론식같은건 아마도 존재하지 않을 것 같다.이런 상황들은 종종있다. 주가같은게 환율이나 물가상승률, 기준금리, 회사의 성장률, 분기별 영업이익 뭐 이런 것들에 영향이 있을 것 같지만 어떤 식으로 표현하기는 애매하다. 어떤 석유화학 공장의 생산품이 반응기의 온도, 압력, 조성, 수위, 제립기 온도 뭐 이런 것들에 영향을 받는건 분명한데 이를 어떤 식으로 표현하기는 어렵다.이런 때 자동차 연비예측 예제를 꺼내들고 적당히 써보면, 잘만드는건 다른얘기지만 일단 뭐라도 만들어볼 수 있다.하지만 개인적으로는 인공신경망을 써봐도 현업에 빡세게 적용하기는 아직은 좀 이른감이 있다고 생각한다. 예측 정확도 적당한 모델은 얼마든지 만들어 볼 수 있다. 90% 넘는건 또 다른 얘기이긴 하지만. 근데 이걸 실제로 써먹는건 또 다른 차원의 얘기다. 이게 일이라는게, 그냥 재밌어보여서 했어요 할수있는 것도 아니고 일정 부분의 성과가 분명히 필요한데, 모델의 정확도는 training에 사용되지 않은 새로운 패턴에 대해 어디로 튈지 예측도 안되고 설명도 안되고 불안하다.개인적인 생각일 뿐이고 관련해서 많은 경험들이 있는것도 아니다. 다른 많은 잠재적인 활용분야가 있는데 내가 모르는 것일 수도 있다. 아직은 그냥 모르는 것들이 너무 많다.일단 여전히 뜨고있고 좋은 기술임에는 확실하니 나름 준비하고 공부는 한다. 취미로 해보기엔 아직은 재미있는 부분들이 있다.SummaryTensorflow을 써서 딥러닝 모델을 만들고 학습도 시켜봤다. 튜토리얼 그대로 베껴서 똑같이 따라하는거지만, 알아야 할 것들이 꽤 많았다. 그리고 모델을 학습시키는 것보다도 훨씬 더 많은 단계들이 필요했던 부분은 바로 데이터 전처리였다. 데이터가 좋아야 모델도 좋다. Garbage in, garbage out. 좋은 데이터를 만들어주는 최일선에 항상 감사한 마음을 가져야 한다.이런 얘기들 제일 마지막에 항상 하는 얘기는, 실은 딥러닝이니 머신러닝이니 인공신경망이니 이런거 보다도 적용하려는 분야에 대한 유저의 전문성이 가장 중요하다는 점이다. Input으로 넣는 특성이 output을 예측하는데 쓸데없는 것들로만 이루어져 있다면 딥러닝 할아버지가 오더라도 어떻게 해볼 수가 없이 시간낭비다. 이게 갖추어진 다음이 머신러닝 이론들, 수학, 코딩스러운 테크닉들 이런 것 같다.다음은 scikit-learn을 해보던지 아니면 coursera 강의 좀 정리해보려 한다.한해가 다 갔다. 그래도 한 달에 포스팅 하나씩은 썼다!!" }, { "title": "[Python] Automation - 2. COM Object", "url": "/posts/python-automation-COM/", "categories": "Python, Automation", "tags": "Python, Automation, COM Object, Aspen Plus", "date": "2022-11-09 19:55:00 +0800", "snippet": "좀 더 고급진 ‘자동화’라는 것을 해보자. 마찬가지로 Python으로 하고 window 운영체제 한정이다.조금만 더 자동화에 대해 파보면 나같은 프로그래밍 초심자는 이해하기 힘든 용어들이 쏟아져 나온다. 대표적으로 API가 그랬고 ProgID 등등 뭐 많았다. 아직 잘 모르겠지만 COM Object도 결국 API중에 하나인건가 싶다.다 알아두면 좋지만, 지금 당장은 딱 필요한것에 대해서만 짚고 넘어가자. 관심있으면 구글링도 해보고 자동화해보면서 관련한 error도 겪어보고 그러면 될 것 같다.확실히 공학용 계산이나 머신러닝에 비해서 자동화라는 주제는 개발자의 영역에 더 가까운 것 같다.1. IntroGUI 자동화는 앞 포스팅에서 다루었다.남은 하나인 COM Object에서 COM은 Component Object Model의 약자로 소프트웨어간 interact를 할 수 있도록 해주는 Microsoft에서 제공하는 어떤 표준 시스템 정도로 생각하면 된다. 나도 잘 몰라서 더 자세히 설명할 자신이 없다. 한 번 코드를 짜서 돌려보면 어떤 느낌인지는 알 수 있다.더 자세한 설명은 한국어로 여기, 영어로 여기를 보자. 영어로 된 부분을 두 번 추천한다. ‘이론적으로는’ 윈도우 내 모든 프로그램을 python으로 제어할 수 있는 것 같다.뭔가 모든 프로그램을 제어하기 위한 코드를 짤 수 있는 일반적인 방법이 없을까 찾아본 적이 있었는데, 아무래도 내 수준에서는 아직 어렵다.2. COM Object2.1. Win32com어찌됬든 이 COM Object를 python으로 사용할 수 있게 해주는 win32com이라는 모듈이 있다. GUI자동화에 비해 훨씬 할 수 있는 일들이 많고 잘 짜놓으면 오류도 적지만, 그만큼 알아야 할 것도 많고 어렵다. 좀 더 깊은 레벨의 자동화고 코드뿐만 아니라 제어해야 할 프로그램 자체에 대해서도 자세히 알아야 한다. 잘 안쓰이는 프로그램에 대한 자동화는 구글링해도 정보가 잘 없다.먼저 워드, 엑셀 또는 Outlook같은 프로그램을 간단하게 제어하는 예제들은 인터넷에 많다. 한 번쯤 따라해보면 감을 갖기 좋을 것 같다.2.2 Define Automation Task이번 포스팅에서는 공정시뮬레이션 프로그램인 Aspen Plus V11을 Python으로 제어해본다. 먼저 이곳과 이곳, 그리고 Matlab 커뮤니티에서 많은 힌트를 얻었다. 물론 모든 답은 구글에서 얻었다.공정시뮬레이션 프로그램 제어를 통해 해보려는 것은 일종의 반복계산, Case Study이다. 잘 되면 최적화에도 써봄직하다. ‘Teach Yourself the Basics of Aspen Plus’라는 책에 있는 예제를 약간 변경해서 해본다.섭씨 90도, 1bar 짜리 프로판 가스를 2개의 컴프레서를 이용해 최종적으로 8bar까지 가압하려고 한다. 첫 번째 컴프레서에서 배출되는 프로판 가스를 섭씨 90도로 다시 냉각시키는 열교환기가 있으며 열교환기를 거치면서 압력손실은 없다. 두 개의 컴프레서 모두 Polytropic Efficiency = 0.72, Mechanical Efficiency = 1 이다.이 때 첫 번째 컴프레서 배출 압력에 따라 두 컴프레서에서 사용하는 전력 합 경향성을 보고 가장 적은 일을 투입하는 경우는 언제인지 확인하려고 한다.2.3 Aspen Plus Control with win32com코드 짜서 돌려보자.import win32com.client as win32import osfrom scipy.optimize import minimizeimport matplotlib.pyplot as pltimport numpy as np사용할 모듈은 COM Object 사용을 위한 win32com, 나머지는 다른 포스팅에서도 많이 썼던 모듈들이다.filename = \"TwoCompressors.bkp\"AspenSimulation = win32.gencache.EnsureDispatch(\"Apwn.Document.37.0\")print(\"Aspen is Launching...\")AspenSimulation.InitFromArchive2(os.path.abspath(filename))print(f\"{filename} file is opening...\")AspenSimulation.Visible = TrueAspen is Launching...TwoCompressors.bkp file is opening...Aspen Plus 파일을 하나 만들었고 이름은 TwoCompressors.bkp라는 파일이다. EnsureDispatch()라는 함수 안의 인수로 제어하고자 하는 프로그램, application의 이름이 들어간다. 엑셀이면 ‘Excel.Application’ 이라고 써준다.그런데 정확한 이름을 찾는 것이 쉽지가 않다. ProgID라고 하던데, 누군가는 CLSID라고도 하고 어떤사람은 레지스트리에서 찾으라고 하고 어떤사람은 특정 프로그램을 활용하라고도 한다.나는 그냥 Aspen Plus 자동화 코드 구글에 돌아다니는 것에서 눈치껏 썼다. 끝에 37은 v11이고 38은 v12 뭐 이런식인 것 같았다. visible = True는 프로그램이 실행되는걸 보겠다는 뜻이다. 위까지 실행한 결과는 아래와 같다.work_compr1 = []work_compr2 = []pres = np.linspace(1.1, 7.9, num = 10)for i in range(len(pres)): AspenSimulation.Tree.FindNode(\"\\Data\\Blocks\\B1\\Input\\PRES\").Value = float(pres[i]) AspenSimulation.Run2() work_compr1.append(AspenSimulation.Tree.FindNode(\"\\Data\\Blocks\\B1\\Output\\WNET\").Value) work_compr2.append(AspenSimulation.Tree.FindNode(\"\\Data\\Blocks\\B3\\Output\\WNET\").Value) print(f\"At P_dis = {pres[i]}, total work for compressor = {work_compr1[i] + work_compr2[i]}\")plt.plot(pres, np.array(work_compr1)+np.array(work_compr2))At P_dis = 1.1, total work for compressor = 6.076464244At P_dis = 1.8555555555555556, total work for compressor = 5.80543241At P_dis = 2.6111111111111116, total work for compressor = 5.73576098At P_dis = 3.366666666666667, total work for compressor = 5.74051231At P_dis = 4.122222222222223, total work for compressor = 5.78007698At P_dis = 4.877777777777778, total work for compressor = 5.83782127At P_dis = 5.633333333333335, total work for compressor = 5.905763298999999At P_dis = 6.388888888888889, total work for compressor = 5.979641303999999At P_dis = 7.144444444444446, total work for compressor = 6.056999866At P_dis = 7.9, total work for compressor = 6.136400717[&lt;matplotlib.lines.Line2D at 0x1180c6d4700&gt;]첫번째 컴프레서의 배출압력을 바꿔가면서 두 개 컴프레서를 돌리는데 필요한 전력값을 확인한다. AspenSimulation.Tree.FindNode(\"\\Data\\Blocks\\B1\\Input\\PRES\") 이 부분에서 FindNode함수 안의 인수는 첫 번째 Compressor의 이름인 B1의 Input값으로 넣어주게 되어 있는 배출 압력이다. 이 부분을 반복적으로 변경해주면서 전력값을 받아낸다. 이런 일종의 주소, ID를 어떻게 알아내느냐가 대부분의 프로그램 자동화의 어려운 점이다. Aspen Plus에서는 프로그램안에서 확인 할 수 있다. Customize 탭에서 ‘Variable Explorer’에서 확인이 가능하다. 자세한 내용은 이 문서를 보자.Aspen Plus를 일종의 계산기로 쓰고 Python으로 반복적인 Task를 주는 것으로 이해하면 된다. 그림을 보면 대략 2.5 ~ 3 bar 사이에 최소값이 존재하는 것으로 보인다.아래 영상처럼 실행된다.def net_work(p): p = float(p) AspenSimulation.Tree.FindNode(\"\\Data\\Blocks\\B1\\Input\\PRES\").Value = p AspenSimulation.Run2() w1 = AspenSimulation.Tree.FindNode(\"\\Data\\Blocks\\B1\\Output\\WNET\").Value w2 = AspenSimulation.Tree.FindNode(\"\\Data\\Blocks\\B3\\Output\\WNET\").Value return w1+w2sol = minimize(net_work, 4)print(sol) fun: 5.77215423 hess_inv: array([[1]]) jac: array([0.67108864]) message: 'Desired error not necessarily achieved due to precision loss.' nfev: 48 nit: 0 njev: 18 status: 2 success: False x: array([4.])이번에는 아예 함수를 하나 짜서 scipy.optimize안에 minimize를 써서 최적화 해본다. 초기값은 4로 넣었다. 결과는 수렴하지 않고 ‘Desired error not necessarily achieved due to precision loss.’라는 오류를 띄우고 초기값 그대로의 값을 return한다. 초기값을 바꿔도 비슷한 결과가 나온다.구글검색을 해보면 stackoverflow에 이 문서가 적당해 보인다.뭔지는 모르지만 minimize함수안에 method를 Nelder-Mead로 지정해 보라고 한다. 아마도 minimize함수에서 default로 제공해주는 최적화 방식이 위 문제를 해결하는데 수치해석적으로 적당하지 않았나보다. 자세한건 scipy documentation에서 minimize에 대한 것들을 보면 될 것이다. Nelder-Mead방식에 대해서도 알아보면 좋겠지만, 일단 넘기자.아래와 같이 코드를 수정한다.def net_work(p): p = float(p) AspenSimulation.Tree.FindNode(\"\\Data\\Blocks\\B1\\Input\\PRES\").Value = p AspenSimulation.Run2() w1 = AspenSimulation.Tree.FindNode(\"\\Data\\Blocks\\B1\\Output\\WNET\").Value w2 = AspenSimulation.Tree.FindNode(\"\\Data\\Blocks\\B3\\Output\\WNET\").Value return w1+w2sol = minimize(net_work, 4, method = 'Nelder-Mead')print(sol)print(sol.x) final_simplex: (array([[2.9 ], [2.89990234]]), array([5.73175519, 5.7317834 ])) fun: 5.7317551899999994 message: 'Optimization terminated successfully.' nfev: 40 nit: 17 status: 0 success: True x: array([2.9])[2.9]결과는 2.9가 나온다. 위에서 뽑았던 그림에서도 얼추 확인할 수 있듯이 적당한 최적값으로 보인다.AspenSimulation.Close()위 코드는 프로그램을 정상적으로 종료해준다. 저장하고 종료하는 것도 가능하다. 궁금하면 찾아보자.2.4 이미 있는 기능이었음이 포스팅 준비하면서 알게 된 점이지만, Aspen Plus안에 이미 최적화 관련된 툴을 제공해준다. Model Analysis Tools 안에 Optimization기능이 이미 있다. 유튜브 영상은 여길 보자. 솔직히 이런 기능이 있는지는 몰랐지만 있을 거 같긴 했다.두 가지를 생각해 볼 수 있었다.첫 번째는 일단 내가 제어하고 싶어하는 프로그램이 있다면 그게 어떤 기능이 있고 어떤 프로그램인지 알아두는 것이 첫 번째 인 것 같다. 굳이 정상적으로 잘 제공해주는 기능을 구현하느라 win32com이라는 모듈을 공부하면서 돌아갈 필요는 없을 것 같다. 그리고 프로그램 자체에서 제공해주는 기능을 사용하는 것이 대부분의 경우에 오류도 적고 시간도 더 빠를 것이다.두 번째는 그렇다고 해서 위 코드들이나 알아본 것들이 의미가 없지는 않다는 점이다. 최적화 외에 무궁무진한 단순 반복적인 계산들을 잘만 쓰면 상당부분 해결해 줄 수 있다. 실제로 회사에서도 다른 일들로 사용했었는데 시간절약을 꽤나 할 수 있었다. 회사일들을 올리기는 어려워서 굳이 다른 예제를 찾아 정리해본 포스팅이었다.Summary뭔가 좀 더 자동화스러운 자동화 해봤다. 어려웠고 알아봐야 할 것도 너무 많았지만 코드자체는 그리 길지 않았다. 만약에 다른 프로그램을 제어해보라고 한다면 쉽지 않을 것 같다.먼저, 제어해야 할 프로그램 이름부터 (컴퓨터가 알아먹을 수 있는) 알아야 한다. ProgID.그리고, 제어해야 할 프로그램 안의 특정 기능이나 변수에 접근할 수 있는 ID같은 것을 알아야 했다. Aspen Plus에서는 variable explorer로 알았지만, 다른 프로그램에서는 구글링아니면 혼자 알아보기는 힘들 수 있다. 그리고 ComObject에서 제공해주는 함수들 좀 알고나서 적절히 조합해 주면 됬다.자동화라는 걸 몇 번 해보다보면 생각보다 선택의 영역이라는 생각이 든다. 모든일을 오류없이 자동화 할 수는 없다. 그리고 자동화라는걸 적용하기에는 내가 투입해야 할 에너지와 시간이 적지 않다. 자동화 코드를 짜고 적용을 하더라도 결국 최종 확인과 코드의 유지보수도 나의 몫이다. 컴퓨터 화면 휙휙 움직이는거 보면 기분은 좀 좋을지 몰라도 모든 일을 이렇게 할 수는 없고 코드 외적으로 현실적으로 고려해야 할 부분이 있다. 잘만 쓰면 파워풀하지만 만능은 아니다." }, { "title": "[Python] Automation - 1. GUI", "url": "/posts/python-automation-gui/", "categories": "Python, Automation", "tags": "Python, Automation, GUI, pyautogui", "date": "2022-10-19 19:55:00 +0800", "snippet": "‘자동화’라는 것을 해보자. Python으로 하고 window 운영체제 한정이다.보통 자동화라고 하면 단순 반복스러운 일들을 클릭 한번으로 화면이 휙휙 바뀌면서 막 컴퓨터가 알아서 해주는 그런 움짤들이 떠오른다. 좀 있어보이게 그런 것도 한번 해보자.따지고보면 컴퓨터한테 어떤 일을 시킨다는 입장에서 정도의 차이야 있겠지만 모든 코딩은 자동화라고도 할수있지 않나 싶다. 구글링 해보니 자동화의 의미에 대해 써놓은 글도 있다.1. Intro먼저 다른 포스팅과는 다르게 이번에 다룰 내용들에는 컴퓨터 용어가 좀 많이 나온다. 본격적인 자동화를 시작하기 전에 몇 가지 용어들을 정리해보자. 구글링과 눈치로 대충 느낌만 알고있는 그런 용어들이라는 점 감안해주길 바란다. 다룰 용어에 대한 자세한 정의가 필요하다면 따로 찾아보기를 추천한다.이번 포스팅에서 다룰 Python을 활용한 자동화는 크게 2가지로 분류할 수 있다. 하나는 GUI라는 것을 제어하는 것이고 다른 하나는 COM Object를 활용하는 것이다. 여기서는 GUI 자동화를 해본다.GUI는 Graphical User Interface의 약자다. 관련한 자세한 자료는 여기와 여기를 읽어보자. 간단하게 사람이 컴퓨터로 일을 할 때 마우스와 키보드를 이용해서 화면에 뜬 아이콘이나 메뉴를 클릭하고 문자를 입력하고 하는 것을 생각하면 된다. GUI자동화는 이러한 마우스와 키보드의 동작을 미리 짜놓은 코드로 제어하는 것이다. 이러한 제어에 pyautogui라는 python 모듈이 쓰인다.처음 이 모듈의 존재를 알았을 때 바로 게임매크로를 만들었었다. 핸드폰 게임이었는데 Android 에뮬레이터로 컴퓨터에서 주로 했었다. 게임 내 각 메뉴 이미지를 하나하나 캡쳐해서 저장해놓고 특정조건이 만족되면 특정 메뉴를 클릭하고 다음 단계를 진행하는 식의 코드였다. 게임이 자꾸 업데이트 되면서 메뉴 모양이 바뀌어 귀찮아서 그만뒀었지만 한동안 잘 가지고 놀았었다. 그래도 게임매크로는 왠만하면 만들지 말자..경험상 원하는 자동화 기능구현이 쉽지만 얕은 수준에서의 자동화다. 못하는 읻도 많고 오류도 많다고 까지 말하고 싶기도 한데, 이건 코드를 짜는 사람 능력따라서도 다른거라서 쉽게 단정짓지는 못하겠다.2. GUI미리 짜놓은 코드대로 python이 키보드와 마우스를 움직여주는 GUI 자동화를 해보자. 슉슉 움직이는 움짤좀 만들어본다.2.1. Pyautoguipyautogui라는 모듈이 필요하다. 없다면 터미널에서 아래명령어로 설치해준다. 터미널이 뭔지 모듈 설치는 뭔지는 여기를 참고하자.pip install pyautoguipyautogui 공식 문서보다는 여기가 좀 더 설명이 쉬운거 같다. 여기코드를 배껴다 쓴다. pyautogui로는 아래 일들을 수행할 수 있다. screen(모니터) size 확인 마우스움직임 제어 (움직임, 드래그, 클릭, 더블클릭, 우클릭, 스크롤 등) 스크린샷 및 RGB 정보 확인 모니터 화면내 특정 이미지 위치 확인 키보드조작 (특정 문자열 작성, 특정 문자 입력 전송)대충 열거하자면 이정도다.어찌됬든 여기서는 그림판을 실행하고 특정문양을 그리는 일을 pyautogui로 해본다. 전혀 의미없어 보이는 일이지만 이런거 조금 연습해보면 내가 하고싶은 진짜 자동화 해볼 수 있다.2.2. Step by Step하나씩 단계를 생각해보자. 여기서 하려는건 그림판에 특정 그림을 그리는 작업을 하려고 한다. 아래 단계대로 생각해보자. 그림판 프로그램 실행 원하는 모양 그리기그림판 프로그램 실행부분만 더 세분화 해보자. 그림판 프로그램 실행 윈도우키를 누른다 그림판 프로그램을 찾는다 (시작화면에서 paint라고 친다) 그림판 프로그램을 실행한다 (Enter키를 누른다) 사람이 마우스와 키보드를 동작하는 순서 그대로를 차례대로 코드로 구현하면 된다. GUI 제어에서 조금 조심해야 할 점은 하나의 동작과 동작 사이에 적당한 시간차이를 두어야 한다는 점이다. 우리가 어떤 프로그램을 실행하라고 마우스로 더블클릭을 하든 엔터를 누르든간에 바로 열리는 것은 아니고 아주 약간의 지연시간이 있다. 이때 쓰는 코드는 time 모듈의 sleep()이다.아래 코드를 실행하면 그림판이 실행된다.import pyautogui as pgimport timetime.sleep(1)pg.press(\"winleft\")time.sleep(1)pg.typewrite(\"paint\")time.sleep(1)pg.press(\"Enter\")time.sleep(1)은 여기에서 1초를 쉬라는 얘기다. 각 단계당 시간을 줄일수 있지만 너무 줄이면 다음단계가 실행되기 위한 화면이 뜨기전에 명령이 전달되면서 내가 원하는 동작이 구현되지 않을 수 있다. 1초정도는 기다리자.paint는 그림판 프로그램의 영어 이름이다. 한글을 입력하는 방법은 좀 복잡한 것 같다. 여기를 참고하자.마지막에는 시작메뉴에서 그림판이 뜬 상태에서 Enter를 누른다. 이후에는 그림판 프로그램이 화면에 뜰 것이다.남은 일은 모니터 화면에 실행된 그림판 프로그램에 특정 모양을 마우스로 그리는 것이다. 아래 단계를 생각해본다. 원하는 모양 그리기 모니터에 실행된 그림판 프로그램 빈 화면에 마우스를 댄다 원하는 모양을 그리도록 드래그 한다 코드는 합쳐서 아래처럼 쓸 수 있다.time.sleep(1)pg.press(\"winleft\")time.sleep(1)pg.typewrite(\"paint\")time.sleep(1)pg.press(\"Enter\")time.sleep(1)pg.click(50, 620)distance = 300change = 20while distance &gt; 0: pg.drag(distance, 0, duration=0.2) distance = distance - change pg.drag(0, distance, duration=0.2) pg.drag(-distance, 0, duration=0.2) distance = distance - change pg.drag(0, -distance, duration=0.2)pg.click(50, 620)은 모니터 화면 (50, 620)위치를 클릭하게 한다. 그림판 프로그램이 모니터 화면에 뜨는 위치는 직전에 실행했을때 종료한 위치에서 뜨는것으로 보인다. 실은 저 부분을 좀 더 오류없이 처리하려면 모니터 화면에서 그림판 프로그램의 특정메뉴 아이콘을 캡쳐해둬서 그 위치를 인식하도록 하는 것이 더 좋을 것이다. pyautogui.locateOnScreen() 함수를 쓰면 된다. 여기서는 굳이 여기까지 하진 않겠다.실행 결과는 아래처럼 움직인다. (영상이 실행안되면 F5로 새로고침 한다.)간단한 동작을 위한 간단한 코드. 알아보고 생각할 건 조금 있었지만 어렵지 않게 구현됬다. pyautogui와 적당한 조건문, 반복문의 조합으로 기상천외한 일들 할 수 있을 것이다.SummaryGUI 자동화는 일단 직관적이다. 내가 평소에 마우스 키보드 조작하던 것을 하나씩 단계별로 코드작성하면 됬다. 각 동작마다 조금씩 기다리도록 주의하면 된다. 화면이 변하는것을 보면 괜히 뭐 있어보이고 재밌기도 하다.그런데, 이정도를 자동화 혹은 RPA라고 말하기는 뭔가 애매하다. 자동화가 아닌건 아닌데 좀 더 많은 것을 오류없이 일을 처리하고 싶다.예를 들면 컴퓨터가 성능이 좀 떨어져서 혹은 인터넷환경처럼 특정 페이지가 로딩되는데 오래걸리거나 프로그램 실행중 오류가 생기는 경우를 생각해보자. 사람이라면 조금 기다리던지 다시 프로그램을 실행할 것이다. 하지만 GUI자동화 방식은 코드 전체가 내 의도와 다르게 작동할 가능성이 매우 높다. 뭔가 컴퓨터와 더 깊은 대화가 필요하다. 다음번에는 COM Object 써보자." }, { "title": "[Eng. Calc.] Symbolic Computation", "url": "/posts/Engineering-Calculation4/", "categories": "Engineering Calculation, Symbolic Computation", "tags": "Engineering Calculation, Python, Symbolic Computation", "date": "2022-10-17 19:55:00 +0800", "snippet": "이번 공학용계산 포스팅의 주제는 Symbolic Computation, 기호 계산(?)이다. 개인적으로는 보통 그냥 심볼릭한 계산이라고 부른다. Symbolic Calculation이라고 검색해보려 했는데 구글에서 자동완성으로 Computation을 추천한다. 고민없이 Computation을 쓰자.Symbolic한 계산이란 것은 인간이 수행하는 문자간의 연산, 미분, 적분, 라플라스 변환, 일반항에 대한 Summation 등을 생각하면 된다. $x$를 미분하면 1, 적분하면 ${1 \\over 2}x^2 +C$ 또는 일반항의 합인 $\\sum_{k=1}^n k = {1 \\over 2}n(n+1)$ 이런 계산들을 컴퓨터한테 시키는 식이다. 계산 결과 또한 문자에 대한 식으로 나오게 된다. 더 자세한건 여기를 한번 읽어보자.물론 컴퓨터는 이런 연산을 인간처럼 이해하고 하지는 않을 것이다. 사람이 일정한 형태로 짜놓은 명령어대로 움직일 것이다. 다만 이러한 일련의 연산들을 통해 사람이 하는 실수를 줄이거나 귀찮은 작업들을 컴퓨터가 해주는 장점들이 있을 수 있다.일하면서는 쓸일이 없었지만 학생 시절에 많이 썼던 기능들이라 따로 소개한다. 알아둬서 나쁠건 없다. 이런게 있구나 하는 정도로 가볍게 넘겨도 좋다.1. Intro먼저 기억해야 할 점 하나는 컴퓨터를 활용한 Symbolic계산은 Numerical한 접근에 비해서 느리다는 점이다. 그럼에도 Symbolic한 계산을 사용하는 이유는 아주복잡한 식들을 ‘입력’하는데 사람이 일일이 유도하고 타이핑하는 작업을 피하기 위해서다. 물론 다른 이유도 많을 수 있지만 나는 그랬다.예를 들어 아주 복잡한 식이 있고(초월함수, 다항함수의 조합에 분모에도 변수가 있지만 미분가능한) 이 식의 다양한 편미분식들을 사용해서 방정식 풀이를 한다고 가정해보자. 미분식들 하나하나 유도하고 입력하다가 지치게 된다. 운이좋아 한방에 잘 입력했으면 모르겠는데 (내 경험상) 항상 실수가 존재하고 이런경우 어디가 문제인지 찾기도 어렵다.이럴 때 근원이 되는 복잡한 식 하나만 신경써서 잘 입력해주고 나머지는 Symbolic한 계산을 통해 미분을 취해준다. 이후에 Numerical한 계산을 할 수 있게 변환해주고 필요한 계산을 하면 된다.여담으로 적분 연산 결과는 해석해가 존재하지 않는 경우도 있고 적분상수 처리나 정적분의 경우 발산등 미분에 비해 일반적으로 신경쓰기 어려운 부분들이 있어 Symbolic한 계산으로 적분은 추천하지 않는다. 가능하면 제한을 많이두고 Numerical한 적분을 하길 바란다.일단은 아래와 같이 간단한 예제를 한번 보자.from sympy import symbols, lambdify, diffimport numpy as npx = symbols(\"x\")s = x**2 + x + 1dsdx = diff(s, x)dsdx_nu = lambdify(x, dsdx)print(f\"s = {s.doit()}\")print(f\"dsdx = {dsdx.doit()}\")print(f\"dsdx(x=1) = {dsdx_nu(1)}\")s = x**2 + x + 1dsdx = 2*x + 1dsdx(x=1) = 3sympy라는 Symbolic계산 전용 모듈을 사용한다. s라는 변수에 $x$에 대한 2차식을 정의했다. 그리고 diff명령어로 미분을 취하고 dsdx라는 변수에 지정해준다 lambdify명령어로 numerical한 계산을 위해 $x$에 대한 함수로 만들어 dsdx_nu라는 이름을 주었다. 계산결과는 생각한대로 잘 나온것을 볼 수 있다.아래 조금 더 복잡한 예제도 한번 보자. 관련 코드는 여기서 따왔다.from sympy import Sum, symbols, Indexed, lambdify, diffimport numpy as npa, x, i = symbols(\"a x i\")s = Sum(Indexed('x',i),(i,0,3))print(f\"s = {s.doit()}\")s2 = Sum(Indexed('x',i),(i,1,3))print(f\"s2 = {s2.doit()}\")s3 = Sum((a**i)*Indexed('x', i), (i, 0, 3))print(f\"s3 = {s3.doit()}\")f = lambdify(x, s)f2 = lambdify(x, s2)f3 = lambdify(x, s3)print(type(f))print(type(f2))print(type(f3))b = np.array([1, 2, 3, 4])df3 = diff(f3(b), a)print(f\"f(b) = {f(b)}\")print(f\"f2(b) = {f2(b)}\")print(f\"f3(b) = {f3(b)}\")print(f\"df3(b)/da = {df3}\")print(type(f(b)))print(type(f2(b)))print(type(f3(b)))print(type(df3))ff3 = lambdify(a, df3)print(ff3(1))s = x[0] + x[1] + x[2] + x[3]s2 = x[1] + x[2] + x[3]s3 = a**3*x[3] + a**2*x[2] + a*x[1] + x[0]&lt;class 'function'&gt;&lt;class 'function'&gt;&lt;class 'function'&gt;f(b) = 10f2(b) = 9f3(b) = 4*a**3 + 3*a**2 + 2*a + 1df3(b)/da = 12*a**2 + 6*a + 2&lt;class 'numpy.int32'&gt;&lt;class 'numpy.int32'&gt;&lt;class 'sympy.core.add.Add'&gt;&lt;class 'sympy.core.add.Add'&gt;20indexed라는 기능을 사용했는데 일반항처럼 idnex가 있는 계수들을 처리할 때 편해보인다. 먼저 사용할 symbol들을 정의하고 식을 정의했다. 아래 코드는 다음 식을 의미한다.\\[s = \\sum_{i=0}^3 x_i\\]s = Sum(Indexed('x',i),(i,0,3))이를 lambdify로 x에 대한 함수를 만들어 f라고 했다. type을 확인해보면 function이라고 한다. numpy list b를 x에 대응해주면 함수 f는 1 + 2 + 3 + 4의 값을 return 한다. 미안하지만 다른 부분은 알아서 눈치껏 이해해보자. 보다 자세한 설명은 Sympy Documents를 확인해본다. 너무 무책임해 보이지만,, 어쩔 수 없다. 분량 조절해야된다.일단 식을 정의하고 필요한 경우 미분이나 추가적인 심볼릭한 연산을 해준 다음에 수치해석적인 처리가 필요하게 되면 lambdify를 통해 type을 변환해주어 방정식 풀이나 숫자 대입을 하는 방식이다. 이 흐름만 기억해두자.딱 여기까지만 봐도 좋다. 이정도만 봐도 필요할 때 언제든 갖다 쓸 수 있다.이 밑의 예제는 훨씬 더 복잡한 상황에서 sympy를 적용해 보았다. 일단 코드를 짜고나니 아까워서 정리해보긴 했는데 아무리 생각해도 조금 과한것 같다.2. Symbolic Computation for Phase Equilibrium2.1. Helmholtz Free Energy ModelSymbolic Computation 예제로 이번 포스팅에서 고른 메인 주제는 Helmholtz Free Energy Model을 이용한 상평형 계산이다. 겪어본 가장 복잡한 식을 활용한 symbolic계산과 방정식 풀이의 조합 예제이다.Cyclohexane이라는 물질에 대한 열역학 모델 식이며 원 논문은 여기에서 받을 수 있다.식이 매우 복잡한 만큼 현존하는 모델 중에 정확도가 가장 높고 예측가능한 온도, 압력 범위가 가장 넓다. Aspen+ 같은 공정모사 프로그램에서 REFPROP라는 열역학 모델로 사용할 수 있다. 단, 적용할 수 있는 물질들의 종류가 조금 제한적이다. 모델이나 계산 패키지에 대해 궁금하다면 여기를 읽어보자.Helmholtz Free Energy식은 아래와 같이 주어진다.\\[{a(\\rho, T) \\over RT} = \\alpha(\\delta, \\tau) = \\alpha^0(\\delta, \\tau) + \\alpha^r(\\delta, \\tau) \\qquad (1)\\]$\\delta$는 Reduced density로 $\\rho / \\rho_c$를 의미하며 $\\tau$는 inverse reduced temperature이며 $T_c/T$로 정의된다. ($\\rho$는 밀도, $T$는 온도이고 밑첨자 c는 critical state를 의미한다.) R은 기체상수, $a$는 Helmholtz energy, $\\alpha$는 reduced Helmholtz energy, 위첨자 0와 r은 각각 ideal과 residual property를 의미한다.식(1)의 ideal term, $\\alpha^0$는 아래와 같이 주어져 있다.\\[\\alpha^0 = a_1 + a_2\\tau + \\ln\\delta + (c_0 - 1)\\ln\\tau + \\sum_{k=1}^4 v_k\\ln\\left(1-\\exp(-u_k \\tau / T_c)\\right) \\qquad (2)\\]파라미터의 값은 $c_0$ = 4, $u_1$ = 773, $u_2$ = 941, $u_3$ = 2185, $u_4$ = 4495, $v_1$ = 0.83775, $v_2$ = 16.036, $v_3$ = 24.636, $v_4$ = 7.1715, $a_1$ = 0.9891140602, $a_2$ = 1.6359660572 이라고 한다.남아있는 residual term, $\\alpha^r$은 아래와 같이 정의된다.\\[\\begin{align*}\\alpha^r(\\delta, \\tau) &amp;= \\sum_{i=1}^5 n_i \\delta^{d_i} \\tau^{t_i} + \\sum_{i=6}^{10} n_i \\delta^{d_i} \\tau^{t_i} \\exp (-\\delta^{l_i})\\\\&amp; +\\sum_{i=11}^{20}n_i \\delta^{d_i} \\tau^{t_i} \\exp(-\\eta_i(\\delta - \\varepsilon_i)^2 - \\beta_i(\\tau - \\gamma_i)^2) \\qquad (3)\\end{align*}\\]이 경우 파라미터는 아래 테이블로 주어진다. $i$ $n_i$ $t_i$ $d_i$ $l_i$ $\\eta_i$ $\\beta_i$ $\\gamma_i$ $\\varepsilon_i$ 1 0.05483581 1.00 4 0 0 0 0 0 2 1.607734 0.37 1 0 0 0 0 0 3 -2.375928 0.79 1 0 0 0 0 0 4 -0.5137709 1.075 2 0 0 0 0 0 5 0.1858417 0.37 3 0 0 0 0 0 6 -0.9007515 2.4 1 2 0 0 0 0 7 -0.5628776 2.5 3 2 0 0 0 0 8 0.2903717 0.5 2 1 0 0 0 0 9 -0.3279141 3 2 2 0 0 0 0 10 -0.03177644 1.06 7 1 0 0 0 0 11 0.8668676 1.6 1 0 0.99 0.38 0.65 0.73 12 -0.1962725 0.37 1 0 1.43 4.2 0.63 0.75 13 -0.1425992 1.33 3 0 0.97 1.2 1.14 0.48 14 0.004197016 2.5 3 0 1.93 0.9 0.09 2.32 15 0.1776584 0.9 2 0 0.92 1.2 0.56 0.2 16 -0.04433903 0.5 2 0 1.27 2.6 0.4 1.33 17 -0.03861246 0.73 3 0 0.87 5.3 1.01 0.68 18 0.07399692 0.2 2 0 0.82 4.4 0.45 1.11 19 0.02036006 1.5 3 0 1.4 4.2 0.85 1.47 20 0.00272825 1.5 2 0 3 25 0.86 0.99 대략 160개 정도의 파라미터가 전체 모델을 정의하는데 쓰인다.Helmholtz Free Energy에 대한 적당한 미분식들을 통해 여러가지 열역학 물성에 대한 식을 뽑아낼 수 있으며 상평형 계산에 필요한 식들도 여기서 나오게 된다. 이 포스팅에서 사용할 물성들은 아래 식으로 구할 수 있다.\\[Z = {p \\over \\rho R T} = 1 + \\delta \\left( {\\partial \\alpha^r \\over \\partial \\delta}\\right)_{\\tau} \\qquad (4)\\]\\[p = R T \\rho_c \\delta \\left[1 + \\left( {\\partial \\alpha^r \\over \\partial \\delta}\\right)_{\\tau} \\right] \\qquad (5)\\]\\[{G \\over {RT}}= 1 + \\alpha^0 + \\alpha^r + \\delta \\left( {\\partial \\alpha^r \\over \\partial \\delta}\\right)_{\\tau} \\qquad (6)\\]물론 위 식들 이외에도 다양한 열역학 관계식들을 활용해서 Heat Capacity, Joule-Thompson Coefficient, Speed of Sound 등등 여러 물성들을 계산할 수도 있다. 일단 여기서는 위 식(4~6)만 사용해본다. 궁금하면 직접 해보자.정리하자면 식(1)로 정리된 Helmholtz Energy를 식(4~6)에서 정의된 편미분 적용을 통해 최종 물성식을 뽑아내고 상평형 계산을 수행해 볼 것이다. 당연히 미분에는 sympy를 쓰고 방정식 풀이를 위해 lambdify와 공학용계산 모듈인 scipy를 쓸 것이다.상평형 계산은 임의의 온도 $\\tau$에서 아래 두개의 방정식(equal pressure, equal gibbs energy)을 만족시키는 두 개의 density $\\delta’$, $\\delta’‘$를 구하게 한다.\\[p(\\tau, \\delta') = p(\\tau, \\delta'') \\qquad (7)\\]\\[G(\\tau, \\delta') = G(\\tau, \\delta'') \\qquad (8)\\]2.2. Python Code파라미터가 160개 가량 되는 모델식을 코드로 입력해야 하는 것 뿐만 아니라 이 식을 손으로 미분까지해서 또 새로 코드를 작성한다는 것은 생산성도 떨어질 뿐더러 실수할 가능성도 너무 많다. (물론, 이런일을 해야 할 상황이 인생에서 많지는 않다.)어찌됬든 sympy를 이용하는 방식이 이러한 상황에서 그나마 가장 효율적이고 실수도 덜한 방법의 조합이라고 생각한다.아래 코드도 몇 번이나 실수한 끝에 겨우 성공한 것이기도 하다. 이 모듈이 없었다면, 시도조차 안했을 것 같다.import sympy as spimport numpy as npfrom scipy.optimize import fsolve사용할 모듈은 sympy가 메인이다. 다른모듈은 다른 포스팅에서 이미 다루었다.delta, tau, i, v, u, n, t, d, l, eta, beta, gamma, epsilon = sp.symbols(\"delta tau i v u n t d l eta beta gamma epsilon\")vi = sp.Indexed('v', i)ui = sp.Indexed('u', i)ni = sp.Indexed('n', i)ti = sp.Indexed('t', i)di = sp.Indexed('d', i)li = sp.Indexed('l', i)etai = sp.Indexed('eta', i)betai = sp.Indexed('beta', i)gammai = sp.Indexed('gamma', i)epsiloni = sp.Indexed('epsilon', i)Tc = 553.6 # kelvinrhoc = 3.224 # mol/dm-3R = 0.008314 # MPa dm-3 / mol KMw = 84.15948 # g mol-1# &lt;------------------------------------------- Define Ideal Helmholtz Energy ------------------------------------------------&gt; #a1 = 0.9891140602a2 = 1.6359660572c0 = 4u_value = np.array([0, 773, 941, 2185, 4495])v_value = np.array([0, 0.83775, 16.036, 24.636, 7.1715])ideal = a1 + a2*tau + sp.log(delta) + (c0 - 1)*sp.log(tau) + sp.Sum(vi*sp.log(1-sp.exp(-ui*tau/Tc)), (i, 1, 4))# &lt;------------------------------------------- Define Residual Helmholtz Energy ------------------------------------------------&gt; #n_value = np.array([0, 0.05483581, 1.607734, -2.375928, -0.5137709, 0.1858417, -0.9007515, -0.5628776, 0.2903717, -0.3279141, -0.03177644, 0.8668676, -0.1962725, -0.1425992, 0.004197016, 0.1776584, -0.04433903, -0.03861246, 0.07399692, 0.02036006, 0.00272825])t_value = np.array([0, 1, 0.37, 0.79, 1.075, 0.37, 2.4, 2.5, 0.5, 3, 1.06, 1.6, 0.37, 1.33, 2.5, 0.9, 0.5, 0.73, 0.2, 1.5, 1.5])d_value = np.array([0, 4, 1, 1, 2, 3, 1, 3, 2, 2, 7, 1, 1, 3, 3, 2, 2, 3, 2, 3, 2])l_value = np.array([0, 0, 0, 0, 0, 0, 2, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])eta_value = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.99, 1.43, 0.97, 1.93, 0.92, 1.27, 0.87, 0.82, 1.4, 3])beta_value = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.38, 4.2, 1.2, 0.9, 1.2, 2.6, 5.3, 4.4, 4.2, 25])gamma_value = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.65, 0.63, 1.14, 0.09, 0.56, 0.4, 1.01, 0.45, 0.85, 0.86])epsilon_value = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.73, 0.75, 0.48, 2.32, 0.2, 1.33, 0.68, 1.11, 1.47, 0.99])residual1 = sp.Sum(ni*(delta**di)*(tau**ti),(i, 1, 5))residual2 = sp.Sum(ni*(delta**di)*(tau**ti)*sp.exp(-delta**li), (i, 6, 10))residual3 = sp.Sum(ni*(delta**di)*(tau**ti)*sp.exp(-etai*((delta-epsiloni)**2)-betai*((tau - gammai)**2)), (i, 11, 20))residual = residual1 + residual2 + residual3# &lt;---------------------------- Differentiate Helmholtz Energy to obtain specific property ----------------------------------&gt; #Z = 1 + delta*sp.diff(residual, delta)P = Z*R*(Tc/tau)*delta*rhocG = 1 + ideal + residual + delta*sp.diff(residual, delta)사용해야 할 symbol들의 종류도 워낙 많아서 좀 헷갈릴 수 있다. Intro에서 다루었던 예제를 생각해 보면 그래도 금방 이해될 것이다. 최종적으로 식(1)에 해당하는 Helmholtz Free Energy를 residual이라는 변수로 정의했고 식(4~6)에 해당하는 미분을 sympy.diff명령어 한줄로 끝낸다.식(1)만 신경써서 작성하면 된다. 여지없이 여기에서 계속 실수를 했었다.def P_num(delta_n, tau_n): P_inter = sp.lambdify((v, u, n, t, d, l, eta, beta, gamma, epsilon, delta, tau), P) return P_inter(v_value, u_value, n_value, t_value, d_value, l_value, eta_value, beta_value, gamma_value, epsilon_value, delta_n, tau_n)def G_num(delta_n, tau_n): G_inter = sp.lambdify((v, u, n, t, d, l, eta, beta, gamma, epsilon, delta, tau), G) return G_inter(v_value, u_value, n_value, t_value, d_value, l_value, eta_value, beta_value, gamma_value, epsilon_value, delta_n, tau_n)def equil_eqn(x): delta_a = x[0] delta_b = x[1] equal_pres = P_num(delta_a, tau_set) - P_num(delta_b, tau_set) equal_G = G_num(delta_a, tau_set) - G_num(delta_b, tau_set) return [equal_pres, equal_G]상평형 계산에 사용할 함수들을 정의한다. 수치해석 계산을 위해 sympy식들을 lambdify를 통해 일종의 형태(type)변환을 해준다. equil_eqn함수가 fsolve를 적용해 풀어줘야 할 방정식에 해당한다.den_a = 9.37den_b = 0.0027delta_a = den_a/rhocdelta_b = den_b/rhoctemp_set = 283tau_set = Tc/temp_setroot_eq = fsolve(equil_eqn, [delta_a, delta_b])root = []root.append(root_eq)temp_list = [temp_set]temp_start = temp_setstep = 10for i in range(1, int((round(Tc)-temp_start)/step) + 2, 1): temp_set = temp_set + step if temp_set &gt;= Tc: break temp_list.append(temp_set) tau_set = Tc/temp_set root_eq = fsolve(equil_eqn, root[i-1]) root.append(root_eq)이전 포스팅에서와 비슷한 느낌으로 이전계산에서 얻은 답을 다음번 계산의 초기값으로 넣어주는 방식이다.den_a_list = np.array(root)[:,0]*rhocden_b_list = np.array(root)[:,1]*rhocpres_list = P_num(np.array(root)[:,0],Tc/np.array(temp_list))import matplotlib.pyplot as plttemp_data = np.array([283,313,343,373,403,433,463,493,523,553])den_a_data = np.array([9.3644,9.0275,8.6788,8.314,7.927,7.5071,7.0341,6.4677,5.7026,3.721])den_b_data = np.array([0.002687,0.0095389,0.026088,0.059385,0.11875,0.21701,0.37422,0.62851,1.08,2.7321])pres_data = np.array([0.0062923,0.024494,0.072215,0.17403,0.3613,0.67039,1.1417,1.8204,2.7605,4.0495])plt.title('Saturated Density')plt.plot(den_a_list, temp_list)plt.plot(den_b_list, temp_list)plt.scatter(den_a_data, temp_data)plt.scatter(den_b_data, temp_data)plt.xlabel('Molar Density(mol/l)')plt.ylabel('Temperature(K)')plt.show()plt.title('Vapor Pressure')plt.plot(temp_list, pres_list)plt.scatter(temp_data, pres_data)plt.xlabel('Tmmperature(K)')plt.ylabel('Pressure(MPa')plt.show()Data는 NIST Webbook에서 따왔고 temp_data, den_a_data, den_b_data, pres_data에 해당한다. 결과는 매우 정확하다. (애초에 NIST Webbook은 이 모델의 결과를 알려준다.)3. Advanced Algorithm위에서 다룬 계산은 상당히 복잡한 비선형 방정식 2개로 이루어진 시스템에 대한 풀이에 해당한다. 한방에 모든 방정식을 풀려고 할 때 수치해석적인 풀이를 위해서는 답에 거의 근접한 초기값을 넣어주어야 한다. 그래서 보통 이러한 계산은 초기값에 대한 민감성을 덜어주기 위해 다양한 방식의 successive substitution 알고리즘이 적용된다.Ryo Akasaka라는 분이 쓰신 이 논문에 나온 알고리즘을 적용해 본다. 나름 이해하기 쉽게 작성이 되어 있어서 관심있으면 읽어보는 것도 좋을 것 같다.실은 위에 애써 짜놓은 코드들 그냥 버리기 아까워서 이거조금 더 써먹어 본다.3.1. Newton-Raphson algorithm for simultaneous equations풀어야 하는 방정식이 여러개 있는 경우에 적용되는 Newton-Raphson 형태의 수치해석 알고리즘을 적용한다. 잠시 논문내용을 정리해본다.Ideal Helmholtz term에 해당하는 식(2)는 아래처럼 표현할 수 있다.\\[\\alpha^0 = \\ln\\delta + f(\\tau) \\qquad (9)\\]여기서 $f$ 는 온도만의 함수에 해당한다.따라서 상평형 조건에 해당하는 식(7~8)을 정리하면 아래와 같이 주어진다.\\[\\delta' \\left[1 + \\left( {\\partial \\alpha^r \\over \\partial \\delta}\\right)_{\\tau, \\delta = \\delta'} \\right] = \\delta'' \\left[1 + \\left( {\\partial \\alpha^r \\over \\partial \\delta}\\right)_{\\tau, \\delta = \\delta''} \\right] \\qquad (10)\\]\\[\\ln\\delta' + \\alpha^r(\\tau, \\delta') + \\delta' \\left( {\\partial \\alpha^r \\over \\partial \\delta}\\right)_{\\tau, \\delta = \\delta'} = \\ln\\delta'' + \\alpha^r(\\tau, \\delta'') + \\delta'' \\left( {\\partial \\alpha^r \\over \\partial \\delta}\\right)_{\\tau, \\delta = \\delta''} \\qquad (11)\\]$J(\\tau, \\delta)$와 $K(\\tau, \\delta)$를 아래와 같이 정의하자.\\[J(\\tau, \\delta) = \\delta \\left[{1 + \\left( {\\partial \\alpha^r \\over \\partial \\delta}\\right)_{\\tau}}\\right] \\qquad (12)\\]\\[K(\\tau, \\delta) = \\ln\\delta + \\alpha^r(\\tau, \\delta) + \\delta \\left( {\\partial \\alpha^r \\over \\partial \\delta}\\right)_{\\tau} \\qquad (13)\\]이러면 식 (10~11)은 최종적으로 아래와 같이 정리된다.\\[J(\\tau, \\delta') = J(\\tau, \\delta'') \\qquad (14)\\]\\[K(\\tau, \\delta') = K(\\tau, \\delta'') \\qquad (15)\\]임의의 $\\tau$ 에서 두 개의 미지수 $\\delta’$와 $\\delta’‘$는 위 식 두개를 풀면서 얻어지게 된다.이 부분에서 Newton-Raphson algorithm for simultaneous equations 를 적용하면 아래와 같이 successive substitution 형태의 식을 얻게된다.\\[\\delta'^{(k+1)} = \\delta'^{(k)} + {\\gamma \\over \\Delta}\\left\\{ (K(\\tau, \\delta'') - K(\\tau, \\delta'))\\left({\\partial J \\over \\partial \\delta}\\right)_{\\tau, \\delta = \\delta''} - (J(\\tau, \\delta'') - J(\\tau, \\delta'))\\left({\\partial K \\over \\partial \\delta}\\right)_{\\tau, \\delta = \\delta''} \\right\\} \\qquad (16)\\]\\[\\delta''^{(k+1)} = \\delta''^{(k)} + {\\gamma \\over \\Delta}\\left\\{ (K(\\tau, \\delta'') - K(\\tau, \\delta'))\\left({\\partial J \\over \\partial \\delta}\\right)_{\\tau, \\delta = \\delta'} - (J(\\tau, \\delta'') - J(\\tau, \\delta'))\\left({\\partial K \\over \\partial \\delta}\\right)_{\\tau, \\delta = \\delta'} \\right\\} \\qquad (17)\\]여기서 $\\Delta$는 아래와 같다.\\[\\Delta = \\left({\\partial J \\over \\partial \\delta}\\right)_{\\tau, \\delta = \\delta''} \\left({\\partial K \\over \\partial \\delta}\\right)_{\\tau, \\delta = \\delta'} - \\left({\\partial J \\over \\partial \\delta}\\right)_{\\tau, \\delta = \\delta'} \\left({\\partial K \\over \\partial \\delta}\\right)_{\\tau, \\delta = \\delta''} \\qquad (18)\\]Step size를 의미하는 $\\gamma$는 특수한 경우가 아닌이상 1로 둔다.최종적으로 아래의 Convergence Criteria를 사용했다.\\[\\left( K(\\tau, \\delta'') - K(\\tau, \\delta') \\right)^2 + \\left( J(\\tau, \\delta'') - J(\\tau, \\delta') \\right)^2 &lt; 10^{-20} \\qquad (19)\\]복잡해 보이지만 그냥 미분좀 더 하면 된다.3.2. Python Code2장에서 사용한 Code와 거의 비슷한 흐름이다.import sympy as spimport numpy as npfrom scipy.optimize import fsolvefsolve함수는 사용되지 않는다. 실수로 안지웠다.delta, tau, i, v, u, n, t, d, l, eta, beta, gamma, epsilon = sp.symbols(\"delta tau i v u n t d l eta beta gamma epsilon\")vi = sp.Indexed('v', i)ui = sp.Indexed('u', i)ni = sp.Indexed('n', i)ti = sp.Indexed('t', i)di = sp.Indexed('d', i)li = sp.Indexed('l', i)etai = sp.Indexed('eta', i)betai = sp.Indexed('beta', i)gammai = sp.Indexed('gamma', i)epsiloni = sp.Indexed('epsilon', i)Tc = 553.6 # kelvinrhoc = 3.224 # mol/dm-3R = 0.0083144621 # MPa dm-3 / mol KMw = 84.15948 # g mol-1# &lt;------------------------------------------- Define Ideal Helmholtz Energy ------------------------------------------------&gt; #a1 = 0.9891140602a2 = 1.6359660572c0 = 4u_value = np.array([0, 773, 941, 2185, 4495])v_value = np.array([0, 0.83775, 16.036, 24.636, 7.1715])ideal = a1 + a2*tau + sp.log(delta) + (c0 - 1)*sp.log(tau) + sp.Sum(vi*sp.log(1-sp.exp(-ui*tau/Tc)), (i, 1, 4))# &lt;------------------------------------------- Define Residual Helmholtz Energy ------------------------------------------------&gt; #n_value = np.array([0, 0.05483581, 1.607734, -2.375928, -0.5137709, 0.1858417, -0.9007515, -0.5628776, 0.2903717, -0.3279141, -0.03177644, 0.8668676, -0.1962725, -0.1425992, 0.004197016, 0.1776584, -0.04433903, -0.03861246, 0.07399692, 0.02036006, 0.00272825])t_value = np.array([0, 1, 0.37, 0.79, 1.075, 0.37, 2.4, 2.5, 0.5, 3, 1.06, 1.6, 0.37, 1.33, 2.5, 0.9, 0.5, 0.73, 0.2, 1.5, 1.5])d_value = np.array([0, 4, 1, 1, 2, 3, 1, 3, 2, 2, 7, 1, 1, 3, 3, 2, 2, 3, 2, 3, 2])l_value = np.array([0, 0, 0, 0, 0, 0, 2, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])eta_value = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.99, 1.43, 0.97, 1.93, 0.92, 1.27, 0.87, 0.82, 1.4, 3])beta_value = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.38, 4.2, 1.2, 0.9, 1.2, 2.6, 5.3, 4.4, 4.2, 25])gamma_value = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.65, 0.63, 1.14, 0.09, 0.56, 0.4, 1.01, 0.45, 0.85, 0.86])epsilon_value = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.73, 0.75, 0.48, 2.32, 0.2, 1.33, 0.68, 1.11, 1.47, 0.99])residual1 = sp.Sum(ni*(delta**di)*(tau**ti),(i, 1, 5))residual2 = sp.Sum(ni*(delta**di)*(tau**ti)*sp.exp(-delta**li), (i, 6, 10))residual3 = sp.Sum(ni*(delta**di)*(tau**ti)*sp.exp(-etai*((delta-epsiloni)**2)-betai*((tau - gammai)**2)), (i, 11, 20))residual = residual1 + residual2 + residual3# &lt;---------------------------- Differentiate Helmholtz Energy to obtain specific property ----------------------------------&gt; #Z = 1 + delta*sp.diff(residual, delta)P = Z*R*(Tc/tau)*delta*rhocG = 1 + ideal + residual + delta*sp.diff(residual, delta)J_s = delta*(1 + delta*sp.diff(residual, delta))K_s = sp.log(delta) + residual + delta*sp.diff(residual, delta)delJ_s = sp.diff(J_s, delta)delK_s = sp.diff(K_s, delta)2장에서 추가로 $J$, $K$와 이들의 편미분에 대한 식만 더 추가했다. 미분처리는 정말 편하다.def J_n(delta_n, tau_n): J_inter = sp.lambdify((n, t, d, l, eta, beta, gamma, epsilon, delta, tau), J_s) return J_inter(n_value, t_value, d_value, l_value, eta_value, beta_value, gamma_value, epsilon_value, delta_n, tau_n)def K_n(delta_n, tau_n): K_inter = sp.lambdify((n, t, d, l, eta, beta, gamma, epsilon, delta, tau), K_s) return K_inter(n_value, t_value, d_value, l_value, eta_value, beta_value, gamma_value, epsilon_value, delta_n, tau_n)def delJ_n(delta_n, tau_n): delJ_inter = sp.lambdify((n, t, d, l, eta, beta, gamma, epsilon, delta, tau), delJ_s) return delJ_inter(n_value, t_value, d_value, l_value, eta_value, beta_value, gamma_value, epsilon_value, delta_n, tau_n)def delK_n(delta_n, tau_n): delK_inter = sp.lambdify((n, t, d, l, eta, beta, gamma, epsilon, delta, tau), delK_s) return delK_inter(n_value, t_value, d_value, l_value, eta_value, beta_value, gamma_value, epsilon_value, delta_n, tau_n)마찬가지로 lambdify이용한 type 변환을 해준다. 이제 숫자를 더 빠르게 뽑아낼 수 있다.def advanced_eqili(tau_test, delta_a, delta_b): for i in range(50): ddelta = delJ_n(delta_b, tau_test)*delK_n(delta_a, tau_test) - delJ_n(delta_a, tau_test)*delK_n(delta_b, tau_test) delta_a = delta_a + (1/(ddelta))*((K_n(delta_b, tau_test) - K_n(delta_a, tau_test))*delJ_n(delta_b, tau_test) - (J_n(delta_b, tau_test) - J_n(delta_a, tau_test))*delK_n(delta_b, tau_test)) delta_b = delta_b + (1/(ddelta))*((K_n(delta_b, tau_test) - K_n(delta_a, tau_test))*delJ_n(delta_a, tau_test) - (J_n(delta_b, tau_test) - J_n(delta_a, tau_test))*delK_n(delta_a, tau_test)) criteria = ((K_n(delta_b, tau_test) - K_n(delta_a, tau_test))**2 + (J_n(delta_b, tau_test) - J_n(delta_a, tau_test))**2) if criteria &lt;= 10**-20: break return [delta_a, delta_b]Newton-Raphson방식으로 얻어지는 successive substitution형태를 advanced_equil로 정의한다. 식(16~18)에 해당한다.이후로는 완전히 동일하다. fsolve가 advanced_equil함수로 치환된 형태다.den_a = 9.3991den_b = 2.31*10**-3delta_a = den_a/rhocdelta_b = den_b/rhoctemp_set = 283tau_set = Tc/temp_setroot_eq = advanced_eqili(tau_set, delta_a, delta_b)root = []root.append(root_eq)temp_list = [temp_set]temp_start = temp_setstep = 10for i in range(1, int((round(Tc)-temp_start)/step) + 2, 1): temp_set = temp_set + step if temp_set &gt;= Tc: break temp_list.append(temp_set) tau_set = Tc/temp_set root_eq = advanced_eqili(tau_set, root[i-1][0], root[i-1][1]) root.append(root_eq)den_a_list = np.array(root)[:,0]*rhocden_b_list = np.array(root)[:,1]*rhocpres_list = R*np.array(temp_list)*rhoc*J_n(np.array(root)[:,0], Tc/np.array(temp_list))import matplotlib.pyplot as plttemp_data = np.array([283,313,343,373,403,433,463,493,523,553])den_a_data = np.array([9.3644,9.0275,8.6788,8.314,7.927,7.5071,7.0341,6.4677,5.7026,3.721])den_b_data = np.array([0.002687,0.0095389,0.026088,0.059385,0.11875,0.21701,0.37422,0.62851,1.08,2.7321])pres_data = np.array([0.0062923,0.024494,0.072215,0.17403,0.3613,0.67039,1.1417,1.8204,2.7605,4.0495])plt.title('Saturated Density')plt.plot(den_a_list, temp_list)plt.plot(den_b_list, temp_list)plt.scatter(den_a_data, temp_data)plt.scatter(den_b_data, temp_data)plt.xlabel('Molar Density(mol/l)')plt.ylabel('Temperature(K)')plt.show()plt.title('Vapor Pressure')plt.plot(temp_list, pres_list)plt.scatter(temp_data, pres_data)plt.xlabel('Tmmperature(K)')plt.ylabel('Pressure(MPa')plt.show()결과 그림은 굳이 필요 없을 것 같다. 2장의 결과와 동일하다. 코드를 직접 가지고 조금 놀다보면 초기값에 좀 덜 민감한 느낌을 받기는 한다.다만 계산시간이 오래걸린다. advanced_eqili함수를 정의할 때 for문으로 작성한 부분이 아마도 문제일 것으로 보인다. Vectorized 형태로 쓰거나 좀 더 빠르게 돌아가는 형태의 코드를 작성해보면 좋겠지만, 아직은 잘 모르겠다. 어찌됬든 돌아는 가고 답은 나온다. 지금은 기능 구현에 집중하자.Summarysympy를 활용한 공학용 계산을 다루었다. 포스팅 작성하면서 구글링 해볼때 심볼릭한 계산을 다루는 python 예제에서는 거의 예외없이 sympy가 사용되었던 것 같다.일단 식을 정의하고 필요한 경우 미분이나 추가적인 심볼릭한 연산을 해준 다음에 수치해석적인 처리가 필요하게 되면 lambdify를 통해 type을 변환해주어 방정식 풀이나 숫자 대입을 하는 방식이었다. 이 흐름만 기억해두자. 다른 복잡한 식들은 그냥 다 잊어도 좋을 것 같다.예제를 잘못 골랐는지 이번 글은 분량조절도 실패하고 개인적으로 너무 힘들었다." }, { "title": "[Eng. Calc.] Nonlinear Equations Solving", "url": "/posts/Engineering-Calculation3/", "categories": "Engineering Calculation, Equation Solving", "tags": "Engineering Calculation, Python, Nonlinear Equation Solving", "date": "2022-09-18 19:55:00 +0800", "snippet": "이번 공학용계산 포스팅의 주제는 Equation Solving, 방정식 풀이다.선형방정식에 대한 풀이는 손으로도 쉽게 풀리기 때문에 여기서는 비선형방정식에 대한 풀이를 다뤄본다. 복잡한 수치해석적인 방법에 대한 소개는 없다. Python으로 어떻게 비선형방정식을 푸는지, 어떤 모듈이 필요한지 계산 예시를 통해 소개한다.화학공학에서 종종 만나게 되는 상태방정식을 통한 물성 계산을 예로 들어본다.1. Intro상태방정식을 통해 순수물질의 물성을 계산해 보고자 한다. Peng-Robison 상태방정식을 사용할 것이다. 계산에 필요한 식들은 아래와 같이 주어져 있다.\\[P = {RT \\over V-b} - {a \\alpha \\over V^2 + 2bV - b^2} \\qquad(1a)\\]\\[P = {\\rho RT \\over 1-\\rho b} - {a \\alpha \\rho^2 \\over 1 + 2b\\rho - (\\rho b)^2} \\qquad(1b)\\]\\[a = 0.45724{R^2{T_c^2} \\over P_c}, \\ b = 0.07780{R T_c \\over P_c}\\]\\[\\alpha = [1 + (0.37464 + 1.54226\\omega - 0.26992\\omega^2)(1 - T_r^{0.5})]^2\\]\\[A = {a \\alpha P \\over R^2T^2}, \\ B = {b P \\over RT}\\]in Polynomial form:\\[Z^3 - (1 - B)Z^2 + (A - 3B^2 - 2B)Z - (AB - B^2 - B^3) = 0 \\qquad(2)\\]Fugacity:\\[\\ln\\phi = Z - 1 - \\ln(Z-B) - {A \\over 2\\sqrt{2} B}\\ln{Z + 2.414B \\over Z - 0.414B} \\qquad(3)\\]$P$는 압력, $T$는 온도, $V$는 부피, $\\rho$는 밀도, $T_c$는 임계온도, $P_c$는 임계압력, $w$는 acentric factor, $Z$는 압축인자, $\\phi$는 fugacity 이다.기체상수 R은 아래와 같이 사용할 단위에 맞게 조정해준다.\\[8.314 {J \\over mol \\ K} \\ {Pa \\ m^3 \\over J} \\ {bar \\over 10^5Pa} \\ {10^6 cm^3 \\over m^3} = 83.14 {bar \\ cm^3 \\over mol \\ K}\\]뭔가 복잡해보인다. 하지만 Python이 다 풀어줄 것이다. 우리가 해야 할 것은 어떤 수식을 풀어달라고 코드짜는 것뿐이다. 이거 하려면 결국 해결할 문제를 수학적으로 표현하고 이해하는 것이 필요하다. 상태방정식을 공부할 필요는 없다. 그저 저런 식들을 푸는데 이런 Python 모듈을 쓰는구나, 내가 풀 문제는 이거니까 이렇게 고쳐봐야지 이정도만 생각하면 충분하다.2. Equation solving - Mono-Variable2.1. Liquid density at specified Temperature &amp; Pressure간단한 상황을 생각해본다. 먼저 온도가 충분히 높고 압력이 충분히 낮아서 기체상으로만 존재하는 상태에서 물질의 밀도를 계산해보려고 한다. NIST Data에 따르면 n-Hexane은 447K, 5.1bar에서 기체로만 존재하며 밀도는 0.00015228 mol/cm3 라고 한다.Peng-Robinson 상태방정식은 아래와 같이 순수물질의 물성을 요구한다. Critical Temperature ($T_c$): 507.82 K Critical Pressure ($P_c$): 30.441 bar Acentric factor ($w$): 0.30식 (1b)를 보면 온도, 압력, 밀도간의 관계식이 주어져있다. 여기에 온도, 압력을 넣어주면 밀도만의 비선형 식으로 정리되며 이를 풀면 원하는 기체 밀도를 얻을 수 있다. 1개의 식이 1개의 변수에 대해서만 표현되어 있어 문제를 풀 수 있다.2.2. Python Code언제나 그렇듯 numpy, scipy.optimize, matplotlib이 사용된다. scipy모듈은 공학용 계산에서 빼놓을 수 없을 것 같다.import numpy as npimport matplotlib.pyplot as pltfrom scipy.optimize import fsolveR = 83.14 # P in bar, V in cm3/mol, T in kelvin## Parameter for n-Hexane, Data from NISTTc = 507.82Pc = 30.441w = 0.30a = 0.45724*(R**2)*(Tc**2)/Pcb = 0.07780*R*Tc/Pcdef alpha(temp): return (1 + (0.37464 + 1.54226 * w - 0.26992 * (w**2))*(1-(temp/Tc)**0.5))**2def pres(temp, den): return R*temp*den/(1-den*b)-a*alpha(temp)*(den**2)/(1+2*b*den-(den*b)**2)tempp = 447presp = 5.1def pres_fsol(den): return (pres(tempp, den)-presp)root_v = fsolve(pres_fsol, 0.001)print(root_v)print(f\"solution is {0.00015228:.6f} mol/cm3, Calculation result is {root_v[0]:.6f} mol/cm3\")[0.0001526]solution is 0.000152 mol/cm3, Calculation result is 0.000153 mol/cm3위에서 정의한 pres라는 함수가 식(1b)와 동일한 식이다. pres_fsol은 식(1b)로 표현되는 압력에서 5.1bar를 빼준 값을 return 해주는 함수로 0이 되어야 할 방정식에 해당한다. 바로 밑에 scipy.optimize에 있는 fsolve라는 명령어로 0이 되어야 할 함수인 pres_fsol을 풀어주며 밀도값인 0.0001526이 얻어졌다.그렇게 복잡하진 않다. 0이 되어야 하는 함수를 정의해주고 fsolve 명령어로 적절한 초기값을 넣어서 실행시켜주면 답이 얻어진다. 0이 되어야 할 함수, 초기값 이 두개만 기억하자.초기값은 적당한 값을 넣어주는데 값에 대한 힌트가 전혀 없다면 0이 되어야 할 함수를 변수에 대해 plot해보면서 확인하는 것을 추천한다. (위 경우에는 pres_fsol을 den에 대해 plot해볼수 있을 것이다. 아래코드처럼 하면 된다.)den_plot = np.linspace(0,0.006,100)plt.plot(den_plot, pres(tempp, den_plot))plt.show()3. Equation solving - Multi-Variable비선형식 1개를 푸는 것을 해봤다. 이번엔 여러개를 동시에 풀어보자. Code는 정확히 똑같은 흐름으로 진행된다.3.1. Phase Equilibrium Calculation for Pure Substance상평형에 대한 계산은 알아야 할 여러 배경지식들이 있지만 일단 여기서는 복잡한 비선형방정식 시스템을 풀이하는 것을 목적으로 하므로 다 생략하고 어떤 식을 풀어야 하는지 상황만 설정해본다.특정 온도에서 Hexane이 기상과 액상의 형태로 공존하는 압력이 존재하며 각 상에서의 밀도를 계산하고자 한다.변수의 종류는 온도 $T$, 이때의 압력 $P$, 액상의 압축인자 $Z_L$, 기상의 압축인자 $Z_V$ 4개이다.input으로 온도가 주어지므로 unknonwn의 개수는 3개이며 이를 구하기 위해 아래 3개의 식이 활용된다.Polynomial Form:\\[Z_L^3 - (1 - B)Z_L^2 + (A - 3B^2 - 2B)Z_L - (AB - B^2 - B^3) = 0 \\qquad(4)\\]\\[Z_V^3 - (1 - B)Z_V^2 + (A - 3B^2 - 2B)Z_V - (AB - B^2 - B^3) = 0 \\qquad(5)\\]Equal Fugacity:\\[\\ln\\phi_L = \\ln\\phi_V \\qquad (6)\\]\\[\\ln\\phi_L = Z_L - 1 - \\ln(Z_L-B) - {A \\over 2\\sqrt{2} B}\\ln{Z_L + 2.414B \\over Z_L - 0.414B}\\]\\[\\ln\\phi_V = Z_V - 1 - \\ln(Z_V-B) - {A \\over 2\\sqrt{2} B}\\ln{Z_V + 2.414B \\over Z_V - 0.414B}\\]이렇게 구해진 변수들로 액상, 기상의 밀도도 계산할 수 있다.\\[\\rho_L = {P \\over Z_LRT}, \\ \\rho_V = {P \\over Z_VRT}\\]3.2. Python Code위에서 사용한 Code에서 이어서 쓴다.def fugz(temp, z, p): A = a*alpha(temp)*p/((R**2)*(temp**2)) B = b*p/(R*temp) return z-1-np.log(z-B)-(A/(2*(2**0.5)*B))*np.log((z + 2.414*B)/(z-0.414*B))def zcubic(temp, z, p): A = a*alpha(temp)*p/((R**2)*(temp**2)) B = b*p/(R*temp) return (z**3)-(1-B)*(z**2)+(A-3*(B**2)-2*B)*z-(A*B-(B**2)-(B**3))def equileqnz(x): zl = x[0] zv = x[1] p = x[2] fugzl = fugz(tempp, zl, p) fugzv = fugz(tempp, zv, p) zcubicl = zcubic(tempp, zl, p) zcubicv = zcubic(tempp, zv, p) return [zcubicl, zcubicv, fugzl-fugzv]tempp = 267root_z = fsolve(equileqnz, [0.0002, 0.99, 0.041])root = []temp_list = [tempp]root.append(root_z)print(root[0])[2.52339063e-04 9.96564628e-01 4.47879861e-02]동시에 풀어야 할 식이 3개이므로 3개의 식을 list형태로 return 해주는 equileqnz라는 함수를 정의한다. 3개의 식은 0이 되어야 하는 식(4), (5), (6)에 해당한다. 초기값은 식이 복잡하다보니 상당히 민감해서 거의 답 근처를 지정해 주어야 했다. 얻어진 답은 root라는 list에 담아준다. 이는 다른 온도에서의 답을 구하기 위한 초기값으로 사용될 것이다.for i in range(1, round(Tc)-267, 1): tempp = tempp + 1 temp_list.append(tempp) root_z = fsolve(equileqnz, root[i-1]) root.append(root_z) # print(f'temp = {tempp}, zl = {root_z[0]}, zv = {root_z[1]}, pres = {root_z[2]}')온도를 1씩 증가시키면서 반복적으로 fsolve 해준다. 초기값은 직전step에서 구한 답을 활용한다.root_all = np.array(root)temp_list = np.array(temp_list)sat_pres = root_all[:,2]denl = sat_pres/(root_all[:,0]*R*temp_list)denv = sat_pres/(root_all[:,1]*R*temp_list)NIST Webbook에서 아래와 같이 n-Hexane의 Saturated Properties를 얻을 수 있다. Temperature (K) Pressure (bar) Density (l, mol/l) Density (v, mol/l) 267 0.043069 7.9198 0.0019495 307 0.1215 7.7147 0.0051461 287 0.29223 7.504 0.011684 327 0.61954 7.2858 0.023603 347 1.1873 7.0582 0.043526 367 2.0971 6.8183 0.074768 387 3.4657 6.5624 0.12159 407 5.4239 6.2853 0.18976 427 8.1157 5.9785 0.28779 447 11.703 5.6277 0.42989 467 16.371 5.2031 0.64382 487 22.355 4.6283 1.0025 507 30.07 3.2679 2.151 아래처럼 입력해준다. txt나 excel파일로 정리해서 python으로 읽어들어올수도 있다.exp_temp = [267,287,307,327,347,367,387,407,427,447,467,487,507]exp_denl = [7.9198,7.7147,7.504,7.2858,7.0582,6.8183,6.5624,6.2853,5.9785,5.6277,5.2031,4.6283,3.2679]exp_denv = [0.0019495,0.0051461,0.011684,0.023603,0.043526,0.074768,0.12159,0.18976,0.28779,0.42989,0.64382,1.0025,2.151]exp_psat = [0.043069,0.1215,0.29223,0.61954,1.1873,2.0971,3.4657,5.4239,8.1157,11.703,16.371,22.355,30.07]exp_temp = np.array(exp_temp)exp_denl = np.array(exp_denl)/1000exp_denv = np.array(exp_denv)/1000exp_psat = np.array(exp_psat)plt.plot(denl, temp_list)plt.plot(denv, temp_list)plt.scatter(exp_denl, exp_temp)plt.scatter(exp_denv, exp_temp)plt.xlabel('density (mol/cm3)')plt.ylabel('Temperature (K)')plt.title('saturated density vs temperature')plt.show()plt.plot(temp_list, sat_pres)plt.scatter(exp_temp, exp_psat)plt.xlabel('Temperature (K)')plt.ylabel('Pressure (bar)')plt.title('Saturated Vapor Pressure Curve')plt.show()데이터와 함께 이쁘게 Plot해주자. 계산값은 선에 해당하고 점들은 Data다. 계산이 잘 수행된 것 같다. Peng-Robinson 상태방정식은 꽤나 quality가 훌륭했다. 물리적으로 의미가 안맞을 수도 있지만 앞선 Regression 예제와 함께 생각해보면 데이터에 잘 맞는 파라미터($T_c, \\ P_c, \\ w$)를 얻을 수도 있을 것이다.Summary비선형 방정식 풀이를 해봤다. scipy.optimize.fsolve를 활용했고 0이 되어야 할 함수를 지정해 주는 것과 초기값을 넣는 것에 주의해야 한다는 것을 알았다.적당한 예시를 찾느라 상태방정식을 가져오긴 했는데, 지금와서 보니 그렇게 좋은 예시는 아니었던 것 같다. 식이 어떻게 저렇게 수립되는지는 전혀 신경 쓰지 말고 풀어야 하는 방정식 시스템이 어떻게 Python Code로 풀리는지 보는게 이번 포스팅의 의도와도 맞다.어디까지나 Technical한 얘기였고 항상 마무리는 이거 다 필요없다는 거다. 정작 중요한 것은 풀어야 할 식을 수학적으로 표현해내는 과정이다. 여기가 실은 가장 시간을 많이 써야하는 부분이고 중요한 단계다." }, { "title": "[Eng. Calc.] Regression and Curve-Fit", "url": "/posts/Engineering-Calculation2/", "categories": "Engineering Calculation, Regression, Curve-Fit", "tags": "Engineering Calculation, Python, Regression, Curve-Fit, Density", "date": "2022-09-05 19:55:00 +0800", "snippet": "이번 공학용 계산 주제는 Regression 이다. 편의상 Fitting 이라고도 한다. Regression, Fitting 원래는 명확한 구분이 있을거 같으면서도 굉장히 혼용되서 사용되는 단어 같다. 실은 딱히 명확한 구분은 없는 것 같다. 관련해선 이 문서를 한 번 읽어보자.여기서는 변수, independent variable $x$에 대해서 함수값, 모델값, dependent variable $y$ 혹은 $f(x)$가 있을 때 이 둘 사이의 관계를 가장 잘 나타내주는 모델 (혹은 파라미터)를 찾는 것을 해보려고 한다. $x$, $y$는 모두 1-d, 2-d 혹은 n-d가 될 수 있다.1. Intro공학용 계산에서 Regression은 생각보다 자주 마주치는 (마주쳐야하는) 문제다. 어떤 데이터가 있고 이를 설명하는 이론식이 있다면 데이터에 잘 맞도록 하는 이론식 내 파라미터를 구하는 일들이다.굳이 얌전히 있는 데이터에 모델식을 끼워맞추는 일 같은건 왜하는 걸까. 하나는 비연속적인 데이터 사이를 메꿔주는 interpolation, 내삽의 의미가 있고 다른 하나는 데이터 외부를 예측해주는 extrapolation, 외삽, 예측, prediction의 의미가 있다.보통 좋은 모델이라고 불리는 이론식 일수록 예측의 범위와 신뢰도가 높고 모델식 내 Parameter가 물리적인 의미를 가지는 경우가 많다.반대로 경험식, Empirical한 식의 경우 데이터 사이를 채워주는 정도로만 사용되며 조금만 범위를 넘어가면 예측의 신뢰도는 많이 떨어지고 물리적인 의미를 가지지 못하는 값을 예측하는 경우도 많다.아래에는 두 가지 예제에 대해 Python으로 Regression 해보려고 한다.2. Regression - Mono-Variable2.1. Saturated Liquid Density - Rackett Equation포화 액체 밀도는 화공업계에서 상당히 많이 필요로 하는 물성 중에 하나다. 경험상 대부분 이 물성에 대해 크게 신경쓰지는 않지만 특수한 경우에는 정확한 예측이 중요할 수 있다. 대개 온도가 주어지면 포화액체 밀도를 구할 수 있는 이론식들이 사용된다.(independent variable은 온도인 $T$이고 dependent variable은 포화액체밀도인 $d_{sat}$이다.)포화 액체 밀도를 표현해 주는 식은 정말 다양하게 많다. 그 중에서 Rackett Equation을 사용하고자 한다. 공정모사 프로그램에서도 활용되는 식이다. 식 모양은 아래와 같이 간단한 형태다.\\[d_{sat} = d_c B^{-(1-T/T_c)^N} \\qquad (1)\\]식 1에서 $d_{sat}$은 우리가 알고자 하는 포화 액체 밀도이고 이를 위한 parameter로 B와 N이 있다. $d_c$는 critical liquid density, $T_c$는 critical temperature, $T$는 온도다.아래 Python 코드를 통해 단순히 empirical한 모델식으로 polynomial 식을 사용해 보고 Rackett모델과 비교해 본다. Data는 NIST Thermophysical Properties of Fluid Systems를 사용하고 물질은 n-hexane이다.2.2. Python Regression사용할 모듈은 많지 않다. numpy, scipy.optimize.curve_fit, scipy.optimize.minimize가 regression에 쓰이고 matplotlib은 데이터 plot에 사용된다.import numpy as npimport matplotlib.pyplot as pltfrom scipy.optimize import minimize, curve_fit온도와 포화액체밀도 데이터다. 각각 Kelvin, mol/l 단위로 순서에 맞게 정리되어 있다.알고보니 numpy 모듈에 polyfit이라는 기능이 있었다. 이 공식 Document 보면 이거 하나로 n차 polynomial 식에 fitting 그냥 해준다.Temp = [277.15, 287.15, 297.15, 307.15, 317.15, 327.15, 337.15, 347.15, 357.15, 367.15, 377.15, 387.15, 397.15, 407.15, 417.15, 427.15, 437.15, 447.15, 457.15, 467.15, 477.15, 487.15, 497.15, 507.15, 507.82] # Temp in Kelvin(K)Dens = [7.8164, 7.7132, 7.6086, 7.5024, 7.3943, 7.2842, 7.1716, 7.0564, 6.9382, 6.8164, 6.6907, 6.5604, 6.4249, 6.2831, 6.134, 5.9761, 5.8073, 5.6248, 5.4244, 5.1995, 4.9394, 4.6229, 4.1954, 3.2241, 2.706] # Liquid Density of Hexane in (mol/l)Temp = np.array(Temp)Dens = np.array(Dens)Degree_of_polyfit = 2coeff_np = np.polyfit(Temp, Dens, Degree_of_polyfit)print(f\"Coefficients for {Degree_of_polyfit}-poly fit are: {coeff_np}\")pred = np.poly1d(coeff_np)print(f\"AAD(%): {np.sum(((pred(Temp) - Dens)/Dens)**2)/len(Temp)*100:.3f}%\")Coefficients for 2-poly fit are: [-7.77250536e-05 4.37154100e-02 1.41319326e+00]AAD(%): 0.575%np.polyfit으로 온도와 밀도간 관계를 잘 설명해주는 2차 식의 계수를 구했고 상대 오차는 0.575%였다. 아래 그림처럼 적당한 Fit이 구해졌다.plt.title(f\"{Degree_of_polyfit}-Poly fit Results\")plt.xlabel(\"Temperature (K)\")plt.ylabel(\"Molar Liquid Density (mol/l)\")plt.plot(Temp, pred(Temp))plt.scatter(Temp, Dens)plt.show()이번에는 Rackett Equation을 써보려고 한다. 어떤 이론적 base를 가지는지는 잘 모르지만 어쨌든 식(1)은 polynomial 보다는 나을 것 같다. 아래 코드에서는 curve_fit기능을 사용해본다.tc = Temp[len(Temp)-1] #Tc = 507.82(K)dc = Dens[len(Dens)-1] #dc = 2.706(mol/l)def Rackett(temp, b, n): return dc*(b**(-(1-temp/tc)**n))p0 = [0.1, 0.1]para, conv = curve_fit(Rackett, Temp, Dens, p0)print(f\"Coefficients for Rackett Equation are: {para}\")predic = np.array(Rackett(Temp, *para))print(f\"AAD(%): {np.sum(((predic-Dens)/Dens)**2)/len(Temp)*100:.3f}%\")Coefficients for Rackett Equation are: [0.26531865 0.28220359]AAD(%): 0.004%식 (1)에서 $B$와 $N$이 각각 0.265, 0.282일 때 Rackett Equation이 n-hexane의 포화액체밀도를 잘 맞추었으며 상대오차는 0.004%였다.plt.title(f\"Racektt Equation fit Results\")plt.xlabel(\"Temperature (K)\")plt.ylabel(\"Molar Liquid Density (mol/l)\")plt.plot(Temp, Rackett(Temp, *para))plt.scatter(Temp, Dens)plt.show()앞서 기술했듯이 이론식의 장점은 예측범위와 신뢰도다. 어쩌면 당연한 얘기지만 2차 polynomial 식은 조금만 벗어나도 예측하는데 사용하지 못하는 수준이다.Temp_expand = np.linspace(200, tc, 50)plt.title(f\"Fit Results\")plt.xlabel(\"Temperature (K)\")plt.ylabel(\"Molar Liquid Density (mol/l)\")plt.plot(Temp_expand, pred(Temp_expand), label = \"Polyfit\")plt.plot(Temp_expand, Rackett(Temp_expand, *para), label = \"Rackett\")plt.scatter(Temp, Dens, label = \"Experiment\")plt.legend()plt.show()아래는 curve_fit이 아닌 minimize를 쓰는 경우다. minimize를 사용하는 경우 cost function, objective function, 목적함수, residue, 편차 등으로 불리는 최소화 해줄 어떤 오차를 Racket_resid라고 정의해주었고 이를 최소화하는 방식으로 parameter를 얻었다.당연히 curve_fit을 쓰는 경우가 더 편하고 코드도 덜 쓰고 신경쓸 것도 적고 심지어 이 경우에 수렴도 조금 잘 되는 느낌을 받았다. 하지만 오차식을 정의하고 minimize를 쓰는 경우는 자유도가 높다. 할 수 있는게 있으니 분명히 언젠가 쓰인다.어쨌든 두 방식 모두 동일한 값을 얻었다.def Rackett_resid(p): b = p[0] n = p[1] res = 0 for i in range(len(Temp)): res = res + ((Rackett(Temp[i], b, n) - Dens[i]))**2 return resp0 = [0.26, 0.28]c = minimize(Rackett_resid, p0)print(c.x)[0.26531862 0.28220363]3. Regression - Multi-Variable조금 더 복잡한 Regression으로 가보자. 위에서는 하나의 independent variable에 대해 Fitting 했다면 이번에는 2개다.3.1. Polymer Density - Tait Equation순수성분의 포화 액체 밀도라고 하면 온도만의 함수 혹은 압력만의 함수가 될 수 있다. 순수성분계에서 ‘Saturated’, ‘포화’라는 단어가 이미 온도가 정해지면 압력은 정해지게끔 혹은 그 반대상황을 의미하기 때문이다. 여기서 포화라는 단어는 기체와 액체가 공존하는 상평형을 의미하기도 한다. 따라서 Rackett Equation은 온도 하나만의 식으로 표현될 수 있었다.하지만 Polymer의 경우 ‘포화’라는 상태가 딱히 없다. 온도와 압력이 모두 주어져야 밀도가 정해진다.이를 위한 이론식으로 Tait Equation이 있으며 아래와 같이 표현된다.\\[v(P,T) = v(0,T) \\left( 1 - C \\ln \\left( 1 + {P \\over B(T)} \\right) \\right) \\qquad (2)\\]\\[B(T) = B_0\\exp(-B_1T) \\qquad (2a)\\]\\[v(0,T) = v_0\\exp(-\\alpha T) \\qquad (2b)\\]\\[C = 0.0894 \\qquad (2c)\\]여기서 $v(P,T)$는 임의의 압력, 온도에서 밀도의 역수인 specific volume을 나타낸다. $v(0,T)$는 압력이 0으로 고정되어 있고 임의의 온도 $T$일 때의 specific volume이다.식(2)에서 independent variable은 온도와 압력인 $T$와 $P$이며 dependent variable은 이때의 specific volume인 $v(T,P)$가 되며 모델식 내 parameter는 $B_0$, $B_1$, $v_0$, $\\alpha$ 이다.온도, 압력에 따른 specific volume 데이터를 활용하여 위 4개의 parameter들을 regression 할 것이다. 데이터는 P.Zoller의 Standard Pressure-Volume-Temperature Data for Polymers라는 책에서 일부 가져왔다. Polymer에 대한 정보는 아래와 같다. High Density Polyethylene Mw = 126,000 (PDI = 4.5) Specific Volume(at ambient condition) = 1.0537 cm3/g3.2. Python Regression먼저 data를 정리해보자. P.Zoller의 책에 데이터는 아래와 같이 테이블로 주어진다. Temp/Pressure 0 20 40 60 80 100 120 140 160 180 200 170 1.3023 1.2781 1.2579 1.241 1.2264 1.2132 1.2015 1.1909 1.181 1.1718 1.1633 180 1.3124 1.2865 1.2654 1.2477 1.2326 1.219 1.2069 1.196 1.1859 1.1764 1.1677 189.7 1.3222 1.2951 1.2731 1.2546 1.2389 1.2249 1.2123 1.2011 1.1908 1.1812 1.1722 199.7 1.3327 1.3039 1.2809 1.2617 1.2455 1.2311 1.2182 1.2066 1.1961 1.1861 1.1772 209.7 1.3431 1.3124 1.2885 1.2687 1.2519 1.2371 1.2237 1.2118 1.2009 1.1908 1.1815 219.7 1.3535 1.3215 1.2963 1.2754 1.2582 1.2428 1.2292 1.2170 1.2058 1.1955 1.186 230.2 1.3639 1.3302 1.3041 1.2826 1.2645 1.2487 1.2345 1.222 1.2105 1.1999 1.1902 240.6 1.3755 1.3393 1.3121 1.2896 1.2711 1.2547 1.2403 1.2273 1.2156 1.2046 1.1946 249.9 1.3853 1.3476 1.319 1.296 1.2768 1.2598 1.2449 1.2318 1.2196 1.2085 1.1985 260.2 1.3964 1.3564 1.3268 1.3027 1.283 1.2656 1.2504 1.2368 1.2245 1.2133 1.2029 270.1 1.4075 1.3656 1.3344 1.3096 1.289 1.2712 1.2555 1.2417 1.229 1.2174 1.2068 온도는 $^\\circ C$, 압력은 Mpa단위다.위 table을 Python Code내에서 사용하고자 할 명령어가 알아들을 수 있게 정리해주는작업이 필요하다. 2차원 행렬에서 특정열 혹은 특정행을 지정하여 가져오는 일들은 python으로 어떤 일을 하려고 할 때 언젠가는 필수적으로 맞닥뜨릴 것이다. 자세한 설명은 하지 않지만 numpy array의 slicing, indexing을 검색해 보면 길을 찾을 수 있을 것이다.여기서는 curve_fit만 사용할 예정이다.import numpy as npimport matplotlib.pyplot as pltfrom scipy.optimize import curve_fit아래 코드에서는 slicing을 통해 세로로 첫번째 열만을 지정할 수 있었다. v0T라는 변수에서 den이라는 2차배열을 어떻게 slicing하는지 이해하면 나중에 여러모로 유용하다.temp = [170, 180, 189.7, 199.7, 209.7, 219.7, 230.2, 240.6, 249.9, 260.2, 270.1]temp = np.array(temp) + 273.15pres = [0, 20, 40, 60, 80, 100, 120, 140, 160, 180, 200]pres = np.array(pres)den_170 = [1.3023, 1.2781, 1.2579, 1.241, 1.2264, 1.2132, 1.2015, 1.1909, 1.1810, 1.1718, 1.1633]den_180 = [1.3124, 1.2865, 1.2654, 1.2477, 1.2326, 1.219, 1.2069, 1.196, 1.1859, 1.1764, 1.1677]den_190 = [1.3222, 1.2951, 1.2731, 1.2546, 1.2389, 1.2249, 1.2123, 1.2011, 1.1908, 1.1812, 1.1722]den_200 = [1.3327, 1.3039, 1.2809, 1.2617, 1.2455, 1.2311, 1.2182, 1.2066, 1.1961, 1.1861, 1.1772]den_210 = [1.3431, 1.3124, 1.2885, 1.2687, 1.2519, 1.2371, 1.2237, 1.2118, 1.2009, 1.1908, 1.1815]den_220 = [1.3535, 1.3215, 1.2963, 1.2754, 1.2582, 1.2428, 1.2292, 1.2170, 1.2058, 1.1955, 1.186]den_230 = [1.3639, 1.3302, 1.3041, 1.2826, 1.2645, 1.2487, 1.2345, 1.222, 1.2105, 1.1999, 1.1902]den_240 = [1.3755, 1.3393, 1.3121, 1.2896, 1.2711, 1.2547, 1.2403, 1.2273, 1.2156, 1.2046, 1.1946]den_250 = [1.3853, 1.3476, 1.319, 1.296, 1.2768, 1.2598, 1.2449, 1.2318, 1.2196, 1.2085, 1.1985]den_260 = [1.3964, 1.3564, 1.3268, 1.3027, 1.283, 1.2656, 1.2504, 1.2368, 1.2245, 1.2133, 1.2029]den_270 = [1.4075, 1.3656, 1.3344, 1.3096, 1.289, 1.2712, 1.2555, 1.2417, 1.229, 1.2174, 1.2068]den = np.array([den_170, den_180, den_190, den_200, den_210, den_220, den_230, den_240, den_250, den_260, den_270])v0T = den[:,0]print(v0T)[1.3023 1.3124 1.3222 1.3327 1.3431 1.3535 1.3639 1.3755 1.3853 1.3964 1.4075]먼저 식(2b)로 표현되는 압력이 0이고 임의의 온도 T에 대한 specific volume을 먼저 regression한다. 여기서는 온도만이 유일한 변수이다.plt.scatter(temp, v0T)def v_at_temp0(temp, alpha, v0): return v0*np.exp(-alpha*temp)p0 = [-0.0007, 0.9]param, conv = curve_fit(v_at_temp0, temp, v0T, p0)plt.plot(temp, v_at_temp0(temp, *param))plt.scatter(temp, v0T)plt.show()다음으로 온도와 압력이 변수인 상태에서의 regression이다. 위에서 얻어진 (2b)에 대한 식은 그대로 활용한다.아래 코드는 이 문서에서 코드를 참고했다.def tait(X, b0, b1): temp, pres = X C = 0.0894 B = b0*np.exp(-b1*temp) return v_at_temp0(temp, *param)*(1-C*np.log(1+pres/B))den_all = den.flatten()pres_all = np.concatenate((pres, pres, pres, pres, pres, pres, pres, pres, pres, pres, pres))temp_np = np.array([temp, temp, temp, temp, temp, temp, temp, temp, temp, temp, temp])temp_all = np.concatenate((temp_np[:,0],temp_np[:,1],temp_np[:,2],temp_np[:,3],temp_np[:,4], temp_np[:,5],temp_np[:,6],temp_np[:,7],temp_np[:,8],temp_np[:,9],temp_np[:,10]))para_all, conv = curve_fit(tait, (temp_all, pres_all), den_all)print(para_all)[9.09419338e+02 5.29825619e-03]for i in range(len(pres)): if i % 2 == 0: plt.plot(temp, tait((temp, pres[i]),*para_all), label = f'{pres[i]} MPa') plt.scatter(temp, den[:,i]) else: passplt.title('Polyethylene Specific Volume using Tait Equation')plt.ylabel('Spec. Vol (cm3/g)')plt.xlabel('Temp(K)')plt.legend()plt.show()Summary공학용 계산 예제로 Regression 혹은 Fitting을 해봤다. 몇가지의 변수들로 이루어진 데이터가 있고 이들간의 관계를 어떻게든 수학적으로 표현하고자 하는 행동들이었다.이론식이 있으면 이론식 내 파라미터를 데이터에 맞도록 하는 작업들이 수행되고 이론식이 없다면 아무식이나 갖다 써보면서 잘 맞는지 확인한다. 이런 식들을 모델이라고 하고 이런 일들을 모델링한다라고도 하는것 같다.이러한 모델링은 결국 모델식과 데이터간의 오차를 최소화 하는 방식으로 이루어지며 이를 편하게 해주는 curve_fit과 minimize라는 기능이 scipy.optimize 모듈에 있었다. 물론 다른 모듈에도 비슷한 기능들이 많을 것이고 python 외에도 다른 선택지는 많다.이런식의 모델링으로 하고자 하는 일은 결국 Case Study, 어떤 판단, 결국 예측이다. 하지만 항상 염두에 둬야 하는건 모든 모델링은 제한적으로만 맞고 대부분은 현실을 정확하게 표현해주지 못한다는 점이다. 이론식에 대한 배경, 이론을 알아가는 과정이 훨씬 더 중요한 이유다.이번 포스팅 내내 Regression하고 Fitting 하는 것만 잔뜩 써놨지만 정작 Fitting 하는 것 자체는 크게 어렵거나 중요하지 않다." }, { "title": "[Eng. Calc.] Differential Equations - Kinetics", "url": "/posts/Engineering-Calculation/", "categories": "Engineering Calculation, ODE-Kinetics", "tags": "Engineering Calculation, Python, Kinetics, Differential Equation", "date": "2022-08-22 19:55:00 +0800", "snippet": "공학용 계산 첫 번째 포스팅이다. 첫번째 주제는 반응속도식 문제 풀이다. 수학으로는 미분방정식 풀이와 fitting 예제에 해당한다. 계산에는 Python을 쓴다.1. Intro간단한 문제에서부터 시작하려고 한다. 이 문서의 예제와 데이터를 따라가본다. 아래 반응을 보자.\\[A \\rightarrow B\\]반응속도식을 정하는건 이런저런 고려들이 들어가는 복잡한 일이지만 일단 간단하게 반응차수는 1로 아래처럼 정리된다고 가정하자.\\[{dC_A \\over dt} = -k_1 C_A \\qquad (1)\\]A라는 물질의 시간에 따른 농도변화는 A의 농도에 비례하며 비례상수는 반응속도상수인 $k_1$이다. A는 B로 변하면서 농도가 감소할 것이기 때문에 음수가 붙는다. 일반적으로 반응 실험 Data로부터 $k_1$을 Regression하고 다른 온도조건에서의 반응결과 예측이나 반응기 설계 등에 활용된다.2. Background식 1을 푸는 방식은 크게 2가지다. 손으로 미분방정식을 풀어서 해석해를 구하거나 수치적으로 근사해서 풀이한다. (Analytic solution을 구하거나 numerical solution을 구한다.)2.1. Analytic Solution식 (1)은 아래처럼 해석해가 구해진다.\\[{dC_A \\over C_A} = -k_1 dt,\\quad\\int_{C_{A,0}}^{C_A} {dC_A \\over C_A} = \\int_{0}^{t} -k_1 dt\\]\\[C_A = C_{A,0}e^{-k_1 t} \\qquad (2)\\]$C_{A,0}$는 t=0 일때의 초기농도를 의미한다.Data는 시간(tdata)와 각 시간에 측정한 A물질의 농도(concentration)이다.time_data = np.array([0, 0.9184, 9.0875, 11.2485, 17.5255, 23.9993, 27.7949, 31.9783, 35.2118, 42.973, 46.6555, 50.3922, 55.4747, 61.827, 65.6603, 70.0939])CA_data = np.array([0.906, 0.8739, 0.5622, 0.5156, 0.3718, 0.2702, 0.2238, 0.1761, 0.1495, 0.1029, 0.086, 0.0697, 0.0546, 0.0393, 0.0324, 0.026])Data를 보니 $C_{A,0}$는 0.906이다. input은 time_data이고 output은 conc_data이니 간단한 nonlinear regression 문제로 바뀌었다. 아래 코드로 $k_1$을 구할 수 있다. (구글에 python nonlinear regression 검색해보자.)import numpy as npimport matplotlib.pyplot as pltfrom scipy.optimize import curve_fittime_data = np.array([0, 0.9184, 9.0875, 11.2485, 17.5255, 23.9993, 27.7949, 31.9783, 35.2118, 42.973, 46.6555, 50.3922, 55.4747, 61.827, 65.6603, 70.0939])CA_data = np.array([0.906, 0.8739, 0.5622, 0.5156, 0.3718, 0.2702, 0.2238, 0.1761, 0.1495, 0.1029, 0.086, 0.0697, 0.0546, 0.0393, 0.0324, 0.026])def C_A(t, k1): C_A0 = CA_data[0] return C_A0*np.exp(-k1*t)para, pcov = curve_fit(C_A, time_data, CA_data)print(f\"k1 = {para}\")k1 = [0.05078669]plt.title(\"Regressed Results\")plt.plot(time_data, C_A(time_data, para))plt.scatter(time_data, CA_data)plt.xlabel(\"Time\")plt.ylabel(\"Concentraion of A\")plt.show()scipy모듈의 curve_fit함수를 사용한 간단한 regression이다. k1은 0.05가 나왔다.참고로 curve_fit은 squared error sum을 최소화 해야 할 오차함수로 쓰는 것으로 보인다. 자세한건 Documentation을 읽어보자.2.2. Numerical Solution이번에는 수치적으로 식(1)을 풀어보자. 아래 코드로 구할 수 있다.import numpy as npfrom scipy import integratefrom scipy import optimizeimport matplotlib.pyplot as plttime_data = np.array([0, 0.9184, 9.0875, 11.2485, 17.5255, 23.9993, 27.7949, 31.9783, 35.2118, 42.973, 46.6555, 50.3922, 55.4747, 61.827, 65.6603, 70.0939])CA_data = np.array([0.906, 0.8739, 0.5622, 0.5156, 0.3718, 0.2702, 0.2238, 0.1761, 0.1495, 0.1029, 0.086, 0.0697, 0.0546, 0.0393, 0.0324, 0.026])def f(y, t, k): C_A = y k1 = k dCAdt = -k1*(C_A) return dCAdtdef Conc(x,param): f2 = lambda y,t: f(y, t, param) CA = integrate.odeint(f2,CA_data[0],x) return CAdef f_resid(p): res = 0 for i in range(len(time_data)): res = res + (Conc(time_data, p)[i] - CA_data[i])**2 return resguess = 0.01c = optimize.minimize(f_resid, guess)print(\"parameter values are \", c.x)parameter values are [0.05078668]plt.title(\"Regressed Results\")plt.plot(time_data, Conc(time_data, c.x))plt.scatter(time_data, CA_data)plt.xlabel(\"Time\")plt.ylabel(\"Concentraion of A\")plt.show()조금 더 복잡해 보인다. 먼저 강조하고 싶은 건 내 머리속에서 처음부터 끝까지 나온 코드가 절대 아니다. 이 문서를 참고했다. 그림 끼워맞추기 처럼 생각해도 좋다. 위 코드를 재사용하려면 이 코드의 이 부분이 어떤 역할을 하는지 눈치껏 보고 내 상황에 맞게 고치면 된다.간단히 설명하자면 f함수는 식(1) 미분방정식 자체를 정의하고 integrate.odeint로 수치적분한 함수를 만들어 주고(식(2)에 해당할 것이다.) 수치적분한 함수와 데이터간의 오차를 f_resid로 정의했다. 여기서 오차는 오차제곱의 합으로 선정했다. 이후 optimize.minimize로 오차 함수를 최소화 하는 $k_1$을 구하도록 했고 결과는 똑같이 0.05가 나왔다.두가지 방식 중 어떤 방식이 더 좋다고 할 수 있을까. 그보다 어느게 좋은 방식인지 말하는 기준은 뭘까.2.3. Analytic vs Numerical해석해를 통해 미분방정식을 푸는 일은 일단 꽤나 직관적이다. 인과관계를 파악하기도 용이할 것이고 어딘가에서 실수가 있었다면 발견하고 고치기도 쉬울 것이다. 참고로 해석해를 구하는 건 프로그래밍으로도 가능하다. (Symbolic한 연산을 수행해주는 sympy 라이브러리가 있다.) 하지만 해석해가 존재하지 않는 미분방정식 또한 존재한다. 따라서 해석해를 구하는 방식은 일반적으로 모든 경우에 적용가능한 방법은 아니다.수치적인 방법은 복잡한 미분방정식 풀이에 적당할 수 있다. 그렇다고 Euler Method같은 미분방정식 풀이 알고리즘을 꼭 알아야 할 필요는 없다. 라이브러리와 Solver들이 잘 갖추어져 있어 그냥 갖다쓰면 된다. 하지만 미분방정식 중에도 풀기 어려운 형태의 식들이 있다. Stiffness같은 수치해석적인 이슈가 있는 경우도 있다. 물론 그런것들을 해결해주는 라이브러리 또한 Python 어딘가에 존재할 것이다.둘 중 뭐가 더 좋다고 말할 수는 없다. 개인적으로는 쉬운 문제인 경우 직접 해석적으로 풀어보고 복잡한 문제라면 어쩔수 없이 수치적인 방법을 쓰는게 좋다고 생각한다. 근데 일단 미분방정식을 풀이할 일 자체가 인생에서 그리 많지는 않다..3. Application훨씬 복잡한 미분방정식 풀이를 해보려고 한다. 당연하게 Numerical method 쓴다. 6개의 물질(A~E)이 포함된 아래 반응에 대한 반응속도 문제를 예제로 삼아보자.\\[A + B \\rightarrow C + D\\]\\[A + C \\rightarrow E\\]반응속도식은 아래와 같이 주어진다고 가정하자.\\[{dC_A \\over dt} = -k_1 C_A^2C_B - k_2C_AC_C \\qquad (3.1)\\]\\[{dC_B \\over dt} = -k_1 C_A^2C_B \\qquad (3.2)\\]\\[{dC_C \\over dt} = k_1 C_A^2C_B - k_2C_AC_C \\qquad (3.3)\\]\\[{dC_D \\over dt} = k_1 C_A^2C_B \\qquad (3.4)\\]\\[{dC_E \\over dt} = k_2C_AC_C \\qquad (3.5)\\]식 (3.1) ~ (3.4)에서 $C_A$의 반응차수가 1이 아니라 2라는 것에 주의하자. 이번에는 미분방정식 5개인 ODEs System을 풀면서 동시에 Data에 맞는 $k_1$과 $k_2$를 fitting 해야 한다.Intro에서 보여줬던 numerical solution 코드를 약간 수정하여 적용하면 아래처럼 된다.import numpy as npfrom scipy import integratefrom scipy import optimizeimport matplotlib.pyplot as plttime_data = np.array([0, 20, 40, 60, 90, 120, 180])/60CAdata = np.array([~~~])CBdata = np.array([~~~])CCdata = np.array([~~~])CDdata = np.array([~~~])CEdata = np.array([~~~])Data = np.array([CAdata, CBdata, CCdata, CDdata, CEdata])def f(y, t, k): C_A = y[0] C_B = y[1] C_C = y[2] C_D = y[3] C_E = y[4] k1 = k[0] k2 = k[1] dCAdt = -k1*(C_A**2)*C_B - k2*(C_A)*(C_C) dCBdt = -k1*(C_A**2)*C_B dCCdt = k1*(C_A**2)*C_B - k2*(C_A)*(C_C) dCDdt = k1*(C_A**2)*C_B dCEdt = k2*(C_A)*(C_C) return [dCAdt, dCBdt, dCCdt, dCDdt, dCEdt]def Conc(x,param): f2 = lambda y,t: f(y, t, param) r = integrate.odeint(f2,y0,x) CA = r[:,0] CB = r[:,1] CC = r[:,2] CD = r[:,3] CE = r[:,4] return [CA, CB, CC, CD, CE]def f_resid(p): res = 0 for i in range(Data.shape[0]): for j in range(Data.shape[1]): if Data[i][j]==0: continue else: res = res + ((Data[i][j] - Conc(time_data, p)[i][j])**2) return resguess = [0.001, 0.001] y0 = Data[:,0] c = optimize.minimize(f_resid, guess)print(\"parameter values are \", c.x)parameter values are [ 0.14028538 -0.02505139]legend = ['CA', 'CB', 'CC', 'CD', 'CE']for i in range(len(Data)): plt.scatter(time_data, Data[i,:], label = legend[i]) plt.plot(time_data, Conc(time_data, c.x)[i])plt.legend()plt.xlabel(\"Time\")plt.ylabel(\"Conc\")plt.show()plt.scatter(time_data, Data[4,:])plt.plot(time_data, Conc(time_data, c.x)[4])plt.title(\"CE Plot\")plt.show()print(f\"MSE = {err}\")앞에서와 마찬가지로 f는 미분식을, Conc는 수치적분된 함수를, f_resid는 최소화 해야할 오차함수를 나타낸다. f_resid에서 return 해주는 res를 보면 단순한 오차의 제곱합임을 알 수 있다.optimize.minimize 함수로 f_resid를 최소화하는 $k_1$, $k_2$를 구해보면 각각 0.14, -0.025가 나온다. 물리적으로 두 속도상수는 양수가 나와야 한다. E농도 그래프의 경우 음수영역에서 움직인다. 수학적으로는 오차함수를 최소화하는 두 계수를 구했지만 물리적으로 맞지않는다. 생각보다 흔하게 발생하는 일이다.3.1. Constraint for Parameter이런 경우에는 사용한 solver인 optimize.minimize에서 $k_1$, $k_2$의 범위를 정해주는 기능이 있는 경우 사용해주면 된다. 아니면 아래처럼 간단한 트릭을 사용해 줄 수도 있다.def f(y, t, k): C_A = y[0] C_B = y[1] C_C = y[2] C_D = y[3] C_E = y[4] k1 = k[0] k2 = k[1] dCAdt = -k1*(C_A**2)*C_B - (k2**2)*(C_A)*(C_C) dCBdt = -k1*(C_A**2)*C_B dCCdt = k1*(C_A**2)*C_B - (k2**2)*(C_A)*(C_C) dCDdt = k1*(C_A**2)*C_B dCEdt = (k2**2)*(C_A)*(C_C) return [dCAdt, dCBdt, dCCdt, dCDdt, dCEdt]애초에 식을 정의할 때 $k_2$에 제곱을 해준다. 이러면 $k_2$로 무슨 값이 나오든 식에 적용할 때에는 양수가 된다. 이런 방법을 잘 활용하면 exponential로 정의해서 양수가 되게 한다거나 $1/(1+x^2)$ 같이 0과 1사이가 되게 한다거나 하는 것도 생각 할 수 있다. 어차피 식이 복잡해져도 계산은 컴퓨터가 해줄 것이다.Fitting 결과는 다음과 같다.parameter values are [ 1.4959010e-01 -8.4175129e-08]$k_2$ 결과값은 -0.00000008로 제곱하면 거의 0이다. 자연스럽게 시간에 따른 E의 농도는 0이될 것이다. 애초에 음수가 아니면 0이 나올 상황이었던 것 같다.3.2. Absolute Error vs Relative ErrorA~D물질의 경우 나타나지 않는 현상이 E에서만 유독 나타나는 이유는 뭘까. 실험데이터가 틀렸을 수 있고 반응속도식 (3.1) ~ (3.5)가 틀렸을 수 있고 또 하나는 데이터 scale의 문제일 수도 있다.일종의 Cost 함수인 f_resid는 오차제곱합, squared error sum이고 아래 식으로 표현된다.\\[Cost = \\sum_i^m(y_i-f(x_i))^2 \\qquad (4)\\]$y$는 Data, $f(x_i)$는 계산값이다. Data를 잘 보면 A~D 물질들의 시간에 따른 농도 데이터는 대부분 1보다 크다. 하지만 E물질의 경우 농도 데이터는 커봐야 0.1이다. 식(4)로 동일하게 Cost를 적용할 경우 아무래도 값이 큰 A~D에 더 잘 맞도록 $k_1$, $k_2$를 움직일 것이다.다시 말하면 Data의 Scale이 다른 것이고 이게 심할때는 일종의 왜곡이 일어나게 된다. 이런 경우 아래와 같이 상대오차, relative error를 Cost로 사용해 볼 수 있다.\\[Cost = \\sum_i^m \\left({y_i - f(x_i) \\over y_i}\\right)^2 \\qquad (5)\\]위 식에는 오차의 분모부분에 다시한번 data를 넣어주어서 작은 값을 가지는 data에는 잘 맞도록 충분한 가중치를 주고 반대로 큰 값을 가지는 data에는 penalty를 주는 느낌이다.f_resid를 수정하고 돌려보면 결과는 다음과 같다.def f_resid(p): res = 0 for i in range(Data.shape[0]): for j in range(Data.shape[1]): if Data[i][j]==0: continue else: res = res + ((Data[i][j] - Conc(time_data, p)[i][j])/Data[i][j])**2 return resparameter values are [0.10749438 0.11924306]결과는 훨씬 좋다. $k_1$, $k_2$도 물리적으로 말이 되고 E물질 농도에 대해 경향성을 놓치지 않으면서 다른 data에도 적당한 fit을 보여준다. 최종 식(3)에 사용하게 될 속도 상수는 아래처럼 주어진다.\\[k_1 = 0.1075, \\quad k_2 = 0.0142\\]Scipy.optimize를 반드시 써야 하는건 아니다. Multiple ODE regression은 다른 예제도 있고 symfit이라는 라이브러리의 경우 조금 더 직관적으로 이해가 되게끔 구성이 되있기도 하다. 이 예제도 충분히 활용해 볼 만 하다. 어쩌면 끔찍할거같지만 엑셀로도 가능할지 모른다.편한만큼 자유도가 떨어지기도 한다. 반대로 자유도가 높으면 할 수 있는건 많아지지만 신경쓸것도 많아진다.머리가 고생하면 몸이 편하지만, 몸이 고생하면 머리가 편하다.Summary공학용 계산 첫번째 시간으로 미분방정식 풀이와 fitting까지 해봤다. 해석해를 구해서 단순 regression만 해보기도 했고 numerical하게 계산해서 오차함수를 정의한 다음에 이를 최소화 하는 함수를 사용하기도 했다.반응속도식을 세우고 상수를 구한다는게 개별 반응의 특성마다 상황이 굉장히 다르고 수식을 세우기 나름인 부분도 커서 일반적인 해결방법에 대한 얘기를 하지는 못한다. 이런 얘기를 하려면 반응공학 수업을 통째로 들어야 하고 작성자 본인도 잘 모른다.이 포스팅에서는 미분방정식이 세워졌을 때 어떤 식으로 문제를 해결할 수 있는지 Python으로 보여주는걸 목표로 잡았고 그 예제로 반응속도식을 선정한 것 뿐이다. 하지만 애써 수식들이 세워졌더라도 fitting 하는 데에 신경써야 할 부분들이 적지 않음을 알 수 있다.Python만이 이런 문제를 해결할 수 있는 건 아니지만 이만큼 또 편하게 시도해보고 답을 찾을 수 있는 프로그래밍 언어도 없다고 생각한다. 다음번 포스팅에는 여러가지 Regression과 방정식 풀이로 예제를 가져올 생각이다." }, { "title": "[머신러닝] 인공신경망 Artificial Neural Network", "url": "/posts/Neural-Network/", "categories": "Machine Learning, Neural Network", "tags": "Machine Learning, Neural Network, Python", "date": "2022-08-19 19:55:00 +0800", "snippet": "인공신경망에 대해 알아본다. 먼저 수학적인 표현에 대해 살펴본 이후 아주 간단한 구조의 인공신경망에 Gradient Descent를 적용해본다. 정리된 알고리즘대로 Python으로 Machine Learning도 진행한다.1. IntroMichael Nielsen의 “Neural Networks and Deep Learning”은 인공신경망에 대해 공부하기 매우 좋은 자료다. Andrew NG 교수의 Coursera수업과 함께 참고하여 이 포스팅 작성한다.인공신경망은 이름에서 알 수 있듯이 인간의 신경계, 뉴런의 행동양식을 모방해서 만들어졌다는 이야기 들어봤을 것 같다. 반면 실제로 인간뉴런의 행동은 그리 간단하지도 않으며 이런 시각이 인공신경망 기법에 대한 정확한 이해를 방해한다는 의견도 있다.내 생각에도 인공신경망은 수학모델이고 Regression에 활용될 뿐이다. 다만 모든 모양의 함수를 표현할 수 있는 매우 큰 그릇같은 느낌이다. 실제로 수학적으로 그렇다. 증명은 여기서 안다루고 관련 자료는 여기 참고하면 된다.보통 아래 그림처럼 인공신경망을 표현한다.가장 왼쪽 Data가 들어가는 Input Layer에서 시작해서 가운데에는 뭔지 모르지만 여러겹의 Hidden Layer를 거쳐 가장 오른쪽의 Output Layer를 통해 결과가 나온다. 원으로 표현된 각 Unit들은 화살표로 빼곡하게 그물망처럼 연결되어 있다.직관적인 느낌은 줄 수 있지만 아직 어떻게 작동하는지 정확히 모르겠다.2. Mathematical Expression아래 그림처럼 가운데 Hidden Layer를 모두 지우고 각 Layer에 Unit들은 한개씩 있는 인공신경망을 생각해본다.위 그림 제일 왼쪽 Input Unit안에 x하나가 있다. x는 Input이고 변수(variable)이며 특성(feature)이라고도 부른다. 화살표를 따라 오른쪽으로 가면 Output이 나오며 모델을 통한 결과값으로 hypothesis를 의미하는 h로 표시되 있다.선을 따라 오른쪽으로 가다보면 $\\theta_1$, $\\theta_0$를 만나며 input인 x와의 선형 조합 과정을 거친다. Slope에 해당하는 $\\theta_1$은 weight, intercept에 해당하는 $\\theta_0$ bias라고 부른다. 편의상 이 중간 과정을 $a$로 표현하면 아래와 같다.\\[a = \\theta_1 x + \\theta_0 \\qquad (1)\\]선형 조합 이후 순차적으로 활성화(activation)이라는 과정을 거쳐 최종 Output을 얻는다. Activation 함수는 여러가지 선택지가 있으며 이 포스팅에서는 sigmoid 함수를 사용하겠다.\\[g(x) = {1 \\over {1 + {e}^{-x}}}\\qquad (2)\\]Sigmoid 함수는 아래그림처럼 생겼다. x가 일정값 이하일때 함수값이 0에 가깝다가 일정값 이상이 되면 급격히 1에 가까워지는 성질로 활성화를 표현한다.결국 최종 h는 아래 식과 같이 표현된다. (여기까지는 Logistic Regression과 완전히 동일한 형태이다.)\\[h_\\theta (x) = g(a) = {1 \\over {1 + {e}^{-(\\theta_1 x + \\theta_0)}}} \\qquad (3)\\]정리해보면 각 Unit들을 이어주는 선은 $\\theta$로 표현되는 weight, bias를 의미하고 왼쪽에서 오른쪽으로 선을 따라가면서 순차적으로 선형조합(식(1))과 활성화(식(2))라는 과정을 거친다. 최종 계산값은 선형조합과 활성화를 거친 식(3)으로 주어진다.인공신경망을 이용한 머신러닝이라고 한다면 $\\theta$로 표현되는 parameter들을(weight, bias) 데이터에 잘 맞게 regression하는 과정일 것이다.좀 더 복잡한 구조로 Input, Output Layer에 Unit이 각각 2개인 경우를 생각할 수 있다.변수는 $x_1$, $x_2$ 2종류, output도 $h_1$, $h_2$로 2개이며 아래 수식과 같이 정의된다.\\[h_{1, \\theta}(x_1, x_2) = {1 \\over {1 + {e}^{-(\\theta_{01}+\\theta_{11}x_1 + \\theta_{21}x_2)}}} \\qquad (4)\\]\\[h_{2, \\theta}(x_1, x_2) = {1 \\over {1 + {e}^{-(\\theta_{02}+\\theta_{12}x_1 + \\theta_{22}x_2)}}} \\qquad (5)\\]각 문자들에 붙는 아래첨자에 숫자들이 늘어나서 좀 복잡해 보일 수 있다. $\\theta_{ij}$는 i번째 변수에서 j번째 output에 관련된 weight를 나타내며 i가 0인경우 j번째 output에 관여하는 bias다. 간단히 생각하면 각각의 output unit에 모이는 변수, weight, bias를 선형조합하고 활성화 과정을 거치는 동일한 방법이다.마지막으로 Hidden Layer를 한개만 추가한 경우를 표현해보자. 아래 그림에는 Input, Hidden, Output Layer에 Unit이 각각 2개씩 총 6개인 구조의 인공신경망이 있다.편의상 hidden layer의 각 unit별 output을 $a$로 표현한다. $\\theta_{ij}^{k}$는 왼쪽부터 k번째 layer에 있는 i번째 input에서 k+1번째 layer에 있는 j번째 output으로 향하는 weight를 나타내고 i가 0인 경우는 bias다. 말로 설명하니까 굉장히 이상한데, 달라진건 아무것도 없다. 동그란 unit으로모이는 화살표들에 관련된 변수와 parameter들을 선형조합하고 활성화하는 동일한 규칙이 적용된다.중간단계에 있는 $a_1$, $a_2$는 아래와 같이 표현된다.\\[a_{1} = {1 \\over {1 + {e}^{-(\\theta_{01}^1+\\theta_{11}^1x_1 + \\theta_{21}^1x_2)}}} \\qquad (6)\\]\\[a_{2} = {1 \\over {1 + {e}^{-(\\theta_{02}^1+\\theta_{12}^1x_1 + \\theta_{22}^1x_2)}}} \\qquad (7)\\]잘 보면 식 (4), (5)와 동일하다. 동일한 규칙, 동일한 구조로 동일한 결과가 나온다. 최종 output인 $h_1$과 $h_2$는 아래와 같다.\\[h_{1} = {1 \\over {1 + {e}^{-(\\theta_{01}^2+\\theta_{11}^2a_1 + \\theta_{21}^2a_2)}}} \\qquad (8)\\]\\[h_{2} = {1 \\over {1 + {e}^{-(\\theta_{02}^2+\\theta_{12}^2a_1 + \\theta_{22}^2a_2)}}} \\qquad (9)\\]만일 hidden layer가 1개층이 아닌 여러개의 층으로 구성되어 있다면 $a_i$였던 것들을 $a_i^j$로 j번째 layer에 있는 i번째 unit 처럼 표현해줄 수 있겠다.절대 식 하나하나 기억할 필요 없다. 한번정도 눈으로 꼼꼼히 따라가보면 좋겠지만 그마저도 크게 필요없다. 오직 기억 할만한 것은 인공신경망은 Input과 Parameters의 선형조합과 이후 활성화 과정으로 이루어진다는 점이다.2.1. Hyper-parameterweight와 bias처럼 선형조합에 사용되는 일종의 coefficient들을 parameter라고 불렀다. 추가로 hidden layer의 layer층 개수나 unit개수 등 구조에 관련된 숫자를 특별히 hyper-parameter라고 부른다. Data의 크기를 포함한 다양한 이유로 hidden-layer층 개수는 많을수도, 적을수도 있다. 관련한 자세한 내용이 궁금하면 이 문서가 좋아보인다.Parameter는 Gradient-Descent같은 방식으로 구하지만 hyper-parameter는 그러지 못한다. 경험적으로 구하거나 trial-error를 해야하며 정형화된 방법은 없는 것 같다. 그냥 일단 해보고 결과보고 그에따라 대응하는 것 같다.2.2. Vectorization식들을 잘 보면 덧셈이 많다. 위 식들은 Feature vector와 parameter vector들의 dot product, 내적으로 표현하면 여러번 반복해서 써야하는 수고로움을 덜 수 있다. 예를 들어 아래와 같은 단순합을 vector를 이용해서 간단하게 표현해보자.\\[S = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + ... + \\theta_n x_n\\]\\[Let \\quad \\vec \\theta = [\\theta_0, \\theta_1, ... , \\theta_n], \\quad \\vec x = [1, x_1, x_2, ..., x_n]^T\\]\\[S = \\vec \\theta \\cdot \\vec x\\]bias에 대응될 변수에는 1을 넣어주고 dot product 하면 끝이다. 문자로 표현해서 인공신경망의 크기나 구조에 상관없이 일반화된 식을 사용할 수도 있다. 자주 얘기하는 Andrew NG교수 4주차 수업에서 vectorization을 활용해 인공신경망을 표현하는 것을 볼 수 있다.굳이 이 얘기를 하는건 코드를 짤 때도 vectorized 형태로 연산을 정의하면 단순 반복문의 경우보다 계산속도가 훨씬 빠르기 때문이다. 단순 for문 보다 numpy array같은거 이용해서 계산하는게 훨씬 빠르긴 하다. 수치연산 관련 코드가 vector나 행렬 형태에서 좀 더 최적화되어 움직이도록 되어 있는 것 같다. 나도 잘 몰라서 더 얘기하기는 힘들다.지금은 크게 신경쓰지 않아도 좋다. 어차피 이런거 신경쓸 정도의 무거운 계산을 하기까지 앞으로 멀기도 했고 그때 쓰는 라이브러리에서 이미 최적화된 방식으로 잘 해줄 것이다.3. Gradient Descent앞에서 얘기한 Input에서 Output까지 가는 일련의 계산 흐름을 FeedForward라고 부른다. 반대로 가는 과정을 Back-Propagation이라고 부르며 Cost를 최소화하는 parameter들을 구하는 과정을 말한다.수식을 간단하게 하기 위해서 hidden layer없이 feed unit 2개, output unit 1개짜리 인공신경망을 만들고 Gradient Descent를 적용해 본다.Output $h$는 다음식으로 표현된다.\\[h = {1 \\over {1 + {e}^{-(\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2)}} } \\qquad (10)\\]연습삼아 Vectorized Form으로 표현 해보자. x vector안에 $x_0$가 추가된 것에 주의해야한다. 변수를 추가한 것은 아니며 단순히 bias를 받아주기 위한 처리에 불과하다. ($x_0$ 값은 모두 1이다.)\\[\\vec x = [x_0(=1), x_1, x_2]^T,\\quad \\vec \\theta = [\\theta_0, \\theta_1,\\theta_2]\\]\\[h_{\\vec \\theta}(\\vec x) = {1 \\over {1 + {e}^{-(\\vec \\theta \\cdot \\vec x)}} } = g(\\vec \\theta \\cdot \\vec x)\\]모델값과 Data간의 오차의 합을 Cost라고 하고 아래식으로 주어진다.\\[J(\\vec \\theta) = {1 \\over{2m}} \\sum_{i=1}^m (h_{\\theta}(x^i)-y^i)^2 \\quad \\quad (11)\\]목표는 Cost를 최소화하는 parameter vector $\\vec\\theta$ 를 구하는 것이다. Gradient Descent에 따르면 이 때 Parameter Update 방식은 아래 식으로 주어진다. (12a, b는 동일한 표현이다.)\\[\\theta_i := \\theta_i - \\alpha {\\partial \\over \\partial {\\theta_i}} J(\\vec \\theta) \\quad (i=0,1,2) \\qquad (12a)\\]\\[\\Delta \\vec \\theta = -\\alpha\\nabla J(\\vec \\theta) \\qquad (12b)\\]남은 일은 편미분하는 일이다.\\[{\\partial \\over \\partial {\\theta_i}}\\left({1 \\over{2m}} \\sum_{i=1}^m (h_{\\theta}(x^i)-y^i)^2\\right) = {1 \\over {m}}\\sum_{i=1}^m (h_{\\theta}(x^i)-y^i){\\partial \\over \\partial {\\theta_i}}h_{\\theta}(x)\\]\\[= {1 \\over {m}}\\sum_{i=1}^m (h_{\\theta}(x^i)-y^i){\\partial \\over \\partial {\\theta_i}}g(\\vec \\theta \\cdot \\vec x)\\]\\[= {1 \\over {m}}\\sum_{i=1}^m (h_{\\theta}(x^i)-y^i)g'(\\vec \\theta \\cdot \\vec x) x_i\\]sigmoid 함수의 미분식은 아래와 같이 정리된다.\\[g(x) = {1 \\over {1 + {e}^{-x}}} \\quad g'(x) = {-{e}^{-x} \\over {(1+{e}^{-x})^2}} = g(x)(1-g(x))\\]따라서 식 12a, b는 아래식처럼 정리된다.\\[\\theta_i := \\theta_i - \\alpha {1 \\over {m}}\\sum_{i=1}^m (h_{\\theta}(x^i)-y^i)g(\\vec \\theta \\cdot \\vec x)(1-g(\\vec \\theta \\cdot \\vec x)) x_i \\qquad (13)\\]좀 더 복잡한 구조에 대한 일반화된 식과 이 구조에서의 Back-propagation에 대해 수식으로 유도하고 정리된 자료를 보고싶다면 Michael Nielsen의 “Neural Networks and Deep Learning”책의 이 Chapter 정독해보자. Notation은 좀 다르지만 원하는 것을 얻을 수 있다.4. Application필요한 식 정리가 끝났다. 데이터는 아래 정리된 표를 사용할 것이다. 식 유도에 사용된 인공신경망 구조에 맞게 Input의 종류는 2가지, Output은 1가지이다. $x_1$ $x_2$ Output 0 0 0 1 0 1 0 1 1 1 1 1 표를 잘 보면 $x_1$, $x_2$ 둘 중 하나만 1이어도 결과값이 1이고 두개 모두 동시에 0일때만 결과값은 0인 관계임을 알 수 있다. 이런 관계를 logical OR function이라고 한다. (보통 0은 거짓, 1은 참을 의미하며 OR연산에서는 둘 중 하나만 참이어도 결과는 참이다.)달리 표현하면 머신러닝을 활용해서 인공신경망으로 logical OR function을 만들어 볼 것이다. (실은 logistic regression이다.)필요한 라이브러리를 Import 해주자. Random은 parameter들의 초기값을 정하는데 써준다.import numpy as npimport random, osimport matplotlib.pyplot as plt위 표에 있는 데이터 입력한다. x0는 1에 해당하며 bias 부분을 표현해 주기 위해 쓰인다.weights = list()for k in range(3): weights.append(random.random())weights = np.array(weights)x0data = [1, 1, 1, 1] # x0 = 1x1data = [0, 1, 0, 1] # x1x2data = [0, 0, 1, 1] # x2x = np.array([x0data, x1data, x2data])ydata = [0, 1, 1, 1] # OutputLearning rate는 학습을 몇 번 돌려보면서 정했다. 총 1000번 학습(parameter update)가 이루어지게 설정했다. err_list는 학습 횟수가 늘어남에 따라 Cost가 어떻게 변하는지 보기 위해 쓰인다.alpha = 40err_list = []def sigmoid(x): return 1/(1+np.exp(-x))# m번째 data와 weights간 선형조합def lincomb(m): return np.dot(weights, x[:, m])# m번째 data에서 계산한 모델값과 데이터간 차이def err(m): return (sigmoid(lincomb(m)) - ydata[m])for j in range(1000): grad_x0 = 0 grad_x1 = 0 grad_x2 = 0 err_loc = 0 for i in range(len(x1data)): grad_x0 = grad_x0 + err(i)*sigmoid(lincomb(i))*(1-sigmoid(lincomb(i)))*x0data[i] grad_x1 = grad_x1 + err(i)*sigmoid(lincomb(i))*(1-sigmoid(lincomb(i)))*x1data[i] grad_x2 = grad_x2 + err(i)*sigmoid(lincomb(i))*(1-sigmoid(lincomb(i)))*x2data[i] err_loc = err_loc + (err(i)**2) err_list.append(err_loc) weights[0] = weights[0] - alpha * grad_x0 * (1/len(x1data)) weights[1] = weights[1] - alpha * grad_x1 * (1/len(x1data)) weights[2] = weights[2] - alpha * grad_x2 * (1/len(x1data))print(weights)for i in range(len(x1data)): print(f\"(x1={x1data[i]}, x2={x2data[i]}) =&gt; y={ydata[i]}, Model Output = {sigmoid(lincomb(i)):.2f}\")plt.plot(err_list)plt.title(\"Error During Learning\")plt.xlabel(\"Epoch\")plt.ylabel(\"Error\")plt.show()[-4.1032397 8.67424375 8.67424407](x1=0, x2=0) =&gt; y=0, Model Output = 0.02(x1=1, x2=0) =&gt; y=1, Model Output = 0.99(x1=0, x2=1) =&gt; y=1, Model Output = 0.99(x1=1, x2=1) =&gt; y=1, Model Output = 1.00위 결과를 보면 데이터와의 오차가 충분히 작아졌다고 판단된다.x_plot_whole = np.arange(-0.25, 1.25, 0.1)x_plot_1 = np.array([1, 0, 1])y_plot_1 = np.array([0, 1, 1])x_plot_0 = np.array([0])y_plot_0 = np.array([0])y_line = (1/weights[1])*(-weights[2]*x_plot_whole - weights[0])plt.scatter(x_plot_1, y_plot_1, marker = 's')plt.scatter(x_plot_0, y_plot_0, marker = 's')plt.plot(x_plot_whole, y_line, 'r')plt.xlabel(\"x1\")plt.ylabel(\"x2\")plt.show()(x1, x2, y) 형태의 data를 2d로 표현해보면 아래 그림과 같다. 파란색 점은 y=1인 경우고 주황색 점은 y=0인 경우에 해당한다. 빨간 선은 학습결과 구해진 weight를 가지고 선형조합 부분을 표현한 것이며 x1과 x2로 이루어진 plane을 y=0과 y=1이 나오는 부분으로 나눠주는 것을 볼 수 있다.빨간색 선 아래 부분은 활성화되지 못하는 영역, 빨간색 선 위 부분은 활성화되는 영역이라고 생각해도 될 것 같다. 빨간선을 Decision Boundary라고 부르기도 한다.Summary인공신경망의 작동방식을 확인해 봤다. 간단히 말하면 Features와 Parameter의 선형조합 이후 활성화 과정으로 이어지는 순차적인 연산이었다.실은 hidden layer가 없는 인공신경망은 결국 logistic regression과 동일한 구조라서 인공신경망을 했다고 말하기는 좀 민망하긴 하다. 하지만 작동원리를 알아보는데는 이정도면 충분할 것 같다. 결국 서로 복잡하게 얼키고 설킨 logistic regression이 인공신경망이라고 생각한다.수식은 나름 정리해봤지만 정말 간단한 케이스에 대해서만 봤을 뿐이다. 진짜 수학적으로 궁금한게 많다면, Michael Nielsen의 “Neural Networks and Deep Learning” 다시 한번 추천한다.인공신경망 먼저 정리해보긴 했는데 다른 모델들도 많다. 인공신경망을 이용한 머신러닝은 Tensorflow, 그외 다양한 방법들은 scikit-learn이 Python 라이브러리로 유명하다. 아, Pytorch도 있다. 이제는 라이브러리를 어떻게 쓰는지 공부해야 한다.." }, { "title": "[머신러닝] Linear Regression with Gradient Descent", "url": "/posts/Gradient-Descent/", "categories": "Machine Learning, Gradient Descent", "tags": "Machine Learning, Gradient Descent, Linear Regression, Python, AI", "date": "2022-08-08 19:55:00 +0800", "snippet": "수치해석시간에 배우는 내용들 기억한다면 빠르게 Skip해도 좋다. 후반부에 Python으로 작성한 코드도 있기 때문에 한 번 정도는 보는 것도 좋을 것 같다.Gradient Descent 방식을 통해 Linear Regression을 진행한다. Machine Learning이란걸 해본다.1. Intro머신러닝에서 사용되는 알고리즘중 하나인 Gradient Descent, 경사하강법에 대해서 간단히 작성한다. 가능한 Andrew NG 교수의 Notation을 따르려고 한다.머신러닝 중에서 답이 함께 있는 데이터를 사용하는 Supervised learning에 해당하며 단순 Linear Regression에 대한 예제로 설명한다. Data는 대표적인 Toy dataset인 iris를 이용한다.2. Background아래와 같이 변수 x에 대한 결과값 y에 대한 Dataset이 있고 우리는 이 둘 사이의 관계를 설명해주는 모델을 만들어야 한다. 모델이 만들어지면 데이터에는 없던 새로운 x를 넣었을 때 결과값인 y를 예측해 줄 것이다.\\[(x^1, y^1)\\]\\[(x^2, y^2)\\]\\[(x^3, y^3)\\]\\[...\\]\\[(x^m, y^m)\\]데이터쌍 (x, y)의 개수는 m개 이다.데이터를 Plot 해봤는데 선형인 관계가 보였다면 자연스럽게 아래와 같이 선형식을 통해 모델을 만들려고 시도할 것이다.\\[h_{\\theta} = {\\theta}_0 + {\\theta}_1 x \\quad \\quad (1)\\]h는 hypothesis를 나타내며 parameter로 ${\\theta}_0, {\\theta}_1$을 쓰는 선형모델로 가정한다는 의미다.이제 문제는 맨 위의 Dataset에 가장 잘 맞는 h식을 찾는 것이며 결국 아래 오차식 $J(\\theta)$을 최소화 시키는 parameter인 ${\\theta}_0, {\\theta}_1$를 찾는 것으로 정리할 수 있다.\\[J({\\theta}_0, {\\theta}_1) = {1 \\over{2m}} \\sum_{i=1}^m (h_{\\theta}(x^i)-y^i)^2 \\quad \\quad (2)\\]최종 목표인 오차함수 최소화는 아래처럼 표현한다.\\[\\underset{\\theta_0, \\theta_1}{min} \\ J(\\theta_0, \\theta_1) \\quad \\quad (3)\\]위 최소화 문제를 만족시키는 Parameter인 ${\\theta_0, \\theta_1}$를 구하는 방식 중에 하나가 여기서 얘기하려는 Gradient Descent 방식이며 일종의 Parameter Update Rule이라고 할 수 있다.결론부터 말하자면 Parameter Update Rule은 아래와 같다.\\[\\theta_i := \\theta_i - \\alpha {\\partial \\over \\partial {\\theta_i}} J(\\theta_0, \\theta_1) \\quad \\quad (4)\\]$\\alpha$는 learning rate라 불리는 임의의 양수이다. Gradient Descent 알고리즘의 동작은 다음의 단계를 거친다. 초기 ${\\theta_0, \\theta_1}$를 가정한다. Parameter Update rule에 따라 반복적으로 ${\\theta_0, \\theta_1}$를 변화시킨다. Convergence가 확인되면 최종 ${\\theta_0, \\theta_1}$를 반환하고 반복을 종료한다.3. DerivationGradient Descent의 핵심인 Parameter Upadate 식(4)는 수학적으로 아래와 같이 유도된다.Cost Function $J({\\theta}_0, {\\theta}_1)$에 대한 미분은 아래와 같이 근사시킬 수 있다.\\[\\Delta J({\\theta}_0, {\\theta}_1) \\approx {\\partial J({\\theta}_0, {\\theta}_1) \\over \\partial {\\theta_0}} \\Delta {\\theta_0} + {\\partial J({\\theta}_0, {\\theta}_1) \\over \\partial {\\theta_1}} \\Delta {\\theta_1}\\]\\[= ({\\partial J({\\theta}_0, {\\theta}_1) \\over \\partial {\\theta_0}}, {\\partial J({\\theta}_0, {\\theta}_1) \\over \\partial {\\theta_1}})\\cdot (\\Delta {\\theta_0}, \\Delta {\\theta_1})\\]\\[= \\nabla J_{\\theta_i} \\Delta{\\theta_i}\\]위 식은 ${\\theta}$가 변화하는 방향 ($\\Delta{\\theta_i}$)에 따라 Cost Function이 어떻게 변하는지($\\Delta J({\\theta}_0, {\\theta}_1)$) 알려준다.$J(\\theta_0 , \\theta_1)$를 최소화하기 위해 $\\Delta J(\\theta_0 , \\theta_1)$가 음수가 되도록 ${\\theta}$의 방향을 설정해 주자는 것이 Gradient Descent의 주된 아이디어 이다. 이를 위해 $\\Delta \\theta_i$를 아래 식과 같이 가정해보자.\\[{\\Delta {\\theta_i} = -\\alpha \\nabla J_{\\theta_i}} \\quad \\quad (5)\\]$\\alpha$는 learning rate라 불리는 임의의 양수이다. 이어서 $\\Delta J$는 아래식과 같이 주어진다.\\[\\Delta J({\\theta}_0, {\\theta}_1) \\approx \\nabla J_{\\theta_i} \\Delta{\\theta_i}\\]\\[= \\nabla J_{\\theta_i} (-\\alpha \\nabla J_{\\theta_i})\\]\\[= -\\alpha \\left| \\nabla J_{\\theta_i} \\right|^2 \\leqq 0\\]정리해보면 식(5)를 통해 $\\Delta J$가 음수가 되도록 $\\theta$를 update해주는 방식으로 Cost Function을 최소화 시켜주는 알고리즘이 Gradient Descent 이다. (좀 더 정확히는 Batch Gradient Descent 이다.)4. Calculation with Python필요한 식 정리가 끝났다. 실제로 Gradient Descent 알고리즘을 통해 Linear Regression을 해보자. 할 수 있는 가장 간단한 형태의 Machine Learning이다.계산에 사용할 Python Module을 import 해준다.import numpy as npimport matplotlib.pyplot as pltimport pandas as pdToy Dataset으로 iris를 이용할 것이다. (붓꽃 꽃받침의 너비/길이, 꽃잎의 너비/길이를 모아놓은 dataset이다.)csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'col_names = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width','Class']data = pd.read_csv(csv_url, names = col_names)변수들간의 관계를 알아보기 위해 두개씩 선택하여 Plot 해본다.Septal_length는 꽃받침 길이, Septal_width는 꽃받침 너비, Petal_length는 꽃잎 길이, Petal_width는 꽃잎의 너비이다.x1 = np.array(data['Sepal_Length'].to_list())x2 = np.array(data['Sepal_Width'].to_list())x3 = np.array(data['Petal_Length'].to_list())x4 = np.array(data['Petal_Width'].to_list())plt.subplots(constrained_layout=True)plt.subplot(2, 3, 1), plt.scatter(x1, x2), plt.xlabel('Sepal_Length'), plt.ylabel('Sepal_Width'), plt.title('x1 vs x2')plt.subplot(2, 3, 2), plt.scatter(x1, x3), plt.xlabel('Sepal_Length'), plt.ylabel('Petal_Length'), plt.title('x1 vs x3')plt.subplot(2, 3, 3), plt.scatter(x1, x4), plt.xlabel('Sepal_Length'), plt.ylabel('Petal_Width'), plt.title('x1 vs x4')plt.subplot(2, 3, 4), plt.scatter(x2, x3), plt.xlabel('Sepal_Width'), plt.ylabel('Petal_Length'), plt.title('x2 vs x3')plt.subplot(2, 3, 5), plt.scatter(x2, x4), plt.xlabel('Sepal_Width'), plt.ylabel('Petal_Width'), plt.title('x2 vs x4')plt.subplot(2, 3, 6), plt.scatter(x3, x4), plt.xlabel('Petal_Length'), plt.ylabel('Petal_Width'), plt.title('x3 vs x4')plt.show()오른쪽 아래 그림(꽃잎 길이 - 꽃잎 너비)가 가장 선형으로 보인다. 이 Dataset만 쓰기로 한다.x = x3 # Petal Lengthy = x4 # Petal Width선형식인 $h = \\theta_1 x + \\theta_0$와 Cost Function $J(\\theta_0, \\theta_1)$을 함수로 선언한다. (각각 y_pred, J_tot로 이름붙였다.) 편의상 기울기 $\\theta_1$을 m으로, 절편 $\\theta_0$를 b로 표현한다.m=0.4, b=-0.5일때 얼추 잘 맞는 것으로 보이며 이 때 Cost값은 0.04 정도이다.def y_pred(m, b, x): return m*x+bdef J_tot(m, b, x, y): return (1/(2*len(x)))*sum(((m*x+b)-y)**2)plt.plot(x, y_pred(0.4,-0.5,x),'r')plt.scatter(x, y)plt.xlabel(\"Petal Length\")plt.ylabel(\"Petal Width\")plt.show()print(J_tot(0.4, -0.5, x, y))0.040613333333333335가장 핵심인 Gradient Descent 알고리즘 부분이다. learning rate $\\alpha$는 0.1로 했으며 parameter m, b의 초기값은 각각 1.5, -4로 시작하였다. 1000번의 반복문을 돌게 하였으며 이전 계산에 비해 Cost가 0.00001도 감소하지 않는다면 수렴했다고 가정하고 반복문을 나오도록 한다. 총 242번 계산끝에 수렴되었다.m = 1.5b= -4J_list = []m_list = []b_list = []alpha = 0.1iteration = 1000tol = 0.00001for i in range(iteration): m_list.append(m) b_list.append(b) grad_m = (-1/len(x))*np.sum((y-(m*x+b))*x) grad_b = (-1/len(x))*np.sum(y-(m*x+b)) m = m - alpha*grad_m b = b - alpha*grad_b J_list.append(J_tot(m, b, x, y)) if i&gt;2: if J_list[i-1]-J_list[i] &lt; tol: print(f\"{i}번째 계산 끝에 수렴 완료\") print(f\"기울기 = {m}, 절편 = {b}\") break242번째 계산 끝에 수렴 완료기울기 = 0.4286328132289579, 절편 = -0.4219131892313386아래코드는 parameter m, b에 따른 Cost값 Contour map으로 표현하고 학습이 진행되는 동안 parameter update되는 궤적을 표현해준다.xlist = np.linspace(-1, 2, 400)ylist = np.linspace(-5, 5, 400)X, Y = np.meshgrid(xlist, ylist)Z = np.zeros(X.shape)for i in range(X.shape[0]): for j in range(X.shape[0]): Z[i][j] = J_tot(X[i][j], Y[i][j], x, y)m_list_p = [m_list[i] for i in range(len(m_list)) if i % 5 == 0]b_list_p = [b_list[i] for i in range(len(b_list)) if i % 5 == 0]fig, ax = plt.subplots(1, 1)levels1 = np.linspace(0.03, 0.1, 2)levels2 = np.linspace(0.2, 1, 3)levels3 = np.linspace(4, 30, 4)levels = np.concatenate([levels1, levels2, levels3])cp = ax.contour(X, Y, Z, levels)plt.plot(m_list_p, b_list_p, 'rx-')plt.title(\"Contour map for Cost with m and b\")plt.xlabel(\"m\")plt.ylabel(\"b\")plt.show()빨간선이 parameter update되는 궤적을 나타낸다. Cost가 가장 작은 부분인 가운데 작은 등고선으로 parameter가 이동하는 것을 볼 수 있다.최종 결과를 Plot 해본다. 반복계산 횟수(학습 횟수라고도 부른다)에 따라 Cost가 감소하는 것을 알 수 있으며 최종 결과식과 data의 경향성이 잘 맞는 것을 확인할 수 있다.plt.plot(J_list)plt.title(\"Cost vs No. of Iteration\")plt.xlabel(\"No. of Iteration\")plt.ylabel(\"Cost\")plt.show()plt.title(\"Calculated Results\")plt.plot(x, y_pred(m, b, x))plt.scatter(x, y)plt.xlabel(\"Petal Length\")plt.ylabel(\"Petal Width\")plt.show()SummaryLabeled Data(붓꽃 꽃잎의 길이-너비 data)를 이용해서 첫번째 Machine Learning을 해보았다. Cost Function을 최소화하는 선형식 h의 parameter를 Gradient Descent 알고리즘으로 구해보았고 실제로 잘 작동하는 것을 확인했다. 모든 경우에 잘 작동하는 것은 아니며 Cost Function이 Convex한 형태여야만 global minimum에 도달할 수 있으며 아닌 경우 local minimum에 빠질 수 있다. 더 궁금해서 자세한 설명이 필요하면 이 문서혹은 Andrew Ng교수의 수업을 추천한다.이 분야에서 말하는 ‘학습’이라는 단어에 대해 이제는 다른 느낌을 가졌으면 좋겠다. 결국 Gradient Descent 알고리즘이 제안하는 parameter update 방식을 반복해서 적용하는 것(오차를 최소화 해나가는 과정)이 ‘학습’을 의미했다. Cost를 최소화 해나가는 반복된 parameter update과정이며 결국 Fitting이고 Regression이다." }, { "title": "[머신러닝] 시작 - 용어정리", "url": "/posts/AI-Definition/", "categories": "Machine Learning, Definition", "tags": "AI, Machine Learning", "date": "2022-08-05 20:55:00 +0800", "snippet": "인공지능 (Artificial Intelligence, AI), 머신러닝 (Machine Learing, ML), 딥러닝 (Deep Learning, DL)의 정의에 대해 찾아본 내용을 정리한다. 시간이 좀 걸리더라도 용어에 대한 정의를 제대로 확인한 후에 더 깊게 들어 가는것이 결국 가장 빠른 방법이다. 여기에는 용어별 정의와 적용/활용 되는 범위와 공부하는데 좋았던 사이트, 자료들 출처도 함께 정리한다.다른 포스팅에 간단한 수학과 Python 프로그래밍 예제 정리할 계획이다.1. Artificial Intelligence1.1. Definition일단 Google에 검색을 해본다. DataRobot이란 곳에서 아래와 같이 말한다. “An AI is a computer system that is able to perform tasks that ordinarily require human intelligence. These artificial intelligence systems are powered by machine learning. Many of them are powered by machine learning, some of them are powered by specifically deep learning, some of them are powered by very boring things like just rules.”좀 애매한 부분이 있어도 간단하고 직관적이다. 인간이 해결할만한 지능이 필요한 문제를 해결해주는 컴퓨터 시스템을 AI라고 부르자 하고 말한다. 실은 대부분의 이쪽분야 분들은 위처럼 얘기하는 것 같았다. 이 페이지에서도 AI는 사람의 행동을 흉내내는 모든 컴퓨터 기술을 말한다고 한다.그 유명한 Andrew NG 교수님의 Coursera 강의에서는 AI를 적용 범위에 따라 두 개로 분류하기도 한다. AGI (Artificial General Intelligence) Do Anything a human can do ANI (Artificial Narrow Intelligence) E.g., smart speaker, self-driving car, web search, AI in farming and factoriesAGI는 그냥 사람복사본이고 ANI는 하나의 기능만 가지는 것으로 말하는 것 같다.여기까지 정리해보면 AI는 인간이 할만한 지능이 필요한 일을 해주는 컴퓨터 시스템 이다.1.2. HierachyGoogle에서 검색해보면 아래 그림같은 다이어그램 많이 볼 수 있다.여기서는 AI는 사람의 행동을 흉내내는 기술로 설명한다. AI가 가장 넓은 범주이고 그 안에 머신러닝과 딥러닝이 보인다.2. Machine LearningAI분야의 처음은 인간이 할 만한 복잡한 일을 컴퓨터가 해결하도록 하기 위해 많은 규칙을 집어넣는 과정이었던 것 같다. 즉 사람이 직접 모든 행동 규칙들을 코드로 짜고 컴퓨터는 주어진 코드대로 행동하는 것이다. 하지만 사진에서 글자를 인식하거나 고양이와 개를 구분하거나 하는 일에 대한 규칙을 정하는 것은 매우 어렵고 어쩌면 불가능하다.이와 반대로 수많은 데이터를 입력해주고 컴퓨터가 직접 그 규칙을 찾도록 하는 방식이 제안되었고 보통 이런 방식을 머신러닝이라고 부른다.2.1. Definition이 책에서는 머신러닝을 아래와 같이 설명한다. Machine learning, as the name suggest, are a group of algorithms that try to enable the learning capability of the computers, so that they can learn from the data or past experiences. The idea is that, as a kid, we gain many skills from learning. One example is that we learned how to recognize cats and dogs from a few cases that our parents showed to us. We may just see a few cats and dogs pictures, and next time on the street when we see a cat, even though it may be different from the pictures we saw, we know it is a cat. This ability to learn from the data that presented to us and later can be used to generalize to recognize new data is one of the things we want to teach our computers to do.과거 경험이나 데이터로부터 배우는 행동을 할 수 있는 알고리즘을 머신러닝이라고 정의한다.Laurence의 Coursera 강의에서는 이를 조금 틀어서 아래와 같이 설명한다.즉 전통적인 AI분야에서는 미리 하드코딩된 Rule과 Input을 주면 답이 나오는 방식이었지만 현재의 머신러닝은 Data와 답을 주어주면 이를 위한 방법을 결과로 주는 알고리즘을 말한다.여기까지 정리해보면 Machine Learning은 AI의 한 분야로 Data로 부터 학습하는(답을 주는 Rule을 찾는) 알고리즘 이다.학습을 한다고 표현을 하니 막연하게 느껴질 수도 있지만, 예시를 한 번 보면 그냥 결국 Fitting 하는거구나 생각이 든다. 추상적이고 막연한 느낌을 가지는 것보다는 그냥 수학이구나 하는 생각을 가지는 게 거리감도 덜 들고 처음에 그냥 해보기에 좋다고 생각한다.넓게 보면 Linear Regression도 Machine Learning이다.2.2. Hierachy이 책에 소개된 것처럼 머신러닝은 보통 아래 그림과 같이 분류된다.예를 들어 개와 고양이를 구분하는 머신러닝 모델을 만들기 위해 데이터를 모은다고 상상해보자. 컴퓨터에게 어떤 사진이 개인지 또는 고양이인지 알려주기 위해 각 사진 파일이름을 개 혹은 고양이라고 작성한다. (이를 Labeling한다고 한다.) 이렇게 정리된 데이터를 특정 알고리즘을 통해 학습시키면 개와 고양이를 구분하는 머신러닝 모델이 만들어진다.이처럼 답이 함깨 정리된 data를 학습에 사용하는 경우는 Supervised Learning이라고 부르며 ‘개 or 고양이’ 또는 ‘참 or 거짓’처럼 이산값이 결과값인 경우는 Classification에 해당하고 내일의 기온을 예측하는 경우와 같이 연속적인 값이 결과값인 경우 Regression이다. 수학에서 말하는 정의와 약간씩 다를 수는 있지만 어쨌든 이쪽분야에서는 이런 느낌으로 얘기하는 것 같다.답이 함께 정리되지 않은 data를 학습에 사용하는 경우는 Unsupervised Learning이라고 부르며 그림에서는 크게 Clustering과 Dimensionality reduction으로 구분하지만 실은 이분야는 이런식으로 구분하기는 쉽지 않고 그냥 이런 분야가 있다 정도로 생각하면 좋을 것 같다.참고로 Andrew NG교수의 위 모든 분야와 하나하나의 모델을 겪어볼 수 있는 범위의 Coursera 강의가 있다. 긴말 필요없이 강추.3. Deep Learning위에 Machine Learning 분류표를 보면 맨 아래 네개의 분류가 있다. 이 중 Classification과 Regression을 보면 Neural Network라는 것이 보인다. Artificial Nerual Network, 인공신경망으로 불리는 이것은 머신러닝 기법 중의 하나로 IBM에서 잘 설명한 문서가 있다.위 그림처럼 인공신경망은 입력층, 은닉층, 출력층 등의 여러 층으로 이루어진 시스템이다. 이들중 중간에 은닉층이 2개이상 층으로 이루어진 복잡하고 거대한 인공신경망을 Deep Neural Network라고 부르며 이를 활용한 머신러닝을 Deep Learning이라고 부른다.여기까지 정리해보면 Deel Learning은 Machine Learning의 한 분야로 2개 이상의 은닉계층을 가지는 인공신경망을 활용한 머신러닝기법 중 하나 이다.컴퓨터 기능 향상과 함께 거대한 인공신경망을 활용한 무거운 계산들이 가능해지면서 이미지인식이나 자연어 처리 등의 분야에서 Deel Learning이 많이 활용되었고 실제로 잘 작동하는 것이 최근의 인공지능에 대한 관심을 설명하는 여러 요인 중에 하나일 것 같다.Summary나름 여기저기서 긁어모은 자료들을 정리하고 엮어보았는데 잘못된 정보가 있을 수도 있고 이쪽 분야에서 충분히 합의되지 않은 내용이 들어가 있을 수도 있다.따라서 관련 책이나 강의를 보는 것을 추천드린다. 직접 코드짜보고 머신러닝을 해보면 훨씬 더 체감이 되고 이해하기 쉽다. 여기서는 그냥 개괄적인 내용만 다루고 자세한 내용은 다른 포스팅에 하나씩 풀어갈 예정이다.특히 Coursera에 있는 Andrew NG교수 강의를 추천드린다. 관련 포스팅의 주된 부분도 이 수업에서 배운 내용을 정리할 예정이다." }, { "title": "[Python] 설치, 개발환경 세팅", "url": "/posts/Python-Start/", "categories": "Python, Initial Setting", "tags": "Python, Initial Setting, VSCode", "date": "2022-08-03 19:55:00 +0800", "snippet": "Python을 시작해 보겠다 마음먹어도 막상 설치하고 구동하는 것부터가 진입장벽이다. 업무에도 활용해 보고 싶은데 심지어 회사 PC에 설치는 신경 써야 할 것도, 안 되는것도 많다. 이번 포스팅에서는 회사 PC에 어떻게든 Python 설치를 하고 사용해보면서 겪었던 일들과 주의점 정리해본다.참고로 이 포스팅에 모든 오류들을 하나하나 다 작성할 수 없다. 여기 없는 오류들은 가능한 Google검색을 해보자.1. IntroGoogle에 Python 설치라고 검색하면 많이 나온다. 몇개 글을 읽어보면 대강 감이 올 것이다. Python을 설치하고 Code작성을 위한 Editor를 설치하고 적절히 환경설정 해주는 작업들이다.하지만 회사에서 설치해서 사용하려면 어떻게 해야할까. IT기업이라면 애초에 IT전문인력일테니 회사의 전반적인 인프라에 대해서도 잘 알겠지만 이런 경우가 아닌 일반 회사에서 업무용이나 스터디용으로 설치해보려고 하면 여러가지를 고민해야 한다.여러가지 시행착오를 거친 결과 License나 회사 정책상 문제가 없었던 방식을 정리한다. 흔하게 겪는 오류와 이에 대한 해결 방법도 같이 메뉴얼처럼 작성해본다. 결론은 Python + VSCode 조합이 2022년 8월 16일 현재 기준으로 회사에서도 사용하는데 전혀 문제가 없는 License이다.잡다한 배경설명이 귀찮을 경우 밑의 License부분은 건너뛰고 바로 3. Install 부분부터 읽으면 된다.2. License어떤 것이든 회사 PC에서 ‘사용’하려 한다면 무조건 상업적 사용으로도 문제가 없는 License인지 확인해야 한다. 심지어 설치할 당시에 무료였여도 설치 이후 주기적으로 확인이 필요할 수 있다. 이런부분을 확인하지 않고 설치를 시도한 경우 설치가 안되면 일단 다행이다. 설치가 되었더라도 회사 전산관련 팀에서 연락이 오면 그나마 다행이다. 최악에는 License 소유회사로부터 어마어마한 비용이 회사로 청구될 수 있다.보통 내가 PC에 설치하고자 하는 어떠한 것들에 대한 공식 페이지가 존재하고 거기에 License관련 조항들이 있는 경우가 대다수다. 여기서 내가 설치하려고 하는 어떠한 것이 어떤 License를 선언했는지 확인하고 상업적인 용도로도 무료인지 확인하는 절차가 필요하다.2.1. Python결론부터 말하면 Python은 회사에서 ‘사용’하는데 전혀 문제가 없는 License이다.추가로 ‘사용’이라고 강조했다. Python으로 작성한 코드나 프로그램을 ‘배포’나 ‘판매’하려 하는 건 또 다른 얘기다. 애초에 그게 가능한 분이시라면 이 글을 읽지도 않을 것이다.2.2. IDE프로그래밍 언어인 Python은 무료다. 다만 Python만 설치한다고 해서 ‘편하게’ Code짤수는 없다. IDE 혹은 개발환경이라고 부르는 일종의 Code Editor가 필요하며 이부분이 여러 선택지와 함깨 여러 License들이 존재한다.적당히 Google에 python 개발환경 추천 이라고 치면 여러 글들이 있다. 이 문서 추천드린다.먼저 아나콘다(Anaconda)라는게 있다. 이게 원래는 무료였는데 어느새인가 유료로 변환되었다. 개인에게는 무료이니 개인 PC에서 처음 시작해볼때는 Anaconda도 추천드린다.또 다른건 Pycharm이다. 이건 애초에 유료다. 개인사용은 무료인 버젼도 있다.VSCode라는 것도 보일 것이다. 공식 홈페이지내 License 문서에서 아래와 같이 말한다. Source Code for Visual Studio Code is available at https://github.com/Microsoft/vscode under the MIT license agreement at https://github.com/microsoft/vscode/blob/main/LICENSE.txt. Additional license information can be found in our FAQ at https://code.visualstudio.com/docs/supporting/faq.MIT License를 Google에 검색해보면 아마 상업적 용도로도 무료라고 설명되어 있을 것이다.VSCode는 마이크로소프트에서 무료로 배포하는 Code Editor고 python외에도 여러가지 프로그래밍 언어를 편집하고 구동할 수 있다. 그리고 여러 Extension들이 있어서 라이센스 확인후 설치하면 꽤 편하게 코드를 작성할 수 있다. 사용자도 굉장히 많은 편이니 만약에 중간에 라이센스가 변경이 되더라도 생각보다 쉽게 알 수 있을 것이다.2.3. 회사 정책‘회사 정책상 Python 쓰면 안됩니다’ 이런곳은 아마 거의 없을 것이다. 그래도 한 번 회사 전산관련팀에 문의해보자. 물어보면 담당자를 알려주든 아니면 대부분 그냥 쓰세요 할 것이다.하지만 중간중간 라이센스 확인은 사용자의 몫인가보다. 인터넷 서핑하다가 Anaconda가 유료로 바뀐걸 알았을 때 황급히 지웠던 경험이 있다. 회사에서는 알려주지 않는다. 당연한 얘기지만 좀 그렇다. 이런 점에서는 사내에서 사용하는 사람들이 많았으면 좋겠다.3. InstallIntro에서 밝힌대로 프로그래밍 언어인 Python과 코드 에디터인 VSCode를 설치할 것이다. 추가로 몇가지 설정과 설치가 필요하며 아래 설명을 그대로 따르면 된다.3.1. Python점프 투 파이썬 01-4 파이썬 설치하기 부분을 보면 자세하게 나와있다. 이대로 진행해도 무방하다. 어차피 Python은 회사에서도 무료다.공식페이지에서 Python 설치를 위한 installer 다운받고 설치한다.드물지만 설치된 Python Version에 따라 같은 Code라도 작동하기도 하고 안하기도 하기 때문에 특정 Version이 필요한 경우도 있다. 나는 현재 3.9.6 Version을 사용중이며 이를 설치하려는 경우 여기로 가면 된다. Version관리를 위해 가상환경을 설정하는 방법도 있다. 하지만 이에 대해 크게 고민하지 말고 이런게 있다 정도만 알고 넘어가자.3.2. VSCode공식페이지에 들어가면 바로 Download for Windows뜬다. Version상관없이 그냥 다운받고 설치한다.설치 중간에 옵션 선택하는 부분에서 아래와 같이 설정해주면 편하다. 까먹고 안해도 상관없다. 나머지는 모두 Yes로 통과한다.Google에 vscode 설치라고 검색해도 많이 나온다. VSCode도 마찬가지로 아직은 상업적 이용으로도 무료라서 큰 걱정 없이 설치하자. (현재는 2022년 8월 16일이다.)3.3. Extensions아직 끝이 아니다. 추가 기능인 extension을 설치해야 한다.VSCode 설치가 완료되어 실행시켜보면 아마 아래 그림처럼 나올 것이다. Extension은 빨간 박스를 클릭하면 검색할 수 있다.python을 실행하려면 추가 기능을 설치해야 한다. 아래처럼 빨간 박스에 python검색하면 큰 화면에 설치 가능한 화면이 뜰 것이다. 사진은 이미 설치된 PC에서 캡쳐한 사진이다.아래 3개의 extension 반드시 설치해야 한다. 검색해서 설치하자. python: 설치하면 pylance등의 추가 extension들이 자동으로 설치된다. Jupyter: 처음에 접할 때 굉장히 편한 Tool이다. 일단 설치하자. Prettier: style을 이쁘게 해준다. 보기 좋은게 최고다.4. Hello World!이제 python code를 작성하고 실행시켜 볼 것이다. 프로그래밍언어 처음배울때 “Hello World!”라는 문자열을 출력해 보는건 국룰이다. Code Editor로 VSCode를 활용해서 Jupyter로 작성/구동해 볼 것이다. 아래 영상대로 진행해보자. (영상은 gif로 반복이 안걸려있다. 처음부터 보려면 새로고침(F5) 눌러본다.)글로 설명하면 아래순서로 진행된다.VSCode는 폴더를 작업환경으로 열 수 있다. 바탕화면에 python이라는 폴더를 하나 만들어보자. 이후에 VSCode 상단 메뉴에서 File &gt; Open Folder로 방금 만든 python폴더를 선택하고 ok를 누른다.자동으로 Explorer 탭이 열리면서 python 폴더가 보인다. 폴더명 옆에 파일추가 모양 아이콘을 클릭하면 파일 이름을 입력할 수 있다. Jupyter는 확장자가 ipynb인 파일을 읽고 편집한다. 첫 파일은 test.ipynb로 적고 Enter누르면 메인 화면에 셀이 하나 뜬다. 드디어 이 셀에 Python 코드를 작성하고 실행하면 된다.메인 화면에 생긴 셀에 아래 코드처럼 작성해본다.print(\"Hello World!\")실행은 셀단위로 수행된다. 오른쪽 상단에 마우스를 대면 뜨는 세모모양 아이콘을 클릭하면 셀에 작성된 Code가 실행된다. (위 영상을 참고한다.)하지만 아마 처음에는 오류가 뜰 것이다. 높은확률로 맨 처음 Jupyter 실행할 때 ipykernel이란 python라이브러리를 설치해야 하기 때문에 뜨는 오류다. 라이브러리는 VSCode Extension과는 또 다르다.Terminal을 열고(상단 메뉴바에서 Terminal &gt; New Terminal을 선택하면 된다.) 아래와 같이 써보자. 혹은 검은색 CMD 커맨드창에서 실행해도 된다.pip install ipykernel아래 영상처럼 써보고 Enter 치면 된다. (영상은 gif로 반복이 안걸려있다. 처음부터 보려면 새로고침(F5) 눌러본다.)아마 회사인터넷을 사용하는 대부분의 경우 또다시 오류가 뜰 것이다. 이 문서를 참고하자. 읽기 귀찮으면 오류 해결을 위해 아래와 같이 써본다.pip install --trusted-host pypi.python.org --trusted-host files.pythonhosted.org --trusted-host pypi.org ipykernelpip install과 설치해야할 라이브러리인 ipykernel사이에 뭔가 막 작성되 있다. 앞으로 회사 PC에서 Python 라이브러리를 설치할 때 계속 쓰게될 명령어다.자 설치가 완료되었다면 다시 셀을 실행해보자. 정상작동 할 것이다.다시 오류메시지가 뜬다면 미안하지만 Google에 그대로 복사붙여넣기 하고 답을 찾아나서야 한다.Summary정리해보자. Python 설치 VSCode 설치 VSCode Extension 설치 (Python, Jupyter, Prettier) Terminal에서 ipykernel 라이브러리 설치 하고 Jupyter 실행 셀에 Python 코드 작성 및 실행현재 기준으로 License상 문제되지는 않는 조합이라 회사에서 설치하고 사용하는데 일단은 문제가 없을 것이다.설치할 때 나타나는 오류들은 Google에 그대로 검색해보고 해결책을 나름 찾아보자. 설치하느라 고생했는데 앞으로 계속 Python을 쓰려고 한다면 이런 습관은 필수다. 웹에 거의 대부분의 답이 있다.초보자가 작성한 포스팅이기 때문에 위에 작성한 용어들 중에 정확하게 사용되지 않은 경우들이 있다. 예를 들어 Code Editor와 개발환경, IDE는 분명히 다른 것이다. 아나콘다는 IDE이지만 Jupyter나 VSCode, Spyder같은 것들은 Code Editor다. 하지만 지금수준에서는 일단은 그냥 그게 그거다. 지금은 기능구현과 이에 필요한 쉬운 설명에만 집중해본다." }, { "title": "[Python] 파이썬 하자고 꼬시는 글", "url": "/posts/python-proposal/", "categories": "Orientation, Python 하자고 꼬시는 글", "tags": "Python", "date": "2022-08-02 11:33:00 +0800", "snippet": "Python으로 하는 프로그래밍은 처음에는 정말 막막하고 하나도 모르겠고 어려웠다. 시간이 지나면서 조금씩 경험이 쌓이고 이제는 좀 생각대로 돌아가는걸 확인하니 취미로 이만한것도 없다. 물론 지금도 쉽지는 않다.화공업계에서 일하면서 공학용 계산에도 써보고 머신러닝도 조금 해보고 나름 자동화도 해보며 주변에 조금씩 티를 내본다. 친한분들에게 해보라고 꼬셔보는데 그렇게 잘 먹히진 않는다. 하지만 나같아도 안했을거 같기 때문에 할말은 없다.그래서 이번에는 내 얘기 말고 인터넷으로 믿을만한 자료를 검색해본다. Chemical Engineering 이쪽 업계에서 Python은 쓸만한가? 아니 그냥 어디서든 쓸만한가? 추천해도 되나? 언젠간 인기 식어서 아무도 안쓰고 사라지면 어쩌나? 나도 좀 궁금 했었다.1. Is Python Useful?일단 갓구글에 is python useful?이라고 검색해본다.2021년 작성된 ‘What is Python used for? 10 practical Python uses’라는 포스팅이다. According to the TIOBE index, which measures the popularity of programming languages, Python is the third most popular programming language in the world, behind only Java and C. There are many reasons for the ubiquity of Python, including: Its ease of use. For those who are new to coding and programming, Python can be an excellent first step. It’s relatively easy to learn, making it a great way to start building your programming knowledge.Its simple syntax. Python is relatively easy to read and understand, as its syntax is more like English. Its straightforward layout means that you can work out what each line of code is doing. Its thriving community. As it’s an open-source language, anyone can use Python to code. What’s more, there is a community that supports and develops the ecosystem, adding their own contributions and libraries. Its versatility. As we’ll explore in more detail, there are many uses for Python. Whether you’re interested in data visualisation, artificial intelligence or web development, you can find a use for the language. 쉽고, 문법도 간단하고, 커뮤니티가 많고 쓸데도 많다고 한다. So, we know why Python is so popular at the moment, but why should you learn how to use it? Aside from the ease of use and versatility mentioned above, there are several good reasons to learn Python: Python developers are in demand. Across a wide range of fields, there is a demand for those with Python skills. If you’re looking to start or change your career, it could be a vital skill to help you. It could lead to a well-paid career. Data suggests that the median annual salary for those with Python skills is around £65,000 in the UK. There will be many job opportunities. Given that Python can be used in many emerging technologies, such as AI, machine learning, and data analytics, it’s likely that it’s a future-proof skill. Learning Python now could benefit you across your career. Python 기술이 있는 사람들에 대한 수요가 많다고 한다. 그리고 돈도 많이 준다고 한다!! Python 좀만 더 열심히 하면 월급 좀 오르려나.아무래도 인터넷에서 프로그래밍 언어를 검색한다는 건 확률적으로 개발자들이 많은 곳을 갈 수밖에 없다. 이들의 기준에서 얘기하는 경우가 꽤 많다고 느껴졌다. 그리고 좋은얘기만 있는 건 아니다. 느리다, 인기는 거품이고 곧 다른언어로 대체될거다 여러 부정적 의견들도 조금씩은 존재한다.2. Is Python Useful for Chemical Engineering?아무래도 화공업계에 있다보니 궁금해진다. 이 바닥에서도 좀 쓰려나. 아무도 안쓰고있나? 괜히 불안하다. 구글에 검색해보면 ‘Step into the Digital Age with Python’ 이라는 글이 나온다. 2021년 미국화학공학회 AIChE어딘가에 실린 글이다.2.1. Python is already widely used by chemical engineers첫 section 제목이었다. 이거 이업계에서 꽤 사용되고 있나보다. 안도했다. It is widely used by major technology companies across many scientific disciplines, and it benefits from the contributions of researchers. Python is considered to be an easy-to-learn programming language, and it is among the first programming languages that many students are taught today. Because more and more students are learning Python, the next generation of engineers will likely be interested in applying it to their professional work. For those out of school, Python is a great programming language to learn independently, with abundant online resources. This time spent learning the language is a smart investment for your career. For example, in the Presidential Lecture given at the 2019 AIChE Annual Meeting (Nov. 10–15, 2019, Orlando, FL), Matt Sigelman, CEO of Burning Glass Technologies, mentioned Python specifically as a skill that adds a premium to a worker’s expected salary.언제나 빠지지 않는 배우기 쉽다는 말 여기도 있다. 어딘가의 CEO가 Python 아는 직원의 기대연봉이 높을거란 얘기도 했단다. 아직 나는 배움이 부족한듯 싶다.2.2. Python adapts to our work화공업계쪽에도 Python은 적응하고 있다고 한다. Python’s greatest power is in its flexibility, and without packages, it would not have its breadth of applications. Table 1 highlights some of the most popular enabling packages engineers use to collect and analyze data, perform calculations, and automate tasks.Python 자체는 open-source이고 사용자도 많기 때문에 수많은 라이브러리 (위에서는 package로 표현)들이 있고 쉽게 사용할 수 있다. 데이터 분석이나 공학용 계산을 하거나 자동화를 수행하는데 쓰인다고 한다. 잘 쓰고 있었나보다.2.3. Python has technical computing capabilities When using Python to solve technical problems, you may be looking for capabilities from classic numerical methods, or you may be interested in applying a promising new approach (e.g., machine learning) to problem-solving. For either scenario, Python is quite capable.공학용 계산에서 전통적으로 사용되던 numerical methods에서 채신기술인 machine learning까지 모두 커버할 수 있는 라이브러리들을 갖고있다. 자세한건 직접 해보면 알 수 있다.3. Next Steps The aim of this article is to raise awareness of what the Python programming language can do for chemical engineers. Python is capable enough for professional programmers yet simple enough to be taught as an entry-level language. If you are looking to upgrade your skills, consider adding Python to your toolbox. It is free to use, free to learn, and can be used for nearly any digital task.이 문단으로 하고 싶은 말 끝났다. 화공업계에 있다면 Python 한 번 써보셨으면 좋겠다. 아니어도 한 번 써보면 좋겠다. 입문용 프로그래밍 언어로도 좋다고 한다. 일단 공짜고 배우는 것도 공짜고 생각보다 많은 일을 할 수 있다. 시간만 좀 들이면 된다.Summary긴말 필요없이 한 번 해보자. Python 좋다고 해보시라고 추천할 수 있다. 내 주변에서도 많이들 써서 같이 헤매봤으면 좋겠다. 일단 회사 PC에 Python 설치하고 개발환경도 세팅해보자." }, { "title": "Orientation", "url": "/posts/Orientation/", "categories": "Orientation", "tags": "Orientation", "date": "2022-08-01 11:33:00 +0800", "snippet": "이 Blog에서 다룰 프로그래밍 언어에 대한 Orientation이다. 뭐 하려고 하는지 어떤 내용 있는지 앞으로 어떤 내용 작성하려는지 정리해본다.1. Intro여기서 다룰 내용과 프로그래밍 언어는 아래와 같다.Language = ['Python', 'VBA', 'HTML/CSS/JavaScript']Area_Python = ['Engeineering Calculation', 'Machine Learning']Area_VBA = ['Some Automation']Area_WEB = ['WEB Page Development']Grade = 'Elementary'먼저, 작성자 자신도 초보다. 하지만 완전 처음에 시작할 때는 실은 어디서 시작해야하는지 부터 막막하다. 작성자가 헤맨부분은 똑같이 헤매지 않기를 바라며 같이 공부해 나가는 걸 목표로 한다.가장 좋은 건 Small Success를 느낄 수 있는 자그마한 문제를 정하고 실제로 코딩을 통해 해결하는 것 같다. 그래야 재미도 있다. 조금씩 성공을 쌓아가면서 꾸준히 취미로 하면 좋을 것 같다..주된 내용들은 아무래도 Python으로 하는 공학용 계산이나 Machine Learning이 될 것 같다. 나머지는 경험해 본 것 중에 쓸만 했던 것 위주로 간단히 정리하려고 한다.2. Plan그래도 한 달에 한 번은 포스팅 써보려고 한다. 취미로 하는게 목표니 강제성은 없다.3. Contents강조하지만 작성자 자신도 초보기 때문에 정확하지 않은 정보가 섞여있을 수 있다. 그저 몇 안되는 경험상 이러이러 하다는 정도로 각 언어별 느낌을 정리해본다.Stackoverflow라는 개발자 커뮤니티가 있다. 개발자만 있는건 아니고 그냥 코드 짜다가 모르는게 있으면 아무나 그냥 질문 올리고 그러면 고수분들이 질문 답해주는 그런 곳이다. 아쉽게도 영어지만 어차피 코드자체가 영어다. 아래 그래프는 그 커뮤니티에서 프로그래밍 언어별로 질문이 올라오는 횟수를 보여준다.3.1. Python 장점1. HOT 하다 위 그래프에서 볼 수 있듯이 최근 몇 년 사이 가장 HOT한 프로그래밍언어다. 일단 그냥 뭔가 있어보인다. 머신러닝 덕분이 큰 것 같다. 할 수 있는 것도 많다. 장점2. 사용자가 많다. 사용자 수가 많다는 얘기는 그만큼 관련된 정보를 얻기도 쉽다는 얘기다. 커뮤니티가 활발하니 다양한 기능들이 알아서 이미 구현/공유 되어있다. 내가 원하는 기능 1부터 100까지 다 코드 직접 작성해서 쓰는 사람 없다. 비슷한 기능 구현된 코드 잘 가져다 쓰는게 이 바닥 능력인것 같다. 베끼기 쉬우려면, 그 언어 쓰는 사람이 많아야 한다. 장점3. 배우기 쉽다. (다른 언어에 비해) 그리고 프로그래밍이 처음이라면 입문언어로 다들 추천할 만큼 쉽다고 한다. 여기서 쉽다는 얘기는 다른 언어에 비해 쉽다는 얘기지 Python이 쉽다는 얘기는 아니라고 생각한다. 장점4. 무료다. 회사에서 ‘사용’하기에도 문제없는 무료 License다. 회사에서 뭔가 해보려는데 비용이 드는 소프트웨어 쓰는건 쉽지않다. Open-Source라는 것은 충분히 매력적이다. 덕분에 사용자도 많고 이건 장점2와 동일하다. 단점1. 그렇게 쉽지만은 않다. 초보자에게는 초기 개발환경 세팅 조금 어렵다. 배우기 쉽다고 하는데 실은 그렇게 쉽지만도 않다. 이거저거 보고 배워야할게 많긴한데 그래도 구글링하면 다 나온다. 어차피 초보자에게는 모든게 다 어렵다. 일단 배우려고 마음먹었을 때에는 크게 단점으로 와닿지는 않을 것 같다. 활용 공학용 계산, Machine Learning, 데이터 시각화, 자동화, Backend개발 등3.2. Visual Basic for Application (VBA, aka Excel 매크로) 장점1. 쉬운 개발환경 Setting 개인 PC에 Excel 다들 설치 되 있을 것이다. (회사원이라면 특히) 그러면 따로 설치할 건 없다. 그냥 설정만 좀 해주면 바로 개발 가능하다. Window 유저 한정 편하다. 장점2. 은근한 활용성 생각보다 활용성 있다. 다른 프로그램 Control도 가능하다. 적당히 공부하면 회사에서 업무 자동화 관련 쓰기에 나쁘지 않다. 단점1. HOT하지가 않다. HOT하지가 않다. 인터페이스도 그닥 좋지 않다. 자연스레 사용자가 그렇게 많지는 않은 것 같다. 활용가능한 범위도 적다. 단점2. 사용자가 많지는 않다. 사용자가 상대적으로 적어서 그런지 인터넷에서 원하는 자료를 찾기 힘들다. 조금 공부해서는 복잡한 기능 구현은 쉽지 않아보인다. 보안관련 이슈도 있다고 하는데 이건 잘 모르겠다. 활용 Excel 매크로, 프로그램 자동화3.3. HTML/CSS/Javascript 장점1. 웹 개발의 모든것 Front-end 웹개발 하려면 이거 알면 된다고 한다. 그냥 이거 좀 보는 순간 모든 웹사이트가 달리보인다. 뭐든지 새롭게 알아간다는건 나쁘지 않다. 요즘 웹개발자 수요 폭발이라던데 전직 가능할지도 모른다. 내 블로그 만들기도 가능은 하다. 가능은. 장점2. 사용자가 많다. 사용자가 많으면 일단 좋다. 편하게 구글링해서 코드 긁어다 쓸 수 있다. 단점1. 웹개발 관심 없으면 쓸모없다. 실은 웹개발에 관심있는 사람이 머 얼마나 있을까 모르겠다. 관심없으면 쓸모없다. Javascript는 좀 다를수 있을것 같다. 단점2. 웹개발 관심 있어도 더 많은 공부 필요하다. HTML/CSS/Javascript 각각 쉬운 언어긴 한데 3개 알았다고 해서 바로 할 수 있는건 많지 않다. 결과물의 퀄리티도 별로다. 그럴듯한 결과물을 만드려면 별의별 희안한 개발도구들 새로 익혀야한다. 이 블로그 자체도 Jekyll과 Git을 추가로 공부해야만 했다. 물론 HTML/CSS/Javascript를 모두 외우듯이 알아야 하는 건 아니다. 재미붙이면 나쁘진 않은데 이 바닥 트렌드가 하도 자주 바뀌어서 쉽지만은 않은 것 같다. 활용 웹개발 (Front-end)4. Recommendation그냥 Python 추천한다. 요즘은 회사에서도 Python정도는 강의를 제공해주는 경우가 많은 것 같다. 그것도 좋고 아니면 유튜브에도 많다. 이 블로그에는 Python 활용한 머신러닝, 공학용 계산 글이 많을 것 같다.언어가 무료라서 그런가 입문 책마저도 무료가 많다. 기초는 점프투파이썬 추천한다. 한글이다.Summary프로그래밍 이쪽은 전공도 아니고 공부한지 오래되지도 않았지만 내 생활이나 업무에 적용해 볼만한 기능들도 꽤 있었다고 생각한다. Python이라는걸 접하고 제일 처음 했던 것은 대학원때 썼던 방정식 풀이로 공학용 계산이었지만 두번째는 게임 매크로만들어서 돌리는 것이였다. 그냥 생각나는거 아무거나 해볼 수 있다.프로그래밍 언어 하나 그저 ‘다룰수 있다’는거 하나만으로도 충분히 큰 Merit가 되는 것 같다. (잘 할 필요도 없다.)일단은 조금 다른 시각으로 세상을 볼 수 있다. 이게 뭐 대단한 것도 아닌게 인터넷 검색만 해 보면 된다. 당분간은 이 이상 취미로 좋은건 또 없을 것 같다.시선을 조금만 돌려보면 세상에는 신기한거 배울거 참 많고 세상도 빠르게 변한다." } ]
