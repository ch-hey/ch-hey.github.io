<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="[머신러닝] 인공신경망 Artificial Neural Network" /><meta property="og:locale" content="en" /><meta name="description" content="인공신경망에 대해 알아본다. 먼저 수학적인 표현에 대해 살펴본 이후 아주 간단한 구조의 인공신경망에 Gradient Descent를 적용해본다. 정리된 알고리즘대로 Python으로 Machine Learning도 진행한다." /><meta property="og:description" content="인공신경망에 대해 알아본다. 먼저 수학적인 표현에 대해 살펴본 이후 아주 간단한 구조의 인공신경망에 Gradient Descent를 적용해본다. 정리된 알고리즘대로 Python으로 Machine Learning도 진행한다." /><link rel="canonical" href="https://ch-hey.github.io/posts/Neural-Network/" /><meta property="og:url" content="https://ch-hey.github.io/posts/Neural-Network/" /><meta property="og:site_name" content="ChBlog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-08-19T19:55:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="[머신러닝] 인공신경망 Artificial Neural Network" /><meta name="twitter:site" content="@" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-08-26T22:25:07+08:00","datePublished":"2022-08-19T19:55:00+08:00","description":"인공신경망에 대해 알아본다. 먼저 수학적인 표현에 대해 살펴본 이후 아주 간단한 구조의 인공신경망에 Gradient Descent를 적용해본다. 정리된 알고리즘대로 Python으로 Machine Learning도 진행한다.","headline":"[머신러닝] 인공신경망 Artificial Neural Network","mainEntityOfPage":{"@type":"WebPage","@id":"https://ch-hey.github.io/posts/Neural-Network/"},"url":"https://ch-hey.github.io/posts/Neural-Network/"}</script><title>[머신러닝] 인공신경망 Artificial Neural Network | ChBlog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="ChBlog"><meta name="application-name" content="ChBlog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="https://raw.github.com/ch-hey/imgcdn/master/rocket.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">ChBlog</a></div><div class="site-subtitle font-italic">Markdown to WEB</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/ch-hey" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['haner1264','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>[머신러닝] 인공신경망 Artificial Neural Network</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>[머신러닝] 인공신경망 Artificial Neural Network</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1660910100" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Aug 19, 2022 </em> </span> <span> Updated <em class="" data-ts="1661523907" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Aug 26, 2022 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/ch-hey">HEY</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="4056 words"> <em>22 min</em> read</span></div></div></div><div class="post-content"><p>인공신경망에 대해 알아본다. 먼저 수학적인 표현에 대해 살펴본 이후 아주 간단한 구조의 인공신경망에 Gradient Descent를 적용해본다. 정리된 알고리즘대로 Python으로 Machine Learning도 진행한다.</p><h2 id="1-intro"><span class="mr-2">1. Intro</span><a href="#1-intro" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Michael Nielsen의 <a href="http://neuralnetworksanddeeplearning.com/">“Neural Networks and Deep Learning”</a>은 인공신경망에 대해 공부하기 매우 좋은 자료다. Andrew NG 교수의 Coursera수업과 함께 참고하여 이 포스팅 작성한다.</p><p>인공신경망은 이름에서 알 수 있듯이 인간의 신경계, 뉴런의 행동양식을 모방해서 만들어졌다는 이야기 들어봤을 것 같다. 반면 실제로 인간뉴런의 행동은 그리 간단하지도 않으며 이런 시각이 인공신경망 기법에 대한 정확한 이해를 방해한다는 의견도 있다.</p><p>내 생각에도 인공신경망은 수학모델이고 Regression에 활용될 뿐이다. 다만 모든 모양의 함수를 표현할 수 있는 매우 큰 그릇같은 느낌이다. 실제로 수학적으로 그렇다. 증명은 여기서 안다루고 관련 자료는 <a href="http://neuralnetworksanddeeplearning.com/chap4.html">여기</a> 참고하면 된다.</p><p>보통 <a href="https://www.ibm.com/kr-ko/cloud/learn/neural-networks">아래 그림</a>처럼 인공신경망을 표현한다.</p><p><img data-src="https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_01_03-DeepNeuralNetwork-WHITEBG.png" alt="Deel Learning" width="500" data-proofer-ignore></p><p>가장 왼쪽 Data가 들어가는 Input Layer에서 시작해서 가운데에는 뭔지 모르지만 여러겹의 Hidden Layer를 거쳐 가장 오른쪽의 Output Layer를 통해 결과가 나온다. 원으로 표현된 각 Unit들은 화살표로 빼곡하게 그물망처럼 연결되어 있다.</p><p>직관적인 느낌은 줄 수 있지만 아직 어떻게 작동하는지 정확히 모르겠다.</p><h2 id="2-mathematical-expression"><span class="mr-2">2. Mathematical Expression</span><a href="#2-mathematical-expression" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>아래 그림처럼 가운데 Hidden Layer를 모두 지우고 각 Layer에 Unit들은 한개씩 있는 인공신경망을 생각해본다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 300'%3E%3C/svg%3E" data-src="https://raw.github.com/ch-hey/imgcdn/master/img/2022-08-10/ANN_1.png" alt="ANN_config1" width="500" height="300" data-proofer-ignore></p><p>위 그림 제일 왼쪽 Input Unit안에 x하나가 있다. x는 Input이고 변수(variable)이며 특성(feature)이라고도 부른다. 화살표를 따라 오른쪽으로 가면 Output이 나오며 모델을 통한 결과값으로 hypothesis를 의미하는 h로 표시되 있다.</p><p>선을 따라 오른쪽으로 가다보면 $\theta_1$, $\theta_0$를 만나며 input인 x와의 선형 조합 과정을 거친다. Slope에 해당하는 $\theta_1$은 weight, intercept에 해당하는 $\theta_0$ bias라고 부른다. 편의상 이 중간 과정을 $a$로 표현하면 아래와 같다.</p>\[a = \theta_1 x + \theta_0 \qquad (1)\]<p>선형 조합 이후 순차적으로 활성화(activation)이라는 과정을 거쳐 최종 Output을 얻는다. Activation 함수는 여러가지 선택지가 있으며 이 포스팅에서는 <a href="https://ko.wikipedia.org/wiki/%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C_%ED%95%A8%EC%88%98">sigmoid 함수</a>를 사용하겠다.</p>\[g(x) = {1 \over {1 + {e}^{-x}}}\qquad (2)\]<p>Sigmoid 함수는 아래그림처럼 생겼다. x가 일정값 이하일때 함수값이 0에 가깝다가 일정값 이상이 되면 급격히 1에 가까워지는 성질로 활성화를 표현한다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 300 300'%3E%3C/svg%3E" data-src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png" alt="img" width="300" height="300" data-proofer-ignore></p><p>결국 최종 h는 아래 식과 같이 표현된다. (여기까지는 <a href="https://en.wikipedia.org/wiki/Logistic_regression">Logistic Regression</a>과 완전히 동일한 형태이다.)</p>\[h_\theta (x) = g(a) = {1 \over {1 + {e}^{-(\theta_1 x + \theta_0)}}} \qquad (3)\]<p>정리해보면</p><ol><li>각 Unit들을 이어주는 선은 $\theta$로 표현되는 weight, bias를 의미하고<li>왼쪽에서 오른쪽으로 선을 따라가면서 순차적으로 선형조합(식(1))과 활성화(식(2))라는 과정을 거친다.<li>최종 계산값은 선형조합과 활성화를 거친 식(3)으로 주어진다.</ol><p>인공신경망을 이용한 머신러닝이라고 한다면 $\theta$로 표현되는 parameter들을(weight, bias) 데이터에 잘 맞게 regression하는 과정일 것이다.</p><p>좀 더 복잡한 구조로 Input, Output Layer에 Unit이 각각 2개인 경우를 생각할 수 있다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 400 300'%3E%3C/svg%3E" data-src="https://raw.github.com/ch-hey/imgcdn/master/img/2022-08-10/ANN_2.PNG" alt="ANN_config1" width="400" height="300" data-proofer-ignore></p><p>변수는 $x_1$, $x_2$ 2종류, output도 $h_1$, $h_2$로 2개이며 아래 수식과 같이 정의된다.</p>\[h_{1, \theta}(x_1, x_2) = {1 \over {1 + {e}^{-(\theta_{01}+\theta_{11}x_1 + \theta_{21}x_2)}}} \qquad (4)\] \[h_{2, \theta}(x_1, x_2) = {1 \over {1 + {e}^{-(\theta_{02}+\theta_{12}x_1 + \theta_{22}x_2)}}} \qquad (5)\]<p>각 문자들에 붙는 아래첨자에 숫자들이 늘어나서 좀 복잡해 보일 수 있다. $\theta_{ij}$는 i번째 변수에서 j번째 output에 관련된 weight를 나타내며 i가 0인경우 j번째 output에 관여하는 bias다. 간단히 생각하면 각각의 output unit에 모이는 변수, weight, bias를 선형조합하고 활성화 과정을 거치는 동일한 방법이다.</p><p>마지막으로 Hidden Layer를 한개만 추가한 경우를 표현해보자. 아래 그림에는 Input, Hidden, Output Layer에 Unit이 각각 2개씩 총 6개인 구조의 인공신경망이 있다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 400'%3E%3C/svg%3E" data-src="https://raw.github.com/ch-hey/imgcdn/master/img/2022-08-10/ANN_3.PNG" alt="img" width="500" height="400" data-proofer-ignore></p><p>편의상 hidden layer의 각 unit별 output을 $a$로 표현한다. $\theta_{ij}^{k}$는 왼쪽부터 k번째 layer에 있는 i번째 input에서 k+1번째 layer에 있는 j번째 output으로 향하는 weight를 나타내고 i가 0인 경우는 bias다. 말로 설명하니까 굉장히 이상한데, 달라진건 아무것도 없다. 동그란 unit으로모이는 화살표들에 관련된 변수와 parameter들을 선형조합하고 활성화하는 동일한 규칙이 적용된다.</p><p>중간단계에 있는 $a_1$, $a_2$는 아래와 같이 표현된다.</p>\[a_{1} = {1 \over {1 + {e}^{-(\theta_{01}^1+\theta_{11}^1x_1 + \theta_{21}^1x_2)}}} \qquad (6)\] \[a_{2} = {1 \over {1 + {e}^{-(\theta_{02}^1+\theta_{12}^1x_1 + \theta_{22}^1x_2)}}} \qquad (7)\]<p>잘 보면 식 (4), (5)와 동일하다. 동일한 규칙, 동일한 구조로 동일한 결과가 나온다. 최종 output인 $h_1$과 $h_2$는 아래와 같다.</p>\[h_{1} = {1 \over {1 + {e}^{-(\theta_{01}^2+\theta_{11}^2a_1 + \theta_{21}^2a_2)}}} \qquad (8)\] \[h_{2} = {1 \over {1 + {e}^{-(\theta_{02}^2+\theta_{12}^2a_1 + \theta_{22}^2a_2)}}} \qquad (9)\]<p>만일 hidden layer가 1개층이 아닌 여러개의 층으로 구성되어 있다면 $a_i$였던 것들을 $a_i^j$로 j번째 layer에 있는 i번째 unit 처럼 표현해줄 수 있겠다.</p><p>절대 식 하나하나 기억할 필요 없다. 한번정도 눈으로 꼼꼼히 따라가보면 좋겠지만 그마저도 크게 필요없다. 오직 기억 할만한 것은 인공신경망은 <strong>Input과 Parameters의 선형조합과 이후 활성화 과정으로 이루어진다</strong>는 점이다.</p><h3 id="21-hyper-parameter"><span class="mr-2">2.1. Hyper-parameter</span><a href="#21-hyper-parameter" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>weight와 bias처럼 선형조합에 사용되는 일종의 coefficient들을 parameter라고 불렀다. 추가로 hidden layer의 layer층 개수나 unit개수 등 구조에 관련된 숫자를 특별히 hyper-parameter라고 부른다. Data의 크기를 포함한 다양한 이유로 hidden-layer층 개수는 많을수도, 적을수도 있다. 관련한 자세한 내용이 궁금하면 <a href="https://ikkison.tistory.com/92">이 문서</a>가 좋아보인다.</p><p>Parameter는 Gradient-Descent같은 방식으로 구하지만 hyper-parameter는 그러지 못한다. 경험적으로 구하거나 trial-error를 해야하며 정형화된 방법은 없는 것 같다. 그냥 일단 해보고 결과보고 그에따라 대응하는 것 같다.</p><h3 id="22-vectorization"><span class="mr-2">2.2. Vectorization</span><a href="#22-vectorization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>식들을 잘 보면 덧셈이 많다. 위 식들은 Feature vector와 parameter vector들의 dot product, 내적으로 표현하면 여러번 반복해서 써야하는 수고로움을 덜 수 있다. 예를 들어 아래와 같은 단순합을 vector를 이용해서 간단하게 표현해보자.</p>\[S = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + ... + \theta_n x_n\] \[Let \quad \vec \theta = [\theta_0, \theta_1, ... , \theta_n], \quad \vec x = [1, x_1, x_2, ..., x_n]^T\] \[S = \vec \theta \cdot \vec x\]<p>bias에 대응될 변수에는 1을 넣어주고 dot product 하면 끝이다. 문자로 표현해서 인공신경망의 크기나 구조에 상관없이 일반화된 식을 사용할 수도 있다. 자주 얘기하는 Andrew NG교수 4주차 수업에서 vectorization을 활용해 인공신경망을 표현하는 것을 볼 수 있다.</p><p>굳이 이 얘기를 하는건 코드를 짤 때도 vectorized 형태로 연산을 정의하면 단순 반복문의 경우보다 계산속도가 훨씬 빠르기 때문이다. 단순 for문 보다 numpy array같은거 이용해서 계산하는게 훨씬 빠르긴 하다. 수치연산 관련 코드가 vector나 행렬 형태에서 좀 더 최적화되어 움직이도록 되어 있는 것 같다. 나도 잘 몰라서 더 얘기하기는 힘들다.</p><p>지금은 크게 신경쓰지 않아도 좋다. 어차피 이런거 신경쓸 정도의 무거운 계산을 하기까지 앞으로 멀기도 했고 그때 쓰는 라이브러리에서 이미 최적화된 방식으로 잘 해줄 것이다.</p><h2 id="3-gradient-descent"><span class="mr-2">3. Gradient Descent</span><a href="#3-gradient-descent" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>앞에서 얘기한 Input에서 Output까지 가는 일련의 계산 흐름을 FeedForward라고 부른다. 반대로 가는 과정을 Back-Propagation이라고 부르며 Cost를 최소화하는 parameter들을 구하는 과정을 말한다.</p><p>수식을 간단하게 하기 위해서 hidden layer없이 feed unit 2개, output unit 1개짜리 인공신경망을 만들고 Gradient Descent를 적용해 본다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 400 300'%3E%3C/svg%3E" data-src="https://raw.github.com/ch-hey/imgcdn/master/img/2022-08-10/ANN_4.PNG" alt="img" width="400" height="300" data-proofer-ignore></p><p>Output $h$는 다음식으로 표현된다.</p>\[h = {1 \over {1 + {e}^{-(\theta_0 + \theta_1 x_1 + \theta_2 x_2)}} } \qquad (10)\]<p>연습삼아 Vectorized Form으로 표현 해보자. x vector안에 $x_0$가 추가된 것에 주의해야한다. 변수를 추가한 것은 아니며 단순히 bias를 받아주기 위한 처리에 불과하다. ($x_0$ 값은 모두 1이다.)</p>\[\vec x = [x_0(=1), x_1, x_2]^T,\quad \vec \theta = [\theta_0, \theta_1,\theta_2]\] \[h_{\vec \theta}(\vec x) = {1 \over {1 + {e}^{-(\vec \theta \cdot \vec x)}} } = g(\vec \theta \cdot \vec x)\]<p>모델값과 Data간의 오차의 합을 Cost라고 하고 아래식으로 주어진다.</p>\[J(\vec \theta) = {1 \over{2m}} \sum_{i=1}^m (h_{\theta}(x^i)-y^i)^2 \quad \quad (11)\]<p>목표는 Cost를 최소화하는 parameter vector $\vec\theta$ 를 구하는 것이다. <a href="https://ch-hey.github.io/posts/Gradient-Descent/">Gradient Descent</a>에 따르면 이 때 Parameter Update 방식은 아래 식으로 주어진다. (12a, b는 동일한 표현이다.)</p>\[\theta_i := \theta_i - \alpha {\partial \over \partial {\theta_i}} J(\vec \theta) \quad (i=0,1,2) \qquad (12a)\] \[\Delta \vec \theta = -\alpha\nabla J(\vec \theta) \qquad (12b)\]<p>남은 일은 편미분하는 일이다.</p>\[{\partial \over \partial {\theta_i}}\left({1 \over{2m}} \sum_{i=1}^m (h_{\theta}(x^i)-y^i)^2\right) = {1 \over {m}}\sum_{i=1}^m (h_{\theta}(x^i)-y^i){\partial \over \partial {\theta_i}}h_{\theta}(x)\] \[= {1 \over {m}}\sum_{i=1}^m (h_{\theta}(x^i)-y^i){\partial \over \partial {\theta_i}}g(\vec \theta \cdot \vec x)\] \[= {1 \over {m}}\sum_{i=1}^m (h_{\theta}(x^i)-y^i)g'(\vec \theta \cdot \vec x) x_i\]<p>sigmoid 함수의 미분식은 아래와 같이 정리된다.</p>\[g(x) = {1 \over {1 + {e}^{-x}}} \quad g'(x) = {-{e}^{-x} \over {(1+{e}^{-x})^2}} = g(x)(1-g(x))\]<p>따라서 식 12a, b는 아래식처럼 정리된다.</p>\[\theta_i := \theta_i - \alpha {1 \over {m}}\sum_{i=1}^m (h_{\theta}(x^i)-y^i)g(\vec \theta \cdot \vec x)(1-g(\vec \theta \cdot \vec x)) x_i \qquad (13)\]<p>좀 더 복잡한 구조에 대한 일반화된 식과 이 구조에서의 Back-propagation에 대해 수식으로 유도하고 정리된 자료를 보고싶다면 Michael Nielsen의 “Neural Networks and Deep Learning”책의 이 <a href="http://neuralnetworksanddeeplearning.com/chap2.html">Chapter</a> 정독해보자. Notation은 좀 다르지만 원하는 것을 얻을 수 있다.</p><h2 id="4-application"><span class="mr-2">4. Application</span><a href="#4-application" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>필요한 식 정리가 끝났다. 데이터는 아래 정리된 표를 사용할 것이다. 식 유도에 사용된 인공신경망 구조에 맞게 Input의 종류는 2가지, Output은 1가지이다.</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">$x_1$<th style="text-align: center">$x_2$<th style="text-align: center">Output<tbody><tr><td style="text-align: center">0<td style="text-align: center">0<td style="text-align: center">0<tr><td style="text-align: center">1<td style="text-align: center">0<td style="text-align: center">1<tr><td style="text-align: center">0<td style="text-align: center">1<td style="text-align: center">1<tr><td style="text-align: center">1<td style="text-align: center">1<td style="text-align: center">1</table></div><p>표를 잘 보면 $x_1$, $x_2$ 둘 중 하나만 1이어도 결과값이 1이고 두개 모두 동시에 0일때만 결과값은 0인 관계임을 알 수 있다. 이런 관계를 logical OR function이라고 한다. (보통 0은 거짓, 1은 참을 의미하며 OR연산에서는 둘 중 하나만 참이어도 결과는 참이다.)</p><p>달리 표현하면 머신러닝을 활용해서 인공신경망으로 logical OR function을 만들어 볼 것이다. (실은 logistic regression이다.)</p><p>필요한 라이브러리를 Import 해주자. Random은 parameter들의 초기값을 정하는데 써준다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">random</span><span class="p">,</span> <span class="n">os</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</pre></table></code></div></div><p>위 표에 있는 데이터 입력한다. x0는 1에 해당하며 bias 부분을 표현해 주기 위해 쓰인다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="n">weights</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">())</span>

<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>

<span class="n">x0data</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="c1"># x0 = 1
</span><span class="n">x1data</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="c1"># x1
</span><span class="n">x2data</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="c1"># x2
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">x0data</span><span class="p">,</span> <span class="n">x1data</span><span class="p">,</span> <span class="n">x2data</span><span class="p">])</span>

<span class="n">ydata</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="c1"># Output
</span></pre></table></code></div></div><p>Learning rate는 학습을 몇 번 돌려보면서 정했다. 총 1000번 학습(parameter update)가 이루어지게 설정했다. <code class="language-plaintext highlighter-rouge">err_list</code>는 학습 횟수가 늘어남에 따라 Cost가 어떻게 변하는지 보기 위해 쓰인다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre><td class="rouge-code"><pre><span class="n">alpha</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">err_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># m번째 data와 weights간 선형조합
</span><span class="k">def</span> <span class="nf">lincomb</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">m</span><span class="p">])</span>

<span class="c1"># m번째 data에서 계산한 모델값과 데이터간 차이
</span><span class="k">def</span> <span class="nf">err</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">lincomb</span><span class="p">(</span><span class="n">m</span><span class="p">))</span> <span class="o">-</span> <span class="n">ydata</span><span class="p">[</span><span class="n">m</span><span class="p">])</span>

<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>

    <span class="n">grad_x0</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">grad_x1</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">grad_x2</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">err_loc</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1data</span><span class="p">)):</span>
        <span class="n">grad_x0</span> <span class="o">=</span> <span class="n">grad_x0</span> <span class="o">+</span> <span class="n">err</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">*</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">lincomb</span><span class="p">(</span><span class="n">i</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">lincomb</span><span class="p">(</span><span class="n">i</span><span class="p">)))</span><span class="o">*</span><span class="n">x0data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">grad_x1</span> <span class="o">=</span> <span class="n">grad_x1</span> <span class="o">+</span> <span class="n">err</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">*</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">lincomb</span><span class="p">(</span><span class="n">i</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">lincomb</span><span class="p">(</span><span class="n">i</span><span class="p">)))</span><span class="o">*</span><span class="n">x1data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">grad_x2</span> <span class="o">=</span> <span class="n">grad_x2</span> <span class="o">+</span> <span class="n">err</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">*</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">lincomb</span><span class="p">(</span><span class="n">i</span><span class="p">))</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">lincomb</span><span class="p">(</span><span class="n">i</span><span class="p">)))</span><span class="o">*</span><span class="n">x2data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">err_loc</span> <span class="o">=</span> <span class="n">err_loc</span> <span class="o">+</span> <span class="p">(</span><span class="n">err</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">err_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">err_loc</span><span class="p">)</span>

    <span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_x0</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x1data</span><span class="p">))</span>
    <span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_x1</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x1data</span><span class="p">))</span>
    <span class="n">weights</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_x2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x1data</span><span class="p">))</span>
</pre></table></code></div></div><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="k">print</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1data</span><span class="p">)):</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"(x1=</span><span class="si">{</span><span class="n">x1data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">, x2=</span><span class="si">{</span><span class="n">x2data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">) =&gt; y=</span><span class="si">{</span><span class="n">ydata</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s">, Model Output = </span><span class="si">{</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">lincomb</span><span class="p">(</span><span class="n">i</span><span class="p">))</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>


<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">err_list</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Error During Learning"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Epoch"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Error"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre>[-4.1032397   8.67424375  8.67424407]
(x1=0, x2=0) =&gt; y=0, Model Output = 0.02
(x1=1, x2=0) =&gt; y=1, Model Output = 0.99
(x1=0, x2=1) =&gt; y=1, Model Output = 0.99
(x1=1, x2=1) =&gt; y=1, Model Output = 1.00
</pre></table></code></div></div><p><img data-src="https://raw.github.com/ch-hey/imgcdn/master/img/2022-08-10/output_3_1.png" alt="png" data-proofer-ignore></p><p>위 결과를 보면 데이터와의 오차가 충분히 작아졌다고 판단된다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre><td class="rouge-code"><pre><span class="n">x_plot_whole</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">x_plot_1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">y_plot_1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">x_plot_0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>
<span class="n">y_plot_0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>

<span class="n">y_line</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="n">weights</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">x_plot_whole</span> <span class="o">-</span> <span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_plot_1</span><span class="p">,</span> <span class="n">y_plot_1</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s">'s'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_plot_0</span><span class="p">,</span> <span class="n">y_plot_0</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s">'s'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot_whole</span><span class="p">,</span> <span class="n">y_line</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"x1"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"x2"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</pre></table></code></div></div><p>(x1, x2, y) 형태의 data를 2d로 표현해보면 아래 그림과 같다. 파란색 점은 y=1인 경우고 주황색 점은 y=0인 경우에 해당한다. 빨간 선은 학습결과 구해진 weight를 가지고 선형조합 부분을 표현한 것이며 x1과 x2로 이루어진 plane을 y=0과 y=1이 나오는 부분으로 나눠주는 것을 볼 수 있다.</p><p>빨간색 선 아래 부분은 활성화되지 못하는 영역, 빨간색 선 위 부분은 활성화되는 영역이라고 생각해도 될 것 같다. 빨간선을 Decision Boundary라고 부르기도 한다.</p><p><img data-src="https://raw.github.com/ch-hey/imgcdn/master/img/2022-08-10/output_4_0.png" alt="png" data-proofer-ignore></p><h2 id="summary"><span class="mr-2">Summary</span><a href="#summary" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>인공신경망의 작동방식을 확인해 봤다. 간단히 말하면 Features와 Parameter의 선형조합 이후 활성화 과정으로 이어지는 순차적인 연산이었다.</p><p>실은 hidden layer가 없는 인공신경망은 결국 logistic regression과 동일한 구조라서 인공신경망을 했다고 말하기는 좀 민망하긴 하다. 하지만 작동원리를 알아보는데는 이정도면 충분할 것 같다. 결국 서로 복잡하게 얼키고 설킨 logistic regression이 인공신경망이라고 생각한다.</p><p>수식은 나름 정리해봤지만 정말 간단한 케이스에 대해서만 봤을 뿐이다. 진짜 수학적으로 궁금한게 많다면, Michael Nielsen의 <a href="http://neuralnetworksanddeeplearning.com/">“Neural Networks and Deep Learning”</a> 다시 한번 추천한다.</p><p>인공신경망 먼저 정리해보긴 했는데 다른 모델들도 많다. 인공신경망을 이용한 머신러닝은 <a href="https://www.tensorflow.org/">Tensorflow</a>, 그외 다양한 방법들은 <a href="https://scikit-learn.org/stable/index.html">scikit-learn</a>이 Python 라이브러리로 유명하다. 아, <a href="https://pytorch.org/">Pytorch</a>도 있다. 이제는 라이브러리를 어떻게 쓰는지 공부해야 한다..</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/machine-learning/'>Machine Learning</a>, <a href='/categories/neural-network/'>Neural Network</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/machine-learning/" class="post-tag no-text-decoration" >Machine Learning</a> <a href="/tags/neural-network/" class="post-tag no-text-decoration" >Neural Network</a> <a href="/tags/python/" class="post-tag no-text-decoration" >Python</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=%5B%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%5D+%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D+Artificial+Neural+Network+-+ChBlog&url=https%3A%2F%2Fch-hey.github.io%2Fposts%2FNeural-Network%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=%5B%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%5D+%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D+Artificial+Neural+Network+-+ChBlog&u=https%3A%2F%2Fch-hey.github.io%2Fposts%2FNeural-Network%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fch-hey.github.io%2Fposts%2FNeural-Network%2F&text=%5B%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%5D+%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D+Artificial+Neural+Network+-+ChBlog" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/venv-Python-32bit/">[Python] Virtual Env(venv)로 가상환경 만들기</a><li><a href="/posts/AI-Tensorflow/">[머신러닝] Tensorflow</a><li><a href="/posts/Neural-Network/">[머신러닝] 인공신경망 Artificial Neural Network</a><li><a href="/posts/Engineering-Calculation/">[Eng. Calc.] Differential Equations - Kinetics</a><li><a href="/posts/Orientation/">Orientation</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/engineering-calculation/">Engineering Calculation</a> <a class="post-tag" href="/tags/machine-learning/">Machine Learning</a> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/automation/">Automation</a> <a class="post-tag" href="/tags/vscode/">VSCode</a> <a class="post-tag" href="/tags/andrew-ng/">Andrew NG</a> <a class="post-tag" href="/tags/aspen-plus/">Aspen Plus</a> <a class="post-tag" href="/tags/com-object/">COM Object</a> <a class="post-tag" href="/tags/coursera/">Coursera</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Gradient-Descent/"><div class="card-body"> <em class="small" data-ts="1659959700" data-df="ll" > Aug 8, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[머신러닝] Linear Regression with Gradient Descent</h3><div class="text-muted small"><p> 수치해석시간에 배우는 내용들 기억한다면 빠르게 Skip해도 좋다. 후반부에 Python으로 작성한 코드도 있기 때문에 한 번 정도는 보는 것도 좋을 것 같다. Gradient Descent 방식을 통해 Linear Regression을 진행한다. Machine Learning이란걸 해본다. 1. Intro 머신러닝에서 사용되는 알고리즘중 하나인 ...</p></div></div></a></div><div class="card"> <a href="/posts/Engineering-Calculation/"><div class="card-body"> <em class="small" data-ts="1661169300" data-df="ll" > Aug 22, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[Eng. Calc.] Differential Equations - Kinetics</h3><div class="text-muted small"><p> 공학용 계산 첫 번째 포스팅이다. 첫번째 주제는 반응속도식 문제 풀이다. 수학으로는 미분방정식 풀이와 fitting 예제에 해당한다. 계산에는 Python을 쓴다. 1. Intro 간단한 문제에서부터 시작하려고 한다. 이 문서의 예제와 데이터를 따라가본다. 아래 반응을 보자. \[A \rightarrow B\] 반응속도식을 정하는건 이런저런 고...</p></div></div></a></div><div class="card"> <a href="/posts/Engineering-Calculation2/"><div class="card-body"> <em class="small" data-ts="1662378900" data-df="ll" > Sep 5, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>[Eng. Calc.] Regression and Curve-Fit</h3><div class="text-muted small"><p> 이번 공학용 계산 주제는 Regression 이다. 편의상 Fitting 이라고도 한다. Regression, Fitting 원래는 명확한 구분이 있을거 같으면서도 굉장히 혼용되서 사용되는 단어 같다. 실은 딱히 명확한 구분은 없는 것 같다. 관련해선 이 문서를 한 번 읽어보자. 여기서는 변수, independent variable $x$에 대해서 ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Gradient-Descent/" class="btn btn-outline-primary" prompt="Older"><p>[머신러닝] Linear Regression with Gradient Descent</p></a> <a href="/posts/Engineering-Calculation/" class="btn btn-outline-primary" prompt="Newer"><p>[Eng. Calc.] Differential Equations - Kinetics</p></a></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/ch-hey">HEY</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/engineering-calculation/">Engineering Calculation</a> <a class="post-tag" href="/tags/machine-learning/">Machine Learning</a> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/automation/">Automation</a> <a class="post-tag" href="/tags/vscode/">VSCode</a> <a class="post-tag" href="/tags/andrew-ng/">Andrew NG</a> <a class="post-tag" href="/tags/aspen-plus/">Aspen Plus</a> <a class="post-tag" href="/tags/com-object/">COM Object</a> <a class="post-tag" href="/tags/coursera/">Coursera</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { function updateMermaid(event) { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { const mode = event.data.message; if (typeof mermaid === "undefined") { return; } let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } let initTheme = "default"; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); window.addEventListener("message", updateMermaid); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
